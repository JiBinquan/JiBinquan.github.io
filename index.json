


[{"content":"","date":"2025年4月9日","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025年4月9日","externalUrl":null,"permalink":"/tags/%E6%8A%80%E6%9C%AF/","section":"Tags","summary":"","title":"技术","type":"tags"},{"content":"\r背景 #\r校园网高昂的流量费用让许多学生苦不堪言。然而，许多高校的IPv6网络流量是免费的，因此在使用百度、知乎等支持IPv6访问的网站时，可以显著节省流量消耗。然而，这一优势在访问仅支持IPv4的网站（如B站等视频平台）时却无法体现，这使得流量消耗最大的场景反而无法享受免费红利，就是离谱。\n本文将介绍一种技术方案：通过Cloudflare Workers和Pages服务部署反向代理节点，实现通过IPv6网络访问仅支持IPv4的网站。该方法利用Cloudflare的全球分布式网络，将请求经由支持IPv6的路径转发至目标服务器，从而绕过校园网IPv4流量计费限制。\n本文基于edgetunnel项目实现，项目连接：\rcmliu/edgetunnel: 在原版的基础上修改了显示 VLESS 配置信息转换为订阅内容。使用该脚本，你可以方便地将 VLESS 配置信息使用在线配置转换到 Clash 或 Singbox 等工具中。\nedgetunnel部署教程：\rCF新服务条款解读 \u0026amp; edgetunnel最新教程：全面解析与功能演示 新老玩家必看内容 CF VLESS 免费节点 优选订阅 Workers \u0026amp; Pages CM喂饭干货满满27 #科学上网\n实现方法 #\r第一步：fork edgetunnel #\r首先，访问 edgetunnel 项目 并 fork 到自己的仓库。\n第二步：Cloudflare部署 #\r注册 Cloudflare 免费账号：\r个人 | Cloudflare 注册并完成邮箱验证后，进入 Cloudflare 控制台中的 Workers 和 Pages 服务。 点击“创建” → 选择 Pages → 通过导入现有 Git 存储库创建项目。 登录 GitHub 后选择你 fork 后的 edgetunnel 项目。 请注意，项目名称不能命名为 “edgetunnel”，其他任意名称均可。保存并启动部署。 部署过程中会显示项目的部署状态。\n第三步：pages变量设置 #\r在 Cloudflare Pages 控制台中，点击你刚刚部署的 Page。 进入“设置” → “变量和机密” 添加以下变量：UUID、ADD、ADDAPI 和 PROXYIP。\n在弹出的窗口中逐个添加变量，变量类型均选择“文本”。\n各变量的参考值如下：\nUUID: 可设置任意字符串（建议使用小写字母和数字组合）。\nADD:\n[2400:cb00:f00e:a8:5d:c71d:2c71:7c63]\r[2803:f800:50:0:d6ce:3a6:7088:f992]\r[2803:f800:51::f49d:b305:cbc7]\r[2803:f800:51::84f0:bd51:326a] ADDAPI:\nhttps://addressesapi.090227.xyz/cmcc-ipv6\rhttps://addressesapi.090227.xyz/ct\rhttps://addressesapi.090227.xyz/cmcc PROXYIP:\nProxylPSG.CMLiussss.net 第四步：重新部署Pages #\r设置完变量后，返回部署页，重试部署\n第五步：订阅到Clash #\r部署完成后，进入相应页面点击访问，会跳转到一个特定网址。\n在该网址后面添加你之前设置的 /UUID 参数：\n下载Clash-verge，导入复制的链接即可完成订阅的导入\n实现原理分析 #\r尚不明确，GPT写的大概看看吧：\n反向代理与协议转换 #\r利用 Cloudflare 的 Workers 和 Pages 服务，可以搭建一个反向代理节点。反向代理工作原理如下：\n请求转发： 当用户发起一个访问请求时，Cloudflare 将该请求首先路由到部署在其边缘节点的 Worker 服务。Worker 服务接受客户端的请求后，再将请求转发至目标服务器（仅支持 IPv4 的网站）。 协议转换： Worker 层既支持 IPv6 也支持 IPv4，通过配置不同的网络接口，Cloudflare 可以在源端使用 IPv6进行传输，再将请求转换并通过其内部网络转发为 IPv4请求。这一过程完全在 Cloudflare 自身内部网络完成，有效地绕过了校园网对于 IPv4 流量的计费限制。 配置与变量定制 #\r通过将关键参数（如 UUID、ADD、ADDAPI 和 PROXYIP）设为 Cloudflare Pages 的环境变量，系统得以灵活管理和动态调整：\nUUID 用于标识用户或特定的订阅配置，使每个请求可以关联到特定的配置。 ADD 与 ADDAPI 则为内部提供地址解析和接口调用，决定了请求如何在 Cloudflare 内部路由及转换。 PROXYIP 则用来指定目标代理服务器的地址，从而实现最终请求的转发及响应返回。 免责声明：\n本文所述技术方案仅供网络技术学习交流，严禁用于任何形式的非法网络访问或流量欺诈行为 使用者应确保自身行为符合所在国家/地区的法律法规及所在高校的网络使用规定，因技术滥用导致的一切后果由使用者自行承担 Cloudflare服务存在明确的使用条款限制（\r服务协议），部署者需自行确保技术实现方式不违反服务提供商的相关规定 网络协议转换可能带来潜在的网络安全风险，包括但不限于数据泄露、网络攻击等安全隐患 作者不对任何因参照本文实施造成的账号封禁、法律纠纷或经济损失承担连带责任 高校网络政策存在动态调整可能，实施前请务必确认本校IPv6流量计费规则 本技术方案可能因服务商策略变更随时失效，请保持对网络技术发展的持续关注 ","date":"2025年4月9日","externalUrl":null,"permalink":"/posts/cloudflare2v6/","section":"文章","summary":"借助Cloudflare的免费转发额度，实现ipv4网站的校园网ipv6访问，以达到校园网免流效果","title":"通过Cloudflare实现IPv6免流上网","type":"posts"},{"content":"","date":"2025年4月9日","externalUrl":null,"permalink":"/tags/%E7%BD%91%E7%BB%9C/","section":"Tags","summary":"","title":"网络","type":"tags"},{"content":"\r目前我写的文章 #\r","date":"2025年4月9日","externalUrl":null,"permalink":"/posts/","section":"文章","summary":"","title":"文章","type":"posts"},{"content":"\rLLM的长文本生成是一个广泛需求但难度较高的研究方向。自从大型语言模型首次进入大众视野以来，大家便积极尝试利用这些模型创作长篇故事。然而，由于上下文长度的限制和灾难性遗忘问题，如何确保生成内容前后一致、逻辑连贯并避免冗余输出，仍是亟待解决的挑战。同时，相较于LLM在语言处理或问答领域的应用，如何客观评估生成文章的质量也是研究者必须面对的重要问题。虽然目前LLM在问答和长文本输入方面已有不少成果，但长文本输出的研究热度相对较低。本文选取了自2024年以来发表的4篇长文本输出相关论文，旨在探讨它们如何应对上述研究难题。\n1. STORM：通过检索和多视角提问综合生成主题大纲 #\r发表单位：斯坦福大学\n论文地址：[\r2402.14207] Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n项目地址：\rstanford-oval/storm: An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.\n录用情况：NAACL 2024\n任务目标 #\r动机：大型语言模型（LLMs）在写作方面表现出色，但如何利用它们撰写类似维基百科的长篇条目文章仍待探索。撰写此类文章需要在写作前的准备阶段进行彻底的研究和规划，而之前生成维基百科文章的工作通常跳过了这一阶段，假设参考文档或文章大纲已存在，这在一般情况下并不现实。 现有问题：收集参考文献和制定大纲需要先进的信息素养技能，这对经验丰富的作家来说也极具挑战性。此外，仅依靠LLMs的参数知识生成大纲或文章存在缺乏细节和幻觉的问题，尤其是在处理长尾主题时。本文的目标即是生成一个具有深度广度的文章大纲，进而生成高质量文章。 具体方法 #\r论文通过提出一个名为STORM（Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking）的系统来解决这个问题。STORM系统的核心思想是通过模拟多视角的对话来研究主题，并基于这些对话创建文章大纲。STORM的工作流程可以分为以下几个关键步骤：\nStep1. 主题驱动的视角构建 #\r发现多样视角：STORM首先通过检索和分析类似主题的维基百科文章来发现不同的视角。它会检索与给定主题相关的维基百科文章，并提取这些文章的目录（TOC），然后基于这些目录来识别可以用于撰写全面文章的不同视角 $ P = {p_1,p_2,\u0026hellip;,p_n}$。此外，作者在 $P$ 中添加了 $p_0$ 作为 “专注于广泛涵盖主题的基本事实的事实编写者”视角，以确保关于原始主题 $t$ 的基本信息也得到涵盖。 指定具体视角：在识别出多样视角后，STORM会为每个视角创建一个角色，这些角色将指导后续的问题提问过程。例如，对于“2022年冬奥会开幕式”这一主题，可能的视角包括活动策划者、参与者、观众等，每个视角都有其独特的关注点。 Step2. 模拟对话 #\r初始化对话：对于每个视角，STORM会模拟一个维基百科写作者与主题专家之间的对话。对话从写作者基于主题和指定视角提出一个问题开始。\n多轮对话：在每一轮对话中，写作者会根据主题、指定的视角以及之前的对话历史提出一个问题。然后，系统会将这个问题分解成多个搜索查询，并使用搜索引擎来查找相关的网络信息。搜索结果会经过基于规则（\rWikipedia:Reliable sources - Wikipedia）的筛选，以确保只使用可信的来源。最后，系统会综合这些可信的来源来生成专家的回答，并将这些回答添加到参考文献集合 $R$ 中。\n结束对话：当写作者没有更多问题时，对话结束。这个过程会为每个视角生成一系列的问题和答案，形成多轮对话的记录。\nStep3. 创建文章提纲 #\r初步提纲生成：在实际写作开始前，STORM先创建一个大纲。即首先提示模型仅根据给定的主题 $t$ 生成草稿大纲 $O_D$。这个提纲是一个包含多级标题的列表，用于指导文章的撰写。 提纲细化：在完成所有视角的模拟对话后，STORM会进一步结合对话中的信息来细化和优化初步大纲 $O_D$，使其更加全面和有组织，改进后的大纲 $O$ 将被用于生成全文。 Step4. 撰写全文 #\r基于提纲撰写：在完成提纲后，STORM会根据提纲和收集到的参考文献来撰写完整的维基百科文章。因为参考文献集合 $R$ 一次性塞不进大模型，系统会逐段撰写文章，每段内容都基于提纲中的相应标题和使用标题检索到的参考文献集合 $R$ 中的相关信息。 文章整合与优化：最后，把生成的各段内容拼接到一起，再直接输入到LLM中提示其删除重复信息，优化文章的连贯性和逻辑性，并根据维基百科的风格规范添加摘要部分，即完成了整体文章的生成。 评估方法 #\r为了评估大纲覆盖率，作者引入了两个指标：标题软召回和标题实体召回。这些指标比较人类撰写的多级章节标题（视为真实值）与O.。考虑到这两组标题元素之间不需要完全匹配，作者使用来自标题的句子BERT嵌入（Reimers和Gurevych，2019年）计算标题软召回（Franti和Mariescu-Istodor，2023）。作者还计算标题实体召回，量化为人类撰写的文章标题中被O覆盖的命名实体百分比。作者提取FLAIR命名实体识别（NER）（Akbik等人，2019年）中的实体。\n数据集 #\rFreshWiki数据集：作者创建了一个名为FreshWiki的数据集，包含2022年2月至2023年9月期间创建或大量编辑的高质量维基百科文章。这些文章是最近编辑的，以避免与训练大型语言模型（LLMs）时使用的数据重叠，确保评估的公正性和有效性。数据集中的文章经过筛选，确保其质量达到B类或以上，并且排除了列表文章和没有子部分的文章。 评估指标 #\r大纲质量评估： 标题软召回率（Heading Soft Recall）：通过计算生成提纲中的标题与人工撰写文章中的标题之间的相似度来衡量提纲的覆盖范围。具体来说，使用Sentence-BERT嵌入来计算标题之间的余弦相似度，然后根据软召回率的定义来评估生成提纲与人工撰写文章的匹配程度。这种方法不要求标题完全一致，而是考虑语义上的相似性。 标题实体召回率（Heading Entity Recall）：计算生成提纲中的标题所涵盖的命名实体在人工撰写文章标题中的比例。通过FLAIR命名实体识别（NER）工具来提取实体，从而评估生成提纲在涵盖文章关键实体方面的表现。 文章质量评估： 自动指标：采用ROUGE分数来评估生成文章与参考文章之间的相似度。ROUGE是一种常用的文本摘要评估指标，通过比较生成文本和参考文本中的重叠n-gram、词对和词序列来衡量文本的相似性。此外，还计算文章级别的实体召回率，以评估生成文章涵盖关键实体的情况。 LLM/人工评价指标：根据Wikipedia标准，从以下五个方面对文章进行评价： 兴趣水平（Interest Level）：评估文章是否引人入胜，能否吸引读者的注意力并激发思考。 连贯性和组织性（Coherence and Organization）：判断文章是否结构清晰、逻辑连贯，段落之间是否有良好的过渡。 相关性和聚焦性（Relevance and Focus）：检查文章是否紧扣主题，避免无关内容的干扰。 覆盖范围（Coverage）：衡量文章对主题的各个方面是否有深入探讨，是否提供了全面的覆盖。 可验证性（Verifiability）：确保文章中的每个陈述都有可靠的引用支持，避免原创研究和未经证实的主张。 基线方法 #\rDirect Gen：直接提示LLM生成提纲，然后使用该提纲生成完整文章。 RAG（检索增强生成）：使用主题进行搜索，并结合搜索结果和主题来生成提纲或整篇文章。 Outline-driven RAG（oRAG）：与RAG在提纲创建上相同，但在生成文章时，进一步使用部分标题进行搜索，以获取更多信息来逐部分生成文章。 LLM评估 #\r作者使用Prometheus进行自动评估，这是一个13B的开源评估LLM ，可以根据定制的1-5分量规对长篇文章进行评分，从兴趣水平、连贯性和组织、相关性和焦点以及覆盖范围的角度给文章打分。下表给出了作者的评分标准。虽然Prometheus评估最好与分数为5的标准文档一起使用，但添加标准文档会超出模型的上下文长度限制。由于Prometheus原论文表明没有参考答案的Prometheus评分也与人类偏好密切相关，因此作者省略了参考答案，并通过迭代删除最短部分的内容来缩短输入文章至2000字以内，以确保输入可以适应模型的上下文窗口。\n人类评估 #\r评估者：邀请了10位经验丰富的维基百科编辑者参与评估，他们至少在维基百科上进行了500次编辑，并且有超过1年的经验。 评估方式：从数据集中随机抽取20个主题，评估这些主题下由STORM和oRAG生成的文章。每位编辑者会根据上述五个方面对每对文章进行评分，使用1到7的评分标准，其中1表示非常差，7表示非常好。此外，编辑者还需提供开放性反馈和成对偏好。 评估结果：通过计算平均评分、成对比较结果以及p值来分析STORM与oRAG之间的差异。结果表明，STORM生成的文章在组织性、覆盖范围、趣味性等方面均优于oRAG，且在与人类撰写的文章相比时，也展现出一定的优势。不过，编辑者也指出了STORM生成文章在中立性和可验证性方面存在的问题，如存在互联网来源的偏见、过度推断等。 实验结果 #\r实验结果：STORM在提纲覆盖范围和文章质量方面均优于基线方法。具体来说，STORM生成的文章在组织性、覆盖范围和引用质量等方面表现出色，且能显著提高文章的趣味性和相关性。 人类评估：经验丰富的维基百科编辑者认为STORM生成的文章在组织性和覆盖范围上优于基线方法，并且在与人类撰写的文章相比时，也展现出一定的优势。不过，编辑者也指出STORM生成的文章在中立性和可验证性方面仍存在挑战，如存在互联网来源的偏见、过度推断等问题。 评价 #\r基本就是使用多智能体的思想进行文章生成，使用的也是斯坦福自己的DsPy框架（不愧是斯坦福啊），先大纲后分部分生成也是逻辑十分顺畅的思路。评估方法与思路也有借鉴意义，人工评估财大气粗（不愧是斯坦福啊），大纲正文分别评估的方式有理有据，是比较不错的开拓性工作（不愧是斯坦福啊）。\n2. LongWriter：释放长上下文LLM的10,000+字生成能力 #\r发表单位：清华大学、智谱AI\n论文地址：[\r2408.07055] LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs\n项目地址：[THUDM/LongWriter: ICLR 2025] LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs\n录用情况：ICLR 2025\n任务目标 #\r​\t当前的长上下文大型语言模型（LLMs）能够处理长达100,000个token的输入，但在生成等长的输出上存在困难，通常输出长度限制在2,000个单词左右，这限制了其在需要长篇内容生成的应用场景中的使用。\n经过作者的研究，这种情况是模型采用的SFT数据集更多偏向于长输入，忽略了长输出任务的构建造成的。具体可见下图2，训练数据的输出文本越长，LLM就越能输出更长的文本。\n具体方法 #\rAgentWrite #\r本文的目的还是构建一个能够一次性输出超长文本的LLM，那没有输出这么长的训练数据怎么办呢？作者就设计了AgentWrite，具体来说，这是一个用来造数据的AGENT。\n这个 AgentWrite 分成两个阶段，首先是利用LLM的规划能力，在给定一个写作指令的情况下输出一个写作大纲，其中包括每段的主要内容和字数要求。\n然后，多次调用LLM，逐步完成每一个子章节的写作任务。这里作者是采用串行的方式进行每个章节的生成，即在生成第 $n$ 块文本时，会把前 $n-1$ 块文本和大纲一起输入给大模型。作者给出的理由是，经过实验，这种方式获得的整体连贯性和质量远优于并行生成的结果。\n数据评估 #\r那光让Agent造数据了，数据的质量怎么样呢？怎么筛选高质量数据？参见下节评估方式。\n其他工作 #\rLongWriter-6k数据集：通过上面的方法，作者利用AgentWrite和GPT-4o生成了包含6,000个SFT数据的LongWriter-6k数据集，输出长度范围为2k到32k个单词。\n模型训练：将LongWriter-6k数据集整合到模型训练中，通过监督微调和直接偏好优化（DPO）来扩展现有模型的输出窗口大小，使其能够生成超过10,000个单词的输出，同时保持输出质量。\n评估方式 #\r数据集 #\r作者通过两个数据集进行评估。\nLongWrite-Ruler：即规定不同的文章生成字数 $L$，如”写一篇关于罗马帝国的 $L$ 字文章（$L∈{1000，2000，5000，10000，20000，30000}$），看看LLM能否很好的达到目标的字数。共48个不同的写作任务提示（一半英文一半中文）。\nLongBench-Write：包含四个长度范围、七个文章类型共120个写作任务提示（一半英文一半中文）\n具体来说，作者通两个指标对输出进行评估，一个用于评分输出长度，另一个用于评分输出质量。\n评估指标 #\r首先，作者使用分段线性函数计算输出长度得分 $S_l$（其中 $l$ 是所需长度，而 $l\u0026rsquo;$是实际输出长度）：\n换句话说，当输出长度匹配要求时，得分是完美的100分。当输出长度大于或小于要求的四倍或三分之一时，得分线性衰减到零。由于过短的输出往往比过长的输出更令人头疼，所以指标为过短的输出设置了更高的分数衰减系数。\n另一方面，为了自动评估输出质量，作者使用GPT-4o，对生成文本在六个维度上对输出进行评分：相关性、准确性、连贯性、清晰度、广度和深度以及阅读体验。并且作者在提示中指示裁判模型仅根据输出的质量进行评分，而不考虑其长度以尽可能地将质量指标与 $S_l$ 解耦。最后通过在六个维度上的平均得分来获得输出质量的整体分数$S_q$。最终的分数 $\\overline{S}$ 是通过计算 $S_l$和 $S_q$的均值得到的。\n实验结果 #\r通过在LongBench-Write上的评估，LongWriter模型在输出长度和质量上均表现出色，其9B参数模型通过DPO进一步改进后，在基准上达到了最先进的性能，甚至超越了更大的专有模型。\n评价 #\r本文从pipeline方法回落到构造数据训练模型，目的在于激发模型输出长文本的潜力？文本质量评估方式也是直接通过LLM打分的方式进行的。就是还是存在这个问题：构造数据训练模型，输出第n个子章节需要输出前n-1个章节、LLM评估整个文章，都隐形限制了文本生成长度还是需要在模型可以接受的处理极限之内。\n3. AutoPatent：用于自动生成专利的多智能体框架 #\r发表单位：中科院深圳先进院、大连理工大学等\n论文地址：[\r2412.09796] AutoPatent: A Multi-Agent Framework for Automatic Patent Generation\n项目地址：\rQiYao-Wang/AutoPatent: Repository of AutoPatent.（代码目前没开源）\n发布时间：2024年12月\n任务目标 #\r动机：专利作为知识产权的重要组成部分，其撰写过程繁琐且耗时费力，通常由熟悉专利法并通过专利代理人考试的人类专利代理人完成，效率低下且成本高昂。随着大型语言模型（LLMs）的发展，其在知识密集型领域的强大能力为自动专利撰写提供了可能。 现有问题：现有的专利处理研究多集中在分类任务或短文本生成任务上，如专利分类、审查、摘要生成等，对于生成完整专利这一任务关注较少。此外，专利的特殊性、专业术语和长文本特性也给LLMs带来了挑战。 本文提出了一个具体的任务：D2P（Draft2Patent），模拟真实场景中发明人和专利代理人之间的互动，将发明人的草稿转换成完整的专利文档。具体来说，作者利用基于智能体的方法来模拟发明人和专利代理人之间的互动，设计了五个问题 $q_1、q_2、\u0026hellip;、q_5$ ，这些问题是关于一项发明的所有相关信息。作者将它们与发明人的答案$a_1、a_2、\u0026hellip;、a_5$ 结合起来形成专利草稿 $D$ 。任务目标即使用专利草稿 $D$ 生成专利 $P$，由专利的标题、摘要、背景、总结、主张和详细描述组成。\n具体方法 #\r作者提出了一个名为AutoPatent的自动多代理专利起草框架，如上图所示。其设计了一个具有八个智能体和三个步骤的专用流程，以模拟现实场景中的专利起草过程。\nStep1. 短文本组件生成（Short Components Generation） #\r目的：根据草稿 $D$ 生成专利的各个短文本组件，包括标题（T）、摘要（A）、背景（B）、总结（S）和权利要求（C），即除去详细描述之外的其他文本。\n方法：使用不同的写作者Agent（componentWriter）来生成这些短文本组件。对于参数规模小于14B的开源模型，使用D2P训练集对其进行微调以增强其生成高质量短组件的能力；对于商业模型或较大模型，则使用零样本提示（见下图）。\n结果：生成的标题、摘要、背景、总结和权利要求与草稿 $D$ 结合形成参考 $R$ ，为详细描述的生成提供有用信息。\n2. 专利撰写指南树（PGTree）构建（Building PGTree） #\r目的：为专利的详细描述生成一个撰写指南树（PGTree），以指导描述写作者完成详细描述的撰写。\n方法：使用规划Agent（planningAgent）根据草稿 $D$ 生成PGTree。PGTree是一个两层多路树结构，第一层提供部分的概述，第二层提供具体的撰写指令。\n结果：生成的PGTree将详细描述的撰写任务分解为多个部分和子部分，为描述写作者提供清晰的撰写指导。\n3. 参考-审查-增强生成（RRAG）（Reference-Review-Augmented Generation） #\r目的：根据PGTree的指导和参考 $R$ 中的信息，生成专利的详细描述。 方法： 检索：描述写作者Agent（descriptionWriter）根据PGTree中的指导 $n_{ij}$ 从参考 $R$ 中检索有用信息 $r_{ij}$。 生成：结合检索到的信息 $r_{ij}$、指导 $n_{ij}$和 PGTree $W$，描述写作者生成详细描述的子部分 $d_{ij}$。 审查与反馈：审查Agent（examinerAgent）主动介入，评估生成的子部分 $d_{ij}$ 的质量，并提供反馈。如果审查未通过，描述写作者将根据反馈对 $d_{ij}$ 进行修改，直到审查通过。 拼接：经过审查和修改的所有子部分 $d_{ij}$ 被拼接在一起，形成完整的详细描述 $D$。 结果：生成的详细描述 $D$ 与之前生成的短文本组件结合，形成完整的专利 $P$。 评估方式 #\r客观指标评估 #\rBLEU：一种基于n-gram的机器翻译自动评估指标，通过比较机器生成的文本与参考文本之间的n-gram重叠程度来衡量生成文本的质量。在本文中，使用BLEU指标来评估生成专利与真实专利在词汇和短语层面的相似度。\nROUGE系列：包括ROUGE-1、ROUGE-2和ROUGE-L，也是基于n-gram的指标，用于评估生成文本与参考文本之间的相似度。ROUGE-1和ROUGE-2分别考虑了单字和双字的重叠，而ROUGE-L则基于最长公共子序列（LCS）来衡量文本相似度。这些指标能够从不同角度反映生成专利与真实专利在内容上的接近程度。\nIRR（Inverse Repetition Rate）：本文新提出的指标，用于衡量专利文本中句子重复的程度。其计算公式为： $$ IRR(\\mathcal{P},t)=\\frac{C_n^2}{\\sum_{i=1}^{n-1}\\sum_{j=i+1}^nf(s_i,s_j)+\\varepsilon} $$\n其中 $P$ 表示专利文本，由 $n$ 个句子组成；$ε$ 是一个小值，用于平滑防止除以零；$t$ 是阈值，用于根据句子 $s_i$ 和 $s_j$ 的 Jaccard 相似度 $J$ 来判断它们是否为重复句；函数 $ f(s_i, s_j) $ 根据Jaccard相似度是否大于等于 $t$ 来取值为1或0。IRR值越低，表示文本中的句子重复程度越低，生成的专利质量越高。\\\nJaccard 相似度： $$ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $$\n人工评估 #\r评估人员：邀请了三位熟悉专利法和专利撰写的专家进行评估，每人拿到两篇专利，一篇来自 评估标准：从准确性、逻辑性、全面性、清晰性、连贯性和一致性六个维度对生成的专利进行评估。具体来说： 准确性：要求专利文本中的每一个技术细节都必须正确，避免模糊表达，参数、结构和过程应具体清晰地描述，以确保发明在技术上可实现，且用词应符合技术领域的标准，避免歧义和不必要的限制。 逻辑性：专利文本的结构应符合专利法的要求，包括摘要、背景、总结、详细描述和权利要求等必要部分，且各部分之间的逻辑连贯，使读者能够逐步理解发明的背景、创新点和具体应用。 全面性：专利文本应充分披露发明，使本领域的技术人员能够理解和实施，避免遗漏和模糊描述，同时使用广泛涵盖各种修改和替代方案的术语，以最大化法律保护的范围。 清晰性：专利文本应在技术和法律方面取得良好的平衡，使技术解决方案的描述既清晰又易于理解，避免不必要的复杂和冗长的句子结构。 连贯性：专利文本必须精确地表达发明，避免使用模糊或不确定的术语，各部分和段落之间应逻辑组织，确保思路的连贯性，使审查员能够逐步理解发明的整体内容。 一致性：专利文本必须与提供的真实专利保持一致，确保技术解决方案的描述准确且连贯，各部分之间以及术语的使用应保持一致，避免出现矛盾，以降低专利被无效的风险并增强其法律稳定性。 评估流程：将生成的专利文本与真实专利文本进行对比，专家们根据上述标准对生成专利的质量进行打分或评价，最终得到AutoPatent框架生成专利的综合质量评估结果。 由于专利的篇幅较长，作者选择不使用基于大语言模型（LLM）的评估方法，因为这些方法往往无法提供公正且准确的结果。\n实验结果 #\r实验结果：实验结果表明，AutoPatent框架显著提高了各种LLMs生成完整专利的能力。以Qwen2.5-7B模型为基础的AutoPatent框架生成的专利，在客观指标和人工评估方面均优于GPT-4o、Qwen2.5-72B和LLAMA3.1-70B等更大、更强大的LLMs生成的专利。 优势：AutoPatent框架能够生成长度更长、内容更完整、质量更高的专利，有效减少了重复错误，提高了专利撰写的效率和质量。 评价 #\r除去短文本的生成部分，主要思想就是在先大纲再局部的基础上加入了一个质量审查模块…也行吧，工程和逻辑上都说的通。这个文本长度够长的，结果直接把LLM评估去了…人工评估样本量就很难上去了。\n4. CogWriter: 约束长格式文本生成的认知写作视角 #\r发表单位：穆罕默德·本·扎耶德人工智能大学、中国科学院大学、南洋理工大学\n论文地址：[\r2502.12568] A Cognitive Writing Perspective for Constrained Long-Form Text Generation\n代码未开源\n发布时间：2025年2月\n任务目标 #\r动机：尽管大型语言模型（LLMs）在自然语言处理任务中展现出了类似人类的写作能力，但在生成符合复杂约束的高质量长篇文本方面仍存在困难。根据认知写作理论，人类写作是一个复杂的认知过程，涉及计划、翻译、回顾和监控等迭代活动。而LLMs的单次生成方式与这些关键认知原则存在根本冲突，主要表现在：将长篇文本生成视为端到端任务，忽视了分层规划过程；自回归架构使得生成的文本不可变，无法进行回顾和重组；缺乏明确的评估机制，难以在长篇生成中保持与目标的一致性。 现有问题：LLMs在生成长篇文本时，容易在长跨度文本中失去连贯性，难以处理复杂的多线程叙事，并且在满足详细指令要求方面表现不佳，如在超过10,000字的文本中遵循详细指令。这些问题限制了LLMs在需要扩展、结构化内容的应用中的使用，如创意设计提案、技术文档和综合研究报告等。 作者将任务定义为受限长篇文本生成任务，即生成一系列相互关联的文本段落 $D = {D₁, D₂, \u0026hellip;, Dₙ}$，其中每个 $Dᵢ$ 代表一个连贯的文本单元，并且必须满足一定的约束条件。这些约束条件通过指令集T来体现，T包含以下三种类型的指令：\n单指令（Single Instruction, SI）：这种指令指定了必须出现在确切、预定义位置的内容。例如，在生成摩天大楼设计文本时，指定第20层必须包含医疗中心。 范围指令（Range Instruction, RI）：这种指令指定了在指定范围内每个描述必须包含的内容。例如，在生成摩天大楼设计文本时，指定第5-12层为公司办公室。 周期指令（Periodic Instruction, PI）：这种指令要求在固定间隔周期性重复特定内容。例如，在生成摩天大楼设计文本时，每5层设置一个安全检查站。 这三种类型的指令被统一到一个综合的检查集 $T = {Tₛ, Tᵣ, Tₚ}$ 中，用于全面指导和约束文本生成过程\n具体方法 #\r框架概述 #\r目标：CogWriter旨在通过整合规划、监控和回顾机制，弥合当前LLMs与人类写作过程之间的差距，从而提升LLMs在长篇文本生成任务中的表现。 核心组件： 规划Agent（Planning Agent）：负责分层分解任务，创建结构化计划，将复杂的写作任务分解为可管理的组件，同时保持它们之间的复杂关系。 生成Agent（Generation Agents）：负责根据计划生成文本段落，同时监控机制持续评估输出，检测内容、结构或要求方面的偏差。当发现问题时，会触发回顾过程，修订和优化输出，确保整体连贯性和对指令的遵循。 规划Agent #\r功能：规划代理作为系统的战略核心，类似于有经验的作家从详细的提纲开始，分析任务要求，并在严格的格式约束下生成初始计划Pinitial。 过程： 生成初始计划：根据任务特定的提示 $p_{plan}$ ，结合指令描述 $T$，生成初始计划 $P_{initial}$。 计划修正：通过监控机制评估初始计划，验证其是否满足任务约束和结构正确性。如果发现问题，会触发修正过程，生成修订后的计划 $P_{revised}$，并进行格式修正，得到最终的计划 $P$。 生成Agent #\r功能：一旦规划代理生成了最终的全局计划 $P$，多个生成Agent将接管，每个代理负责根据特定的描述任务 $Di$ 生成内容。 过程： 验证和调整本地计划：每个生成代理首先验证和细化分配到的本地计划 $P_i$ ，通过监控和回顾机制确保其符合指令要求 $T$。如果发现不一致，会调整计划，得到 $P_i\u0026rsquo;$。 生成内容：根据调整后的计划 $P_i\u0026rsquo;$，生成初始内容 $D_{initiali}$。 长度调整：由于当前大多数LLMs在控制输出长度方面存在限制，会使用修订函数调整内容，使其满足指定的长度 $L$，同时保留关键细节、语义完整性和整体连贯性。 整体流程 #\r规划阶段： 规划代理根据任务指令生成初始计划。 通过监控和修正机制，确保计划的高质量和有效性。 生成阶段： 多个生成代理根据优化后的计划并行生成文本段落。 在生成过程中，持续监控和回顾，确保内容符合要求。 对生成的文本进行长度调整和优化，确保最终输出既符合指令又具有良好的连贯性和逻辑性。 评估方法 #\r数据集 #\r数据集名称：LongGenBench-16K 数据集特点：该数据集专门用于评估模型在复杂约束下的长篇文本生成能力，包含四个场景，每个场景需要生成大约16,000个标记。具体场景包括： 日记写作（Diary Writing）：要求在一年的每周生成连贯的内容，注重时间一致性，每篇日记至少200字。 菜单设计（Menu Design）：同样要求在一年的每周生成连贯的内容，注重时间一致性，每篇菜单至少200字。 摩天大楼设计（Skyscraper Design）：通过详细设施安排来评估空间推理能力，每层楼描述至少150字。 城市规划（Urban Planning）：通过详细城市街区的设施安排来评估空间推理能力，每个街区描述至少150字。 指令类型：每个场景涉及三种指令类型： 单指令（Single Instructions）：指定必须出现在确切、预定义位置的内容。 范围指令（Range Instructions）：指定在指定范围内每个描述必须包含的内容。 周期指令（Periodic Instructions）：指定在固定间隔周期性重复的内容。 数据集规模：包含400个测试实例，每个场景100个实例。 评估指标 #\r主任务完成率（Comp. Rate）\n定义：主任务完成率衡量模型是否按顺序完成了所有指定的子任务，例如在日记写作任务中，是否为一年中的每一周都生成了条目。 评估方式：通过检查生成的文本是否涵盖了所有要求的子任务来计算完成率。例如，在摩天大楼设计任务中，是否为每一层楼都提供了描述。 指令遵循准确性\n定义：指令遵循准确性评估模型生成的文本是否符合指定的指令要求，包括单指令、范围指令和周期指令。 具体指标： 单指令准确性（Acc. Once）：衡量模型是否在指定的确切位置包含了特定内容。 范围指令准确性（Acc. Range）：评估模型是否在指定的范围内正确地包含了相关内容。 周期指令准确性（Acc. Periodic）：检查模型是否按照规定的间隔周期性地重复了特定内容。 评估方式：通过对比生成文本与指令要求，统计符合要求的内容出现的次数与总要求次数的比例来计算准确性。 平均准确性（Avg. Acc.）\n定义：平均准确性是单指令、范围指令和周期指令准确性的平均值，提供了一个综合的性能指标。 评估方式：将上述三种指令准确性的数值相加后取平均值。 字数统计（Words）\n定义：字数统计用于评估生成文本的长度是否达到了任务要求的最低字数标准。 评估方式：直接统计生成文本的字数，并与任务要求的最低字数进行比较。 长度控制性能\n定义：长度控制性能评估模型是否能够按照指定的长度要求生成每个文本段落，例如在摩天大楼设计任务中，每层楼的描述是否达到了150字的要求。 评估方式：通过分析生成文本的长度分布，检查其是否集中在目标长度附近，以及是否存在较大的偏差。 实验结果 #\r实验结果表明，CogWriter在所有评估指标上均表现出显著的改进。例如，当以Qwen-2.5-14B-Instruct为骨干时，与基线模型相比，完成率提高了0.51，平均准确性提高了0.17。对于Llama3.3-70B-Instruct和GPT-4o，CogWriter实现了接近完美的完成率，并显著提高了指令遵循准确性。\n评价 #\r认知科学拯救世界了，现在LLMs研究者人均认知科学家，你永远不知道下一篇看到的工作来自哪个远古的认知科学理论。论文整体…感觉思路跟上一篇专利的差不多，先生成后修改。评估方式方面，因为他任务定义成约束条件下文本生成了，主要关注的都是任务完成情况……也算另辟蹊径吧……。\n总结 #\r长文本生成目前还是没有一个完整的评估基准去进行方法的评估比较，目前感觉还是都处于探索的阶段。俗话说的好，文无第一，如何客观的评价一篇文章的好坏本来就是及其难以取得共识的问题，加上长文本生成任务的超长输出难以让LLM进行一次性完整评估的限制，自动评估基本只能是从某些侧面入手去进行评估文章某一方面的质量。那人工双盲评估算是可以让大众都能接受的一种评估方法，就是费钱。但是，从上面的工作中我们也能获得一些共识：\n对于模型一次无法完全输出的超长文本，先大纲后局部具体生成的方法是目前共识（因为这至少能一定程度上保证全文逻辑大方面的一致性）。对于局部输出质量的约束可以加入一定的质量监督模块进行保底。 ROUGE分数与LLM评估虽然说服性有待考证，不过依然是主流的长文本评估方式。 ","date":"2025年4月1日","externalUrl":null,"permalink":"/posts/250401%E9%95%BF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/","section":"文章","summary":"简单介绍STORM、LongWriter、AutoPatent和CogWriter这四篇2024年以来的LLM长文本生成论文","title":"【速览】四篇长文本生成论文速览","type":"posts"},{"content":"","date":"2025年4月1日","externalUrl":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent","type":"tags"},{"content":"","date":"2025年4月1日","externalUrl":null,"permalink":"/tags/%E7%A0%94%E7%A9%B6/","section":"Tags","summary":"","title":"研究","type":"tags"},{"content":"","date":"2025年4月1日","externalUrl":null,"permalink":"/tags/%E9%95%BF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/","section":"Tags","summary":"","title":"长文本生成","type":"tags"},{"content":"\r原文题目：Xu 等 - 2024 - Search-in-the-Chain Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\n原文链接：\rSearch-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks | Proceedings of the ACM Web Conference 2024\n项目地址：\rxsc1234/Search-in-the-Chain: Code for Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks\n动机 #\r面对需要多步推理和实时知识的复杂任务（如多跳问答、事实核查、长文本生成等），LLMs 存在以下短板：\n知识准确性不足：模型可能依赖记忆中的错误知识或产生与现实矛盾的“幻觉”； 推理链易断裂：传统检索增强方法（如分步检索）会打断模型的连贯推理； 可追溯性差：生成内容缺乏支持性证据，用户难以验证信息的可信度。 现有的检索增强方法虽能引入外部知识，却面临两难：\n检索错误可能误导模型：若检索到错误信息，LLM 可能被“带偏”； 静态推理路径：传统方法无法动态调整推理方向，导致错误积累。 方法 #\r图1 SearChain流程示意，此过程是在称为循环树的树上的节点识别深度优先搜索（正确的推理路径是绿色的）。最终内容包括推理过程和支持文档的引用。\r为突破上述瓶颈，作者提出 SearChain，一种结合大模型与信息检索的动态交互框架，核心设计包括以下三部分：\n1. 全局推理链（Chain-of-Query, CoQ） #\rLLM 将复杂问题分解为一系列子问题（查询-答案对），形成一条完整的推理链。 与传统分步推理的区别：CoQ 要求模型预先规划全局推理路径，而非逐次解决局部子问题，避免检索打断推理逻辑。 2. 检索系统的验证与补全 #\r验证机制：针对每个子问题的答案，检索系统通过外部知识库验证一致性。若检索结果置信度高且与模型答案冲突，则反馈修正建议。 补全机制：若模型标记某子问题为“未知”，检索系统直接提供缺失知识，帮助模型完善推理链。 3. 树状动态推理路径（Tree-of-Reasoning） #\r通过多轮交互，SearChain 将线性推理链扩展为树状结构，支持深度优先搜索（DFS）。 动态纠错与探索：当某节点需修正或补充时，模型可回溯至该节点生成新分支，灵活调整推理方向，避免“一条路走到黑”。 具体流程 #\r本流程来自于对SearChain原仓库代码的理解，其中的多轮迭代是采用LLMs多轮对话的方式进行的。\n第一步：生成CoQ #\r先让大模型根据当前用户问题，生成数个子问题。\n如果模型（认为自己）知道子问题答案，在子问题后直接生成相对应的答案。 如果模型不知道问题答案，则在对应子问题前生成[Unsolved Query]标签。 图2 CoQ生成Prompt\r以上的子问题应该可以构成一个解决用户问题的完整推理链。\n第二步：遍历CoQ #\r针对目前生成的CoQ，遍历其中的子问题：\n如果这个子问题之前处理过，则跳过之（因为处理过代表已经检索校正过了）。 找到CoQ中第一个没有处理过的子问题： 如果这个子问题为[Unsolved Query]，则利用检索器找到 Top1 的相关文档，使用一个阅读器生成答案（这里用的是DPR，而不是LLM），返回（问题，答案，证据）三元组。 如果这个子问题有直接生成的答案，则利用检索器找到 Top1 的相关文档，使用一个阅读器生成答案与其置信度，如果自信度高于预先规定的阈值，返回（问题，答案，证据）三元组。否则保留之前直接生成的答案不作修改。 将刚才处理过的问题记录为已处理。 第三步：修正CoQ #\r在获得刚才子问题的处理结果后，提示LLM重新生成一个新的CoQ。\n这里使用的LLM的多轮对话方式完成，LLM可以看到之前历史的CoQ与框架给出的子问题修正证据。\n以此迭代循环，直到有一条CoQ的所有问题都被校正或达到最大迭代次数（代码中设置为5）\n创新点 #\r全局规划 vs 局部优化\nSearChain 强调从全局视角规划推理链，而非逐次解决子问题。实验表明，面对困难子问题时，SearChain 能通过改写或进一步分解问题继续推理（平均推理步骤比基线多 30%），而传统方法容易“卡壳”。\n知识解耦与精准干预\n模型自身知识占主导（75%以上），检索系统仅在必要时介入：修正高置信错误（约 20%）或补全缺失知识（约 5%）。 这种“按需检索”机制减少了对模型的干扰，检索误导率比基线降低 50%以上。 可追溯的内容生成\nSearChain 为推理链中的每个步骤标记支持性文档引用。例如在回答“吉米·巴特尔效力的球队”时，生成内容会标注球员获奖信息、所属球队等来源。实验显示，其引用覆盖度（SKC）和位置准确性（AMP）显著优于 New Bing 等工具。\n图3 SearChain和New Bing标记支持文档引用的案例研究\r实验结果 #\r表1 SearChain 与基线模型在复杂知识密集型任务上的表现。加粗表示在不同设置下的最佳结果。FC：事实核查，LFQA：长篇问答。LFQA 的评估指标：ROUGE-L。其他任务的评估指标：cover-EM。\r在 8 个知识密集型任务（多跳问答、槽填充、事实核查等）的测试中，SearChain 表现优异：\n准确率提升：在 HotpotQA、Musique 等数据集上，比最优基线（如 DSP、Tree-of-Thought）平均高 5-10%； 效率平衡：尽管引入多轮交互，整体耗时与基线相当，未显著增加计算负担； 开源可复现：代码已公开，支持 Vicuna-13B 等开源模型。 讨论 #\r这篇文章的方法好处很多，逻辑一致的推理路径、对参考文档的精确引用、动态修改的推理链等。就是看代码的时候感觉在RAG的部分，采用Top1文档的检索方案以及使用性能不如LLM的DPR作为阅读器有点拖了整体框架的后腿，感觉如果换成更好的RAG策略这个方法还能获得更好的性能。\n","date":"2025年3月3日","externalUrl":null,"permalink":"/posts/searchain%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"介绍了一个动态推理链的方法，感觉方法比较灵活，算是很好的思路","title":"【速览】Search-in-the-Chain：通过搜索知识密集型任务来互动增强大型语言模型","type":"posts"},{"content":"","date":"2025年3月3日","externalUrl":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"RAG","type":"tags"},{"content":"","date":"2025年3月3日","externalUrl":null,"permalink":"/tags/%E5%A4%9A%E8%B7%B3%E9%97%AE%E7%AD%94/","section":"Tags","summary":"","title":"多跳问答","type":"tags"},{"content":"","date":"2025年3月3日","externalUrl":null,"permalink":"/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","section":"Tags","summary":"","title":"论文笔记","type":"tags"},{"content":"","date":"2025年3月2日","externalUrl":null,"permalink":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/","section":"Tags","summary":"","title":"服务器","type":"tags"},{"content":"\r1.基本指令 #\r1.1 文件与目录操作 #\r查看当前目录和文件列表 #\r命令：ls\n示例：\nls # 列出当前目录下的文件和文件夹（颜色区分文件类型） ls -l # 详细列表格式，显示权限、所有者、文件大小、修改时间等信息 ls -a # 显示所有文件，包括隐藏文件（以.开头的文件） ls -lh # 以人类可读格式显示文件大小 切换目录 #\r命令：cd\n示例：\ncd /var/log # 进入 /var/log 目录 cd ~ # 进入当前用户的主目录 cd - # 返回上一个工作目录 cd .. # 进入当前目录的上一级目录 查看当前路径 #\r命令：pwd\n示例：\npwd # 输出当前所在目录的完整路径 创建和删除目录 #\r创建目录：mkdir\n示例：\nmkdir new_folder # 创建一个名为 new_folder 的目录 mkdir -p /tmp/a/b/c # 递归创建多级目录（a、b、c） 删除目录：rmdir 和 rm -r\n示例：\nrmdir empty_folder # 删除空目录 rm -r folder_to_remove # 递归删除目录及其内容（注意：删除操作不可逆，请谨慎使用） 文件复制、移动、跨服务器传输和删除 #\r复制文件或目录：cp\n示例：\ncp source.txt destination.txt # 复制文件 cp -r source_directory/ target_dir/ # 递归复制整个目录 移动或重命名文件/目录：mv\n示例：\nmv old_name.txt new_name.txt # 重命名文件 mv file.txt /path/to/destination/ # 移动文件到指定目录 删除文件：rm\n示例：\nrm file.txt # 删除单个文件 rm -f file.txt # 强制删除文件（不提示确认） rm -rf directory/ # 强制递归删除目录及其内容 文件远程传输：scp\n基本语法\nscp [选项] 源路径 目标路径 示例：\n使用 -r 参数递归复制整个目录\nscp file.txt user@example.com:/home/user/ #从本地复制文件到远程主机 scp -r username@remote_host:/path/to/remote_file /path/to/local_directory/ #从远程主机复制目录到本地 查看和编辑文件内容 #\r查看文件内容：cat、more、less、head、tail\n示例：\ncat file.txt # 显示整个文件内容 more file.txt # 分页显示文件内容 less file.txt # 分页显示，并支持向前翻页 head -n 10 file.txt # 显示文件前10行 tail -n 10 file.txt # 显示文件最后10行 tail -f /var/log/syslog # 实时跟踪日志文件更新 编辑文件：vi、nano、vim\n示例：\nvi file.txt # 使用 vi 编辑文件 nano file.txt # 使用 nano 编辑文件 文件搜索与查找 #\r根据文件名查找：find\n示例：\nfind / -name \u0026#34;config*.txt\u0026#34; # 从根目录开始查找所有以 config 开头的 txt 文件 根据内容搜索：grep\n示例：\ngrep \u0026#34;error\u0026#34; /var/log/syslog # 在 syslog 文件中查找包含 \u0026#34;error\u0026#34; 的行 grep -R \u0026#34;function_name\u0026#34; ./ # 在当前目录递归查找包含 \u0026#34;function_name\u0026#34; 的文件 文件权限与所有者管理 #\r修改权限：chmod\n示例：\nchmod 755 script.sh # 设置文件权限为 rwxr-xr-x chmod u+x script.sh # 仅为当前用户添加可执行权限 修改所有者：chown 和 chgrp\n示例：\nchown user:group file.txt # 同时修改文件的所有者和所属组 chown user file.txt # 修改文件所有者 chgrp group file.txt # 修改文件所属组 2. 网络管理 #\r2.1 网卡相关 #\r网络连接 #\r查看网络接口状态\n命令：ip link show 或 ifconfig（部分系统可能需要安装 net-tools）\n示例：\nip link show # 查看所有网络接口的状态 # 或者 ifconfig -a # 显示所有网络接口（包括未激活的） 激活/禁用网络接口\n使用 ip 命令\n示例：\nsudo ip link set eth0 up # 激活 eth0 接口 sudo ip link set eth0 down # 禁用 eth0 接口 配置静态 IP 地址\n示例（临时配置）：\nsudo ip addr add 192.168.1.100/24 dev eth0 sudo ip route add default via 192.168.1.1 注意：永久配置需要修改对应网络管理配置文件（如 /etc/network/interfaces、/etc/sysconfig/network-scripts/ifcfg-eth0 或 NetworkManager 的配置文件），不同的发行版配置方式不同。\n查看当前网络连接情况\n使用 netstat 或 ss 命令\n示例：\nnetstat -tulnp # 列出所有监听中的 TCP/UDP 端口及其对应的进程（需要 root 权限） # 或者使用 ss 命令（更快更现代） ss -tulnp ip查询 #\r其实ifconfig 就能看到ip信息了\n查看本机IP地址\n命令：ip addr show\n示例：\nip addr show eth0 # 查看指定接口的 IP 地址信息 ip addr show # 查看所有网络接口的 IP 地址信息 查看路由信息\n命令：ip route\n示例：\nip route show # 查看当前路由表信息 域名解析与网络诊断\n使用 ping 测试网络连通性\n示例：\nping www.google.com 使用 traceroute（或 tracepath）追踪网络路径\n示例：\ntraceroute www.google.com # 或者 tracepath www.google.com 2.2 防火墙 #\r防火墙在服务器管理中扮演着重要的安全角色，不同系统和发行版可能默认使用不同的防火墙工具，如 iptables、ufw（Ubuntu常用）、firewalld（CentOS/RHEL常用）。\n查看目前开放端口 #\r使用 iptables 查看规则\n示例：\nsudo iptables -L -n -v 解释：\n-L 列出所有规则 -n 禁止域名解析，加快显示速度 -v 显示详细信息（包括流量统计等） 使用 ufw 查看状态（适用于 Ubuntu 系统）\n示例：\nsudo ufw status verbose 使用 firewall-cmd 查看开放端口（适用于 firewalld 系统，如 CentOS 7/8）\n示例：\nsudo firewall-cmd --list-all 开放指定端口 #\r使用 iptables 开放端口\n示例（开放 TCP 端口 8080）：\nsudo iptables -A INPUT -p tcp --dport 8080 -j ACCEPT # 保存规则（不同系统保存方法不一样，Ubuntu可能需要使用 iptables-persistent 或编写脚本） 使用 ufw 开放端口\n示例：\nsudo ufw allow 8080/tcp sudo ufw reload # 重新加载规则 使用 firewall-cmd 开放端口\n示例：\nsudo firewall-cmd --zone=public --add-port=8080/tcp --permanent sudo firewall-cmd --reload 关闭指定端口 #\r使用 iptables 关闭端口\n示例（关闭 TCP 端口 8080）：\nsudo iptables -D INPUT -p tcp --dport 8080 -j ACCEPT # 或者直接拒绝该端口的访问 sudo iptables -A INPUT -p tcp --dport 8080 -j DROP 注意：iptables 规则不会永久生效，重启后会失效。需要保存规则（不同系统保存方式不同）。\n使用 ufw 关闭端口\n示例（关闭 TCP 端口 8080）：\nsudo ufw deny 8080/tcp sudo ufw reload # 重新加载规则 或者直接删除规则：\nsudo ufw delete allow 8080/tcp 使用 firewall-cmd 关闭端口\n示例（关闭 TCP 端口 8080）：\nsudo firewall-cmd --zone=public --remove-port=8080/tcp --permanent sudo firewall-cmd --reload 3. 磁盘管理 #\r3.1 磁盘空间管理 #\r查看磁盘使用情况\n命令：df\n示例：\ndf -h # 以人类可读格式显示各文件系统的使用情况 查看目录或文件大小\n命令：du\n示例：\ndu -sh /var/log # 显示 /var/log 目录的总大小 du -h --max-depth=1 / # 显示根目录下每个子目录的大小 查看磁盘分区和挂载情况\n命令：lsblk\n示例：\nlsblk # 列出所有块设备及其挂载点 命令：fdisk -l 或 parted -l\n示例：\nsudo fdisk -l 监控磁盘 I/O 状况\n工具：iostat（需要安装 sysstat 软件包）\n示例：\niostat -x 2 # 每2秒刷新一次详细的 I/O 信息 3.2 外部磁盘挂载与卸载 #\r挂载外部磁盘\n查看设备信息与挂载情况\n命令：lsblk 或 fdisk -l\n示例：\nlsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 4K 1 loop /snap/bare/5 loop1 7:1 0 55.4M 1 loop /snap/core18/2846 sda 8:0 0 447.1G 0 disk ├─sda1 8:1 0 512M 0 part /boot/efi └─sda2 8:2 0 446.6G 0 part / sdb 8:16 0 14.6T 0 disk ├─sdb1 8:17 0 3.7T 0 part /data0 ├─sdb2 8:18 0 3.7T 0 part /data1 └─sdb3 8:19 0 7.3T 0 part /data2 创建挂载点\n示例：\nsudo mkdir -p /mnt/external_disk 挂载设备（临时挂载，重启失效）\n示例（假设设备为 /dev/sdb1）：\nsudo mount /dev/sdb1 /mnt/external_disk 如果需要指定文件系统类型或挂载选项，可使用：\nsudo mount -t ext4 -o defaults /dev/sdb1 /mnt/external_disk 自动挂载设置（编辑 /etc/fstab）\n示例：\n在 Linux 中，可以通过编辑 /etc/fstab 文件来设置开机自动挂载某个硬盘到指定目录。以下是具体步骤：\n(1)确定硬盘分区信息\n首先，使用 lsblk 或 fdisk -l 命令查看硬盘分区信息：\nlsblk 或者：\nsudo fdisk -l 找到你要挂载的分区，例如 /dev/sdb1。\n(2)获取分区 UUID（推荐）\n使用 blkid 命令查看该分区的 UUID：\nsudo blkid /dev/sdb1 示例输出：\n/dev/sdb1: UUID=\u0026#34;12345678-abcd-efgh-ijkl-9876543210\u0026#34; TYPE=\u0026#34;ext4\u0026#34; 记录下 UUID 的值（12345678-abcd-efgh-ijkl-9876543210）。\n(3)创建挂载目录\n决定要挂载的目录，比如 /mnt/mydisk：\nsudo mkdir -p /mnt/mydisk (4)编辑 /etc/fstab\n使用文本编辑器（如 vim 或 nano）打开 /etc/fstab：\nsudo vi /etc/fstab 在文件末尾添加一行（注意，有的磁盘文件系统类型是xfs的，刚才blkid命令可以看到，就把下面ext4改成xfs）：\nUUID=12345678-abcd-efgh-ijkl-9876543210 /mnt/mydisk ext4 defaults 0 2 UUID：替换为你实际的 UUID（推荐使用 UUID，而非设备路径 /dev/sdb1，以防设备名变化）。 /mnt/mydisk：你要挂载的目录。 ext4：文件系统类型（如果是 NTFS、xfs、btrfs 等，需修改）。 defaults：默认挂载选项（rw,relatime 等）。 0 2： 第一个 0：是否需要 dump 备份（一般设为 0）。 第二个 2：文件系统检查顺序（/ 根目录通常为 1，其他磁盘为 2）。 (5)测试挂载\n运行以下命令测试挂载：\nsudo mount -a 如果没有报错，则配置正确。\n(6)重启验证\nsudo reboot 重启后，使用 df -h 或 mount | grep /mnt/mydisk 检查是否自动挂载成功：\ndf -h mount | grep /mnt/mydisk 这样，每次开机时，系统都会自动挂载该硬盘到指定目录。\n/dev/sdb1 /mnt/external_disk ext4 defaults 0 2 卸载外部磁盘\n命令：umount\n示例：\nsudo umount /mnt/external_disk # 或者通过设备名卸载 sudo umount /dev/sdb1 注意：卸载前请确保没有进程正在使用挂载目录，可以使用 lsof /mnt/external_disk 查看。\n4. 其他问题 #\r看看这里面写了么：\rServer Management Notes（作者：abel）\n","date":"2025年3月2日","externalUrl":null,"permalink":"/posts/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","section":"文章","summary":"实验室服务器维护常用命令记录（Linux系统下），主要包括网络管理与磁盘管理的常用操作，后续也可能更新补充","title":"服务器管理操作笔记","type":"posts"},{"content":"\r原文题目：Tan 等 - 2024 - Small Models, Big Insights Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLM\n原文链接：\rSmall Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs - ACL Anthology\n1. 背景与当前问题 #\r在当前开放域问答场景下，虽然大语言模型（LLM）具备强大的生成能力，但在某些情况下直接调用模型回答可能会存在两个问题：\n干扰信息的引入\n当LLM能够独立回答问题时，额外的检索步骤可能会引入无关信息，从而降低最终答案的质量。\n检索时机和内容的选择\n如何在合适的时机、针对性地进行检索，是目前亟待解决的挑战。现有的解决方案多依赖：\n微调方法（如Self-RAG）：需要大量计算资源，并可能引发灾难性遗忘； 多次推理策略（如SKR）：在评估生成内容质量的过程中增加了推理成本和响应延迟。 通过对ELI5数据集的分析，论文展示了在不同场景下检索对回答质量的影响，从而揭示了问题的实际复杂性。\n图1 失败检索影响推理的例子与统计数据\r2. 解决思路与理论依据 #\r论文的核心创新在于提出利用参数较小的代理模型辅助大参数LLM进行检索决策。\n理论依据 #\r众所周知，大模型因为庞大的参数量，推理需要的计算成本还是挺高的，并且参数量越大的LLM，在达到更好性能的同时其推理成本也随之增高，一个大模型系列往往会推出不同参数量级的模型以满足不同计算资源条件下的推理需要。而现有的解码器类型语言模型通常共享相似的 Transformer 结构，并且在共同的文本语料上进行预训练，如Common Crawl、书籍和维基百科。理论上这些模型在不同知识领域的掌握程度和检索需求上能够达成一定程度的共识。\n表1 Llama 3.1系列不同量级模型的训练数据对比\r以此，作者提出猜想：能否使用参数较小的LLM来辅助大参数LLM进行检索？如果小参数LLM的代理模型能够正确回答某个问题，那么大参数LLM很有可能也能正确回答该问题，代理模型生成的启发式答案可以为 LLMs 提供关于其知识能力的线索。\n实验验证 #\r作者通过代理模型与LLM的一致性实验验证了上面的猜想。\n图2 EM分数高于特定值的样本在总体中的比例\r在ASQA数据集上，70B与7B语言模型在EM得分大于0.5的样本上差异微小，差异主要体现在EM得分小于0.5的样本上。\n7B模型和70B模型在EM得分大于0.5的样本中重叠超过82.19%。这表明，代理模型可以正确回答的问题，LLM也很可能能够正确回答。\n3. 具体方法 #\r本节详细介绍论文提出的多模块协同检索生成框架，阐述如何利用小模型（代理模型）生成启发式答案，再通过一系列判断和重写步骤确定检索需求，最后辅以大模型生成高质量答案。其具体流程如下图：\n图3 SlimPLM的具体流程\r3.1 启发式答案生成 #\r流程： 直接输入用户问题 x，通过低参数量的代理模型 PM（Proxy Model）生成一个初步的“启发式答案” $\\hat{a}$ ̂,即： $$ \\hat{a} = 𝑃𝑀(𝑥) $$\n目的：\n判断该问题是否需要进一步检索； 为后续查询重写提供线索。 3.2 检索必要性判断 #\r使用模型： 预训练的检索必要性判断模型（RJ）。\n输入与输出： 输入为用户问题 $x$ 与启发式答案 $\\hat{a}$，输出为二分类结果：\n若答案质量高，则认为LLM可直接回答； 若质量低，则需要触发检索流程。 训练策略：\n利用现有问答数据集的简短答案样本，使用启发式答案与真实答案之间的匹配比例作为指标，即对于启发式答案 $\\hat{a}$ 和真实答案候选集合 $Y={y_1,y_2,…y_n }$，计算: $$ r = \\frac{|{y|y\\in\\hat{a}∧ y\\in Y}|}{|Y|} $$\n当 $r$ 大于规定的阈值 $\\theta$ 时，标记为正 （Known (True)），反之，标签为负（Known (False)）。\n为了确保数据的平衡性，对标签为负的样本进行欠采样，使其与标签为正的样本数量大致相等、\n图4 RJ训练数据构造使用的Prompt\r3.3 启发式查询重写与查询过滤 #\r当检索必要性判断模型认为启发式答案质量不足时，需要触发后续的检索流程。为确保检索的精准性，系统采用**查询重写模型（QR）**对原始问题和启发式答案进行进一步处理。具体操作如下：\n将3.1中PM生成的启发式答案分解为 n 个“陈述”$ {c_1,c_2,…,c_n}$ 根据这 n 个陈述，通过QR模型改写生成 n 个相对应的查询 ${q_{(c_1)},q_{(c_2)},…,q_{(c_n)}}$ 用户问题 x 直接派生n 个查询$ {q_{(x_1)},q_{(x_2)},…,q_{(x_n)}}$ （这一部分在文章后续就没出现过了？） 在获取n个陈述与其对应的n个查询后， 再次利用上一步的检索必要性判断模型（RJ），来评估每个重写查询是否需要检索，过滤掉不需要检索的查询。作者希望通过这样的方法，对LLM针对用户问题掌握的知识进行更精细化的区分与鉴别。\n模型训练 #\r这里QR模型的训练是通过GPT4进行数据生成然后进行指令微调完成的。\n图5 QR训练数据构造使用的Prompt\r3.4 问题检索与最终答案生成 #\r针对性知识检索：\n将上一步筛选出的需要检索的查询交由检索模型 R 进行检索，确定LLM缺失知识的文档集合 $D_{ref}$。\n整合检索结果： 将从检索模块获得的文档集合 $D_{ref}$ 作为上下文，连同原始用户问题一起输入大参数LLM，生成最终答案。\n这种方法实现了通过小模型先行判断，再针对性地调用检索，最终由大模型生成高质量答案的目标，既降低了推理成本，又提高了回答的准确性。\n4. 实验设计与结果 #\r数据集与评价指标 #\r论文在多个数据集上进行了实验，包括：\n数据集（随机抽取 400 个问题）：\nNatural Questions (NQ)：由谷歌创建的单跳开放域QA数据集，答案分为长答案和短答案两种形式。 Trivia-QA：斯坦福大学开发的单跳开放域QA数据集，包含多个候选答案。 ASQA：专注于模糊事实性问题的长篇问答数据集。每个问题都标注了长篇答案和可从生成文本中提取的问题-答案对，需要整合来自不同来源的事实信息的答案 MuSiQue：通过将多个单跳问题合成多跳问题来评估模型的推理能力的多跳QA数据集 ELI5：源自 Reddit 论坛的长格式问答数据集 评价指标：\n精确匹配(Exact Match，EM)：测试生成的答案包含正确的目标答案的比例。 Hit@1：如果模型生成答案包含至少一个正确答案，则Hit@1为 1，否则为 0。 Rouge-N ：用于衡量模型生成摘要文本的质量。统计了预测值和标准值两个字符串的n-gram重叠单元的数量，并计算了重叠单元在标准字符串中的占比，作为召回率。 实验设置 #\r生成器：\n使用 Llama2-70B-Chat 和 Qwen-72B-Chat 检索器：\n采用 BM25 检索和 E5base 重排 代理模型：\n使用 Llama2-7B-Chat 作为代理和插件模型 实验结果 #\r表2 主实验结果\r检索增强生成方法相比于不使用检索的方法具有明显优势。 与其他RAG方法相比，SlimPLM在准确性和检索参考文献的相关性上更具优势。 更为显著的是，该方法仅需一次大模型的推理调用，从而大大降低了计算成本。 5. 消融实验 #\r表3 消融实验结果\r为验证各模块的重要性，论文进行了详细的消融实验，主要结论如下：\n去除查询重写（w/o QR）：\n检索必要性判断仅在普通对话和直接RAG间切换，查询重写能增强检索参考文献的全面性和相关性。\n去除检索必要性判断（w/o RJ）：\n所有问题均触发检索，导致在LLM本可直接回答的问题上出现偏差。\n去除基于声明的查询过滤（w/o QF）：\n不过滤不需要检索的内容会使得检索结果的质量下降。\n以上实验结果充分证明了各个模块在整体框架中的必要性与有效性，这性能差距的计算方法充分体现了数字统计的艺术。\n6. 推理成本与局限性 #\r推理成本计算 #\r表4 针对每种数据集SlimPLM不同组件消耗的Token数量统计\r论文通过对比小模型与大模型的计算资源消耗（假设7B模型每个 token 的开销约为70B模型的1/10），得出附加成本仅为单次大模型推理成本的1/4至1/3，从而实现了经济高效的推理过程。\n局限性 #\r尽管方法表现出色，但也存在一定的局限性：\n适用场景限制：\n当用户问题超出LLM预训练语料范围或几乎不需要外部知识时，SlimPLM的方法优势不明显。\n知识能力差异：\n代理模型与大模型之间的知识水平存在差距，启发式答案可能不能完全反映大模型的真实知识能力，进而影响检索判断的准确性。\n方法复杂性：\n多模型的集成流程较为复杂，实际部署和维护时可能面临一定挑战，未来的研究可尝试将功能集成至单一生成框架中。\n7. 总结与讨论 #\r论文的主要创新点在于：\n提出使用小参数LLM输出模拟大参数LLM输出进行评估的方法，以一种低成本方法确定检索的时间和方式来辅助判断模型输出时是否需要检索； 设计基于启发式答案的检索必要性判断模型，有效降低了大模型的调用次数； 提出基于陈述分解的查询重构策略，有针对性地过滤出缺失知识的部分。 其实读完整篇论文，不难发现，这篇论文的讨论基础即不能把7B、8B之类的LLM当作大模型来看。当接受这个前提背景设置后，本文确实提出了一个合理的假设与一个逻辑通顺的解决方法，结合高质量的图表和文本，确实符合ACL的调性。\n另外我们也可以发现，在大小模型协同的RAG模型框架论文里，效率与成本往往是作者关注的性能优势。其实反过来一想，现在主流的LLM都是“通才模型“，也就是说，你提示词设置好，基本上很多方法的子模型对应的子任务都能让LLM直接解决。那如何与之前的那种单纯的提示工程方法做出差异与性能优势，是大小模型协同的RAG模型框架需要考虑的问题 。\n","date":"2025年2月26日","externalUrl":null,"permalink":"/posts/slimplm%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"241218组会报告文字版补档，使用小型LLM辅助进行检索判断的RAG框架","title":"【组会】SlimPLM：利用小型代理模型来决定大语言模型检索的时间和内容","type":"posts"},{"content":"看多跳RAG论文少不了这三个数据集的身影，基本上是多跳RAG必测的三个数据集，由于不同方法的测试标准也不一样，直接了解这三个数据集本来的设计思路和数据格式还是很必要的，便于后面的数据处理和评估设计。\n训练集、验证集和测试集 #\r在机器学习和深度学习中，数据集通常分为三种类型：训练集（train）、验证集（dev）和测试集（test）。它们各自的作用和特点如下：\n训练集（Train Dataset）：\n用途：用于训练模型，帮助模型学习数据的特征。 特征：包含大量的样本，通常是整个数据集中最大的部分。 目标：通过调整模型参数，使得模型在该数据集上的表现达到最佳。 验证集（Dev Dataset）：\n用途：用于调整模型的超参数和进行模型选择，帮助评估模型的泛化能力。 特征：一般较小于训练集，但在模型训练过程中多次使用。 目标：提供模型在未见数据上的表现，以防止过拟合，并帮助选择最佳模型结构或超参数。 测试集（Test Dataset）：\n用途：用于评估最终模型的性能，给出对新数据的真实表现。 特征：通常与训练集和验证集完全独立，且仅在模型训练结束后使用。 目标：评估模型在真实应用场景中的表现，提供模型的泛化能力和有效性指标。 总结 #\r训练集：用来训练模型。 验证集：用来调优模型。 测试集：用来最终评估模型性能。 这种划分有助于确保模型的学习和评估是公正和有效的，减少对模型性能的偏见。\nHotPotQA： 用于多样化、可解释的多跳问题解答的数据集 #\r论文题目：Yang 等 - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi-hop Question Answering\n论文链接：[\r1809.09600] HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering (arxiv.org)\n项目地址：\rHotpotQA Homepage\n构建思路 #\rHotPotQA是2018年由卡内基梅隆大学发布的用于多样化、可解释的多跳问题解答的数据集。\n数据格式 #\r训练集格式 #\rhotpot_train_v1.1.json\n{ \u0026#34;supporting_facts\u0026#34;: [ [\u0026#34;str(title)\u0026#34;, \u0026#34;int(sent_id)\u0026#34;] // 支持文档的标题和对应句子的序号（第几句） // ... ], \u0026#34;level\u0026#34;: [ // 以下三种之一作为值。 \u0026#34;easy\u0026#34;, \u0026#34;hard\u0026#34;, \u0026#34;medium\u0026#34; ], \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, // 问题内容 \u0026#34;context\u0026#34;: [ [ \u0026#34;str(Title)\u0026#34;, // 文档标题 [ \u0026#34;str(Sent)\u0026#34;, // 句子内容 \u0026#34;str(Sent)\u0026#34; // 句子内容 // ... ] ] // ... ], \u0026#34;answer\u0026#34;: \u0026#34;str\u0026#34;, // 这里应该是一个字符串，代表答案内容 \u0026#34;_id\u0026#34;: \u0026#34;str\u0026#34;, // 这里应该是一个唯一的字符串标识符 \u0026#34;type\u0026#34;: [ // 以下两种之一作为值。 \u0026#34;bridge\u0026#34;, \u0026#34;comparison\u0026#34; ] } 下面是对各个字段的解释：\nsupporting_facts:\n类型: List of Lists，包含数个字符串列表，每个字符串列表由一个字符串和一个整数构成。 含义: 支持该问题的事实。包含支持问题答案相关事实的文档题目（title）和相关事实的句子在文档中对应的定位（sent_id）。 level:\n类型: List of Strings，可能的取值为 \u0026quot;easy\u0026quot;, \u0026quot;hard\u0026quot;, \u0026quot;medium\u0026quot;。 含义: 问题的难度级别。它表示该问题的难度，可以是“easy”（简单）、“hard”（困难）或“medium”（中等）。 question:\n类型: String。 含义: 问题文本。这是实际提出的多跳问题的内容。 context:\n类型: List of Lists，包含数个字符串列表（应该是10个）。 含义: 可能能够提供上下文信息的文档或句子。信息由几个能够提供关键信息的文档和其他干扰文档组成。每个列表有一个标题（Title）和数个句子（Sent）。 answer:\n类型: String。 含义: 问题的答案。这是根据给定的上下文回答问题的答案。 _id:\n类型: String。 含义: 唯一标识符。用于唯一标识该条问答数据。 type:\n类型: List of Strings，可能的取值为 \u0026quot;bridge\u0026quot;, \u0026quot;comparison\u0026quot;。 含义: 问题的类型。它可以是两种类型之一： \u0026quot;bridge\u0026quot;：表示需要跨越多个上下文信息来连接事实并推导出答案。 \u0026quot;comparison\u0026quot;：表示需要对比多个事实来得出答案。 示例：\n{ \u0026#34;supporting_facts\u0026#34;: [ [ \u0026#34;Allie Goertz\u0026#34;, 0 ], [ \u0026#34;Allie Goertz\u0026#34;, 1 ], [ \u0026#34;Allie Goertz\u0026#34;, 2 ], [ \u0026#34;Milhouse Van Houten\u0026#34;, 0 ] ], \u0026#34;level\u0026#34;: \u0026#34;hard\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;Musician and satirist Allie Goertz wrote a song about the \\\u0026#34;The Simpsons\\\u0026#34; character Milhouse, who Matt Groening named after who?\u0026#34;, \u0026#34;context\u0026#34;: [ [ \u0026#34;Lisa Simpson\u0026#34;, [ \u0026#34;Lisa Marie Simpson is a fictional character in the animated television series \\\u0026#34;The Simpsons\\\u0026#34;.\u0026#34;, \u0026#34; She is the middle child and most intelligent of the Simpson family.\u0026#34;, \u0026#34; Voiced by Yeardley Smith, Lisa first appeared on television in \\\u0026#34;The Tracey Ullman Show\\\u0026#34; short \\\u0026#34;Good Night\\\u0026#34; on April 19, 1987.\u0026#34;, \u0026#34; Cartoonist Matt Groening created and designed her while waiting to meet James L. Brooks.\u0026#34;, \u0026#34; Groening had been invited to pitch a series of shorts based on his comic \\\u0026#34;Life in Hell\\\u0026#34;, but instead decided to create a new set of characters.\u0026#34;, \u0026#34; He named the elder Simpson daughter after his younger sister Lisa Groening.\u0026#34;, \u0026#34; After appearing on \\\u0026#34;The Tracey Ullman Show\\\u0026#34; for three years, the Simpson family were moved to their own series on Fox, which debuted on December 17, 1989.\u0026#34; ] ], [ \u0026#34;Marge Simpson\u0026#34;, [ \u0026#34;Marjorie Jacqueline \\\u0026#34;Marge\\\u0026#34; Simpson (n\\u00e9e Bouvier) is a fictional character in the American animated sitcom \\\u0026#34;The Simpsons\\\u0026#34; and part of the eponymous family.\u0026#34;, \u0026#34; She is voiced by Julie Kavner and first appeared on television in \\\u0026#34;The Tracey Ullman Show\\\u0026#34; short \\\u0026#34;Good Night\\\u0026#34; on April 19, 1987.\u0026#34;, \u0026#34; Marge was created and designed by cartoonist Matt Groening while he was waiting in the lobby of James L. Brooks\u0026#39; office.\u0026#34;, \u0026#34; Groening had been called to pitch a series of shorts based on \\\u0026#34;Life in Hell\\\u0026#34; but instead decided to create a new set of characters.\u0026#34;, \u0026#34; He named the character after his mother Margaret Groening.\u0026#34;, \u0026#34; After appearing on \\\u0026#34;The Tracey Ullman Show\\\u0026#34; for three seasons, the Simpson family received their own series on Fox, which debuted December 17, 1989.\u0026#34; ] ], [ \u0026#34;Bart Simpson\u0026#34;, [ \u0026#34;Bartholomew JoJo \\\u0026#34;Bart\\\u0026#34; Simpson is a fictional character in the American animated television series \\\u0026#34;The Simpsons\\\u0026#34; and part of the Simpson family.\u0026#34;, \u0026#34; He is voiced by Nancy Cartwright and first appeared on television in \\\u0026#34;The Tracey Ullman Show\\\u0026#34; short \\\u0026#34;Good Night\\\u0026#34; on April 19, 1987.\u0026#34;, \u0026#34; Cartoonist Matt Groening created and designed Bart while waiting in the lobby of James L. Brooks\u0026#39; office.\u0026#34;, \u0026#34; Groening had been called to pitch a series of shorts based on his comic strip, \\\u0026#34;Life in Hell\\\u0026#34;, but instead decided to create a new set of characters.\u0026#34;, \u0026#34; While the rest of the characters were named after Groening\u0026#39;s family members, Bart\u0026#39;s name is an anagram of the word \\\u0026#34;brat\\\u0026#34;.\u0026#34;, \u0026#34; After appearing on \\\u0026#34;The Tracey Ullman Show\\\u0026#34; for three years, the Simpson family received its own series on Fox, which debuted December 17, 1989.\u0026#34; ] ], [ \u0026#34;Allie Goertz\u0026#34;, [ \u0026#34;Allison Beth \\\u0026#34;Allie\\\u0026#34; Goertz (born March 2, 1991) is an American musician.\u0026#34;, \u0026#34; Goertz is known for her satirical songs based on various pop culture topics.\u0026#34;, \u0026#34; Her videos are posted on YouTube under the name of Cossbysweater.\u0026#34;, \u0026#34; Subjects of her songs have included the film \\\u0026#34;The Room\\\u0026#34;, the character Milhouse from the television show \\\u0026#34;The Simpsons\\\u0026#34;, and the game Dungeons \u0026amp; Dragons.\u0026#34;, \u0026#34; Her style has been compared to that of Bo Burnham.\u0026#34;, \u0026#34; In December 2015, Goertz released a concept album based on the Adult Swim series \\\u0026#34;Rick and Morty\\\u0026#34;, \\\u0026#34;Sad Dance Songs\\\u0026#34;, with the album\u0026#39;s cover emulating the animation and logo of the series.\u0026#34;, \u0026#34; The album was made possible through Kickstarter.\u0026#34;, \u0026#34; She is co-host of Everything\u0026#39;s Coming Up Podcast, a Simpsons-focused podcast along with Julia Prescott.\u0026#34; ] ], [ \u0026#34;Milhouse Van Houten\u0026#34;, [ \u0026#34;Milhouse Mussolini van Houten is a fictional character featured in the animated television series \\\u0026#34;The Simpsons\\\u0026#34;, voiced by Pamela Hayden, and created by Matt Groening who named the character after President Richard Nixon\u0026#39;s middle name.\u0026#34;, \u0026#34; Later in the series, it is revealed that Milhouse\u0026#39;s middle name is \\\u0026#34;Mussolini.\\\u0026#34;\u0026#34; ] ], [ \u0026#34;Los Angeles Reader\u0026#34;, [ \u0026#34;Los Angeles Reader was a weekly paper established in 1978 and distributed in Los Angeles, United States.\u0026#34;, \u0026#34; It followed the format of the (still active) Chicago Reader.\u0026#34;, \u0026#34; The paper was known for having lengthy, thoughtful reviews of movies, plays and concerts in the LA area.\u0026#34;, \u0026#34; James Vowell was its founding editor.\u0026#34;, \u0026#34; Among its writers were Keith Fitzgerald, Nigey Lennon, Lionel Rolfe, Lawrence Wechsler, Mick Farren, Richard Meltzer, Heidi Dvorak, Chris Morris, Jerry Stahl, Steven Kane, Andy Klein, Allen Levy, Jim Goad, Kirk Silsbee, Henry Sheehan, Samantha Dunn, Natalie Nichols, Steve Appleford, Eric Mankin (also editor), Paul Birchall, Eddie Rivera (who wrote the paper\u0026#39;s first cover story), Amy Steinberg, Harry Sheehan, Dan Sallit, Myron Meisel, David Ehrenstein.\u0026#34;, \u0026#34; Tom Davis, Bruce Bebb, Stuart Goldman, Ernest Hardy, Kevin Uhrich, Erik Himmelsbach and David L. Ulin.\u0026#34;, \u0026#34; It is famous for being the first newspaper to publish Matt Groening\u0026#39;s cartoon strip, Life in Hell on April 25, 1980.\u0026#34;, \u0026#34; James Vowell hired Matt Groening as his assistant editor in 1979.\u0026#34;, \u0026#34; Groening was also originally a Reader music critic.\u0026#34;, \u0026#34; It also ran a cartoon strip by David Lynch (director of Blue Velvet) called The Angriest Dog in the World, a strip notable for having exactly the same drawing panels for its entire run.\u0026#34;, \u0026#34; James Vowell and his wife Codette Wallace bought the Reader from the Chicago Reader in February 1989.\u0026#34;, \u0026#34; They sold \\\u0026#34;The Reader\\\u0026#34; to New Times Media in 1996, which merged it with the \\\u0026#34;Los Angeles View\\\u0026#34; to form \\\u0026#34;New Times LA\\\u0026#34;.\u0026#34; ] ], [ \u0026#34;Homer Simpson\u0026#34;, [ \u0026#34;Homer Jay Simpson is a fictional character and the main protagonist of the American animated television series \\\u0026#34;The Simpsons\\\u0026#34; as the patriarch of the eponymous family.\u0026#34;, \u0026#34; He is voiced by Dan Castellaneta and first appeared on television, along with the rest of his family, in \\\u0026#34;The Tracey Ullman Show\\\u0026#34; short \\\u0026#34;Good Night\\\u0026#34; on April 19, 1987.\u0026#34;, \u0026#34; Homer was created and designed by cartoonist Matt Groening while he was waiting in the lobby of James L. Brooks\u0026#39; office.\u0026#34;, \u0026#34; Groening had been called to pitch a series of shorts based on his comic strip \\\u0026#34;Life in Hell\\\u0026#34; but instead decided to create a new set of characters.\u0026#34;, \u0026#34; He named the character after his father, Homer Groening.\u0026#34;, \u0026#34; After appearing for three seasons on \\\u0026#34;The Tracey Ullman Show\\\u0026#34;, the Simpson family got their own series on Fox that debuted December 17, 1989.\u0026#34; ] ], [ \u0026#34;List of The Simpsons video games\u0026#34;, [ \u0026#34;\\\u0026#34;The Simpsons\\\u0026#34; is an American animated television sitcom created by Matt Groening for the Fox Broadcasting Company.\u0026#34;, \u0026#34; The series is a satirical parody of a middle class American lifestyle epitomized by its eponymous family, which consists of Homer, Marge, Bart, Lisa and Maggie.\u0026#34;, \u0026#34; It is set in the fictional town of Springfield, and lampoons American culture, society and television, and many aspects of the human condition.\u0026#34;, \u0026#34; The family was conceived by Groening shortly before a pitch for a series of animated shorts with producer James L.\\u00a0Brooks.\u0026#34;, \u0026#34; Groening created a dysfunctional family and named the characters after members of his own family, substituting Bart for his own name.\u0026#34;, \u0026#34; The shorts became a part of \\\u0026#34;The Tracey Ullman Show\\\u0026#34; on April 19, 1987 and after a three-season run, the sketch was developed into a half-hour prime time show and became a hit series for Fox.\u0026#34;, \u0026#34; The growing popularity of the series motivated video game developers to create video games based on the series.\u0026#34;, \u0026#34; Two pinball machines have also been produced; one self-titled, that was only made available for a limited time after the first season finale (1990) and \\\u0026#34;The Simpsons Pinball Party\\\u0026#34; (2003).\u0026#34;, \u0026#34; Additionally, several handheld device games have been released, such as \\\u0026#34;Bartman: Avenger of Evil\\\u0026#34; (1990) and \\\u0026#34;Bart Simpson\u0026#39;s Cupcake Crisis\\\u0026#34; (1991).\u0026#34; ] ], [ \u0026#34;The Simpsons: An Uncensored, Unauthorized History\u0026#34;, [ \u0026#34;The Simpsons: An Uncensored, Unauthorized History is a non-fiction book about the American animated television series \\\u0026#34;The Simpsons\\\u0026#34;.\u0026#34;, \u0026#34; It was written by John Ortved, and first published in October 2009 by Faber and Faber.\u0026#34;, \u0026#34; In the United Kingdom, the book is called Simpsons Confidential: The uncensored, totally unauthorised history of the world\u0026#39;s greatest TV show by the people that made it.\u0026#34;, \u0026#34; The book is an oral history of the show, and concentrates particularly on the writers and producers of the show.\u0026#34;, \u0026#34; The book includes entire chapters devoted to key figures such as creator Matt Groening and James L. Brooks and Sam Simon, who helped develop the series.\u0026#34;, \u0026#34; According to National Public Radio reviewer Linda Holmes, \\\u0026#34;Ortved\u0026#39;s thesis, essentially, is that lots of people are responsible for the success of \\\u0026#34;The Simpsons\\\u0026#34;, and their creator, Matt Groening, has too often been viewed as the sole source to the detriment of others who also deserve to be praised.\\\u0026#34;\u0026#34; ] ], [ \u0026#34;List of The Simpsons guest stars\u0026#34;, [ \u0026#34;In addition to the show\u0026#39;s regular cast of voice actors, celebrity guest stars have been a staple of \\\u0026#34;The Simpsons\\\u0026#34;, an American animated television sitcom created by Matt Groening for the Fox Broadcasting Company, since its first season.\u0026#34;, \u0026#34; \\\u0026#34;The Simpsons\\\u0026#34; focuses on the eponymous family, which consists of Homer, Marge, Bart, Lisa and Maggie.\u0026#34;, \u0026#34; The family was initially conceived by Groening for a series of animated shorts, which originally aired as a part of \\\u0026#34;The Tracey Ullman Show\\\u0026#34; between 1987 and 1989.\u0026#34;, \u0026#34; The shorts were developed into a half-hour prime time series which began in December 1989.\u0026#34;, \u0026#34; The series\u0026#39; 27th season began in September 2015 and episodes of \\\u0026#34;The Simpsons\\\u0026#34; have aired.\u0026#34;, \u0026#34; A feature film adaptation of the series called \\\u0026#34;The Simpsons Movie\\\u0026#34;, was released in 2007.\u0026#34; ] ] ], \u0026#34;answer\u0026#34;: \u0026#34;President Richard Nixon\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5a8d7341554299441c6b9fe5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34; } 验证集格式 #\rhotpot_dev_fullwiki_v1.json、hotpot_dev_distractor_v1.json\n验证集分成两种，distractor版的content字段由两个关键文档和八个干扰文档组成。full_wiki版更加困难，content字段不一定包含关键文档（所有这个字段其实没什么用），要求模型从全部的维基百科的第一段中检索并找到关键信息，然后回答问题。\n测试集格式 #\rhotpot_test_fullwiki_v1.json\n测试集只由问题和full_wiki版的上下文（就是没什么卵用）组成。为了保证公平性，答案不公开，需要在\rHotpotQA Homepage 上将模型和其输出结果进行上传获得最终的评估分数。\n格式：\n[ { \u0026#34;_id\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;context\u0026#34;: [ [ \u0026#34;str(Title)\u0026#34;, // 文档标题 [ \u0026#34;str(Sent)\u0026#34;, // 句子内容 \u0026#34;str(Sent)\u0026#34; // 句子内容 // ... ] ] // ... ] } ] 开放域问答 #\r可以在此页面下载作者团队处理过的WiKi离线语料库：\rHotpotQA Homepage\n2WikiMultiHopQA：用于全面评估推理步骤的多跳问答数据集 #\r论文题目：Ho 等 - 2020 - Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps\n论文链接：\rConstructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps - ACL Anthology\n项目地址：\rAlab-NII/2wikimultihop\n构建思路 #\r​\t有研究表明：HotpotQA 中的许多示例不需要多跳推理即可解决。\n​\t为此，作者在每个样本中引入了新信息，即包含全面而简洁的信息来解释预测的证据。本数据集中的证据信息是一组三元组，其中每个三元组都是从 Wikidata 获得的结构化数据（主题实体、属性、对象实体），作者认为这样可用于解释预测并测试模型的推理和推理技能。\n图 1：数据集中的一个推理问题示例。其与 HotpotQA 之间的区别在于解释推理路径的证据信息。\r数据格式 #\r训练集/验证集 #\rtrain.json、dev.json\n{ \u0026#34;_id\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;type\u0026#34;: [\t// 以下四种之一作为值。 \u0026#34;compositional\u0026#34;, \u0026#34;inference\u0026#34;, \u0026#34;bridge_comparison\u0026#34;, \u0026#34;comparison\u0026#34; ], \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;context\u0026#34;: [ [ \u0026#34;str(Title)\u0026#34;, // 文档标题 [ \u0026#34;str(Sent)\u0026#34;, // 句子内容 \u0026#34;str(Sent)\u0026#34; // 句子内容 // ... ] ] // ... ], \u0026#34;supporting_facts\u0026#34;: [ [\u0026#34;str(title)\u0026#34;, \u0026#34;int(sent_id)\u0026#34;] // 支持文档的标题和对应句子的序号（第几句） // ... ], \u0026#34;evidences\u0026#34;: [ [\u0026#34;str(subject entity)\u0026#34;, \u0026#34;str(relation)\u0026#34;, \u0026#34;str(object entity)\u0026#34;] // ... ], \u0026#34;answer\u0026#34;: \u0026#34;str\u0026#34; } _id: 每个样本的唯一标识 question: 字符串 answer: 问题的答案（测试数据中无此信息） supporting_facts: 列表，每个元素包含[标题, 句子id]，标题是段落标题，句子id是模型使用的句子的索引（从0开始）（测试数据中无此信息） context: 列表，每个元素包含[标题, 句子]，句子是一个句子列表 evidences: 列表，每个元素是一个包含[主体实体, 关系, 客体实体]的三元组,有几组supporting_facts就有几组这个（测试数据中无此信息） type: 字符串，问题类型有：比较、推理、组合和桥接比较 entity_ids: 字符串，包含两个Wikidata id（桥接比较问题为四个），例如 \u0026lsquo;Q7320430_Q51759\u0026rsquo;（数据里面没找到啊？） 示例\n{ \u0026#34;_id\u0026#34;: \u0026#34;13f5ad2c088c11ebbd6fac1f6bf848b6\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge_comparison\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;Are director of film Move (1970 Film) and director of film M\\u00e9diterran\\u00e9e (1963 Film) from the same country?\u0026#34;, \u0026#34;context\u0026#34;: [ [ \u0026#34;Stuart Rosenberg\u0026#34;, [ \u0026#34;Stuart Rosenberg (August 11, 1927 \\u2013 March 15, 2007) was an American film and television director whose motion pictures include \\\u0026#34;Cool Hand Luke\\\u0026#34; (1967), \\\u0026#34;Voyage of the Damned\\\u0026#34; (1976), \\\u0026#34;The Amityville Horror\\\u0026#34; (1979), and \\\u0026#34;The Pope of Greenwich Village\\\u0026#34; (1984).\u0026#34;, \u0026#34;He was noted for his work with actor Paul Newman.\u0026#34; ] ], [ \u0026#34;M\\u00e9diterran\\u00e9e (1963 film)\u0026#34;, [ \u0026#34;M\\u00e9diterran\\u00e9e is a 1963 French experimental film directed by Jean-Daniel Pollet with assistance from Volker Schl\\u00f6ndorff.\u0026#34;, \u0026#34;It was written by Philippe Sollers and produced by Barbet Schroeder, with music by Antione Duhamel.\u0026#34;, \u0026#34;The 45 minute film is cited as one of Pollet\u0026#39;s most influential films, which according to Jonathan Rosenbaum directly influenced Jean-Luc Goddard\u0026#39;s \\\u0026#34;Contempt\\\u0026#34;, released later the same year.\u0026#34;, \u0026#34;Footage for the film was shot around the Mediterranean, including at a Greek temple, a Sicilian garden, the sea, and also features a fisherman, a bullfighter, and a girl on an operating table.\u0026#34; ] ], [ \u0026#34;Move (1970 film)\u0026#34;, [ \u0026#34;Move is a 1970 American comedy film starring Elliott Gould, Paula Prentiss and Genevi\\u00e8ve Wa\\u00efte, and directed by Stuart Rosenberg.\u0026#34;, \u0026#34;The screenplay was written by Joel Lieber and Stanley Hart, adapted from a novel by Lieber.\u0026#34; ] ], [ \u0026#34;Ian Barry (director)\u0026#34;, [ \u0026#34;Ian Barry is an Australian director of film and TV.\u0026#34; ] ], [ \u0026#34;Peter Levin\u0026#34;, [ \u0026#34;Peter Levin is an American director of film, television and theatre.\u0026#34; ] ], [ \u0026#34;Brian Johnson (special effects artist)\u0026#34;, [ \u0026#34;Brian Johnson( born 1939 or 1940) is a British designer and director of film and television special effects.\u0026#34; ] ], [ \u0026#34;Rachel Feldman\u0026#34;, [ \u0026#34;Rachel Feldman( born August 22, 1954) is an American director of film and television and screenwriter of television films.\u0026#34; ] ], [ \u0026#34;Hanro Smitsman\u0026#34;, [ \u0026#34;Hanro Smitsman, born in 1967 in Breda( Netherlands), is a writer and director of film and television.\u0026#34; ] ], [ \u0026#34;Jean-Daniel Pollet\u0026#34;, [ \u0026#34;Jean-Daniel Pollet (1936\\u20132004) was a French film director and screenwriter who was most active in the 1960s and 1970s.\u0026#34;, \u0026#34;He was associated with two approaches to filmmaking: comedies which blended burlesque and melancholic elements, and poetic films based on texts by writers such as the French poet Francis Ponge.\u0026#34; ] ], [ \u0026#34;Howard W. Koch\u0026#34;, [ \u0026#34;Howard Winchel Koch( April 11, 1916 \\u2013 February 16, 2001) was an American producer and director of film and television.\u0026#34; ] ] ], \u0026#34;supporting_facts\u0026#34;: [ [ \u0026#34;Move (1970 film)\u0026#34;, 0 ], [ \u0026#34;M\\u00e9diterran\\u00e9e (1963 film)\u0026#34;, 0 ], [ \u0026#34;Stuart Rosenberg\u0026#34;, 0 ], [ \u0026#34;Jean-Daniel Pollet\u0026#34;, 0 ] ], \u0026#34;evidences\u0026#34;: [ [ \u0026#34;Move (1970 film)\u0026#34;, \u0026#34;director\u0026#34;, \u0026#34;Stuart Rosenberg\u0026#34; ], [ \u0026#34;M\\u00e9diterran\\u00e9e (1963 film)\u0026#34;, \u0026#34;director\u0026#34;, \u0026#34;Jean-Daniel Pollet\u0026#34; ], [ \u0026#34;Stuart Rosenberg\u0026#34;, \u0026#34;country of citizenship\u0026#34;, \u0026#34;American\u0026#34; ], [ \u0026#34;Jean-Daniel Pollet\u0026#34;, \u0026#34;country of citizenship\u0026#34;, \u0026#34;French\u0026#34; ] ], \u0026#34;answer\u0026#34;: \u0026#34;no\u0026#34; } 测试集 #\r与训练集和验证集格式相同，但是answer、supporting_facts和evidences留空。\n进行评测需要联系作者，格式要求见github。\n开放域问答 #\r可以在此页面下载作者团队处理过的WiKi离线语料库：\rpara_with_hyperlink.zip\nMuSiQue：通过单跳问题组合构建的多跳问题 #\r论文题目：Trivedi 等 - 2022 - ♫ MuSiQue Multihop Questions via Single-hop Question Composition\n论文链接：\r♫ MuSiQue: Multihop Questions via Single-hop Question Composition | Transactions of the Association for Computational Linguistics | MIT Press\n项目地址：\rStonyBrookNLP/musique: Repository for MuSiQue: Multi-hop Questions via Single-hop Question Composition, TACL 2022\n构建思路 #\r作者引入了一种自下而上的过程，通过仔细选择和组合从现有数据集中获得的单跳问题，构建具有挑战性的多跳阅读理解问答数据集。其背后的关键思想包括：\n（i）从大量单跳问题中组合多跳问题，这使得能够系统地探索广泛的候选多跳问题空间。\n（ii）应用一套严格的筛选标准，确保没有子问题能够在未找到其连接的前一个子问题答案的情况下被回答（这一关键特性我们正式定义为MuSiQue条件的一部分。\n（iii）在每个单跳问题的层面上减少训练-测试泄漏，从而减轻简单记忆技巧的影响。\n（iv）添加难以识别的干扰上下文。\n（v）在子问题层面上创建不可回答的多跳问题。\n以这种方法，作者构建了一个新的多跳问答数据集MuSiQue-Ans，包含约25,000个2-4跳的问题，具有六种不同的组合结构（见表1）。 MuSiQue-Ans 比两个先前的多跳推理数据集HotpotQA和2WikiMultihopQA更具挑战性且更难以作弊。特别是，它的人机差距是前者的三倍，且断开推理（DiRe）得分显著较低，该得分反映了一个数据集通过断开推理作弊的程度。\n另外，通过引入上下文不足的概念，作者还发布了一个数据集变体 MuSiQue-Full，包含约50,000个多跳问题，这些问题形成了可回答与不可回答问题的对比对。 MuSiQue-Full 的挑战性更高，且更难以作弊。\n数据格式 #\r训练集/验证集格式 #\r{ \u0026#34;id\u0026#34;: \u0026#34;nhop_indx(str)\u0026#34;, \u0026#34;paragraphs\u0026#34;: [ { \u0026#34;idx\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;is_supporting\u0026#34;: [ // 以下两种之一作为值。 false, true ] } ], \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;question_decomposition\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;paragraph_support_idx\u0026#34;: \u0026#34;int\u0026#34; } ], \u0026#34;answer\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;answer_aliases\u0026#34;: [ \u0026#34;str\u0026#34;//... ], \u0026#34;answerable\u0026#34;: [ // 以下两种之一作为值。 false, true ] } id: \u0026quot;nhop_indx(str)\u0026quot; 表示当前问答任务的唯一标识符，nhop表示该问题有几跳。 paragraphs:\nparagraphs 是一个段落列表，包含多个与问题相关的文本段落。 idx: 每个段落的索引编号，类型为整数 (int)，用于标识段落顺序或位置。 title: 段落的标题，类型为字符串 (str)，表示该段落的主题或概述。 paragraph_text: 段落的具体内容，类型为字符串 (str)，包括完整的文本信息。 is_supporting: 这是一个布尔值列表，表示该段落是否支持回答当前的问题。可能的取值为 true（支持）或 false（不支持）。在-ans 数据中，一定存在true字段，而在-full数据中则不一定。 question:\n当前需要回答的问题，类型为字符串 (str)，代表任务的核心问题。 question_decomposition:\n问题的分解列表，适用于多步骤问题或复杂问题的分解。 id: 分解后问题的编号，类型为整数 (int)。 question: 每个分解问题的内容，类型为字符串 (str)（这里的问题不是简单子问题，而是涵盖推理步骤的子问题拆解，会有一些奇怪的代词）。 answer: 对分解问题的答案，类型为字符串 (str)。 paragraph_support_idx: 支持分解问题答案的段落索引，类型为整数 (int)，指向 paragraphs 中的某个段落。 answer:\n最终的回答，类型为字符串 (str)，是针对 question 的完整答案。 answer_aliases: 答案的别名或同义词列表，每个别名为字符串 (str)，用于处理多种可能的正确回答。 answerable:\n表示当前问题是否可以被回答的布尔值，可能的取值为 true（可回答）或 false（不可回答）。在-ans 数据中，都为true。 示例\n{ \u0026#34;id\u0026#34;: \u0026#34;2hop__28482_46077\u0026#34;, \u0026#34;paragraphs\u0026#34;: [ { \u0026#34;idx\u0026#34;: 0, \u0026#34;title\u0026#34;: \u0026#34;Pavlodar\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Pavlodar (Kazakh and Russian: Павлодар) is a city in northeastern Kazakhstan and the capital of Pavlodar Region. It is located 450 km northeast of the national capital Nur-Sultan, and 405 km southeast of the Russian city of Omsk along the Irtysh River. , the city has a population of 331,710. The population of \\\u0026#34;Pavlodar\\\u0026#34; is composed predominantly of ethnic Russians and Kazakhs with significant Ukrainian, German and Tatar minorities. The city is served by Pavlodar Airport.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;San Jose, San Pablo, Laguna\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Barangay San Jose (commonly known as Malamig) is one of the 80 barangays of San Pablo City in the Philippines. Located along the eastern part of the city, it is bordered by Brgy. Concepcion on the north and Brgy. San Francisco on the west.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 2, \u0026#34;title\u0026#34;: \u0026#34;Siege of Sloviansk\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The Siege of Sloviansk was an operation by the Armed Forces of Ukraine to recapture the city of Sloviansk in Donetsk Oblast from pro-Russian insurgents who had seized it on 12 April 2014. The city was taken back on 5 July 2014 after shelling from artillery and heavy fighting. The fighting in Sloviansk marked the first major military engagement between pro-Russian separatists and Ukrainian government forces, in the first runoff of battles of 2014.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 3, \u0026#34;title\u0026#34;: \u0026#34;Springfield, Tennessee\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Springfield is a city in and the county seat of Robertson County, which is located in Middle Tennessee on the northern border of the state. The population was 16,478 at the 2010 census and 16,809 in 2016.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 4, \u0026#34;title\u0026#34;: \u0026#34;Baltic Sea\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Since May 2004, with the accession of the Baltic states and Poland, the Baltic Sea has been almost entirely surrounded by countries of the European Union (EU). The only remaining non-EU shore areas are Russian: the Saint Petersburg area and the exclave of the Kaliningrad Oblast.\u0026#34;, \u0026#34;is_supporting\u0026#34;: true }, { \u0026#34;idx\u0026#34;: 5, \u0026#34;title\u0026#34;: \u0026#34;Estonia\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The superior god of Oeselians as described by Henry of Latvia was called Tharapita. According to the legend in the chronicle Tharapita was born on a forested mountain in Virumaa (Latin: Vironia), mainland Estonia from where he flew to Oesel, Saaremaa The name Taarapita has been interpreted as \\\u0026#34;Taara, help!\\\u0026#34;/\\\u0026#34;Thor, help!\\\u0026#34; (Taara a(v)ita in Estonian) or \\\u0026#34;Taara keeper\\\u0026#34;/\\\u0026#34;Thor keeper\\\u0026#34; (Taara pidaja) Taara is associated with the Scandinavian god Thor. The story of Tharapita\u0026#39;s or Taara\u0026#39;s flight from Vironia to Saaremaa has been associated with a major meteor disaster estimated to have happened in 660 ± 85 BC that formed Kaali crater in Saaremaa.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 6, \u0026#34;title\u0026#34;: \u0026#34;Eastern Front (World War I)\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;This offensive was unanticipated by the Turks, as it was in the middle of winter. The Turkish situation was exacerbated by the Third Army\u0026#39;s commander Kamil Pasha and Chief of Staff Major Guse absence. Coupled with an imbalance of forces -- the Russians had 325 000 troops, while the Turks only 78 000 -- the situation appeared grim for the Central Powers. After three months of fighting, the Russians captured the city of Trabzon on April 18, 1916.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 7, \u0026#34;title\u0026#34;: \u0026#34;Oklahoma, Clearfield County, Pennsylvania\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Oklahoma is a census-designated place located in Sandy Township, Clearfield County, in the state of Pennsylvania. As of the 2010 census the population was 782. It is bordered to the northwest by the city of DuBois.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 8, \u0026#34;title\u0026#34;: \u0026#34;Belarus\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Belarus (; , ), officially the Republic of Belarus (, ), formerly known by its Russian name Byelorussia or Belorussia (), is a landlocked country in Eastern Europe bordered by Russia to the northeast, Ukraine to the south, Poland to the west, and Lithuania and Latvia to the northwest. Its capital and most populous city is Minsk. Over 40% of its is forested. Its major economic sectors are service industries and manufacturing. Until the 20th century, different states at various times controlled the lands of modern-day Belarus, including the Principality of Polotsk (11th to 14th centuries), the Grand Duchy of Lithuania, the Polish–Lithuanian Commonwealth, and the Russian Empire.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 9, \u0026#34;title\u0026#34;: \u0026#34;El Quinche\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;El Quinche is a city of Ecuador, in the Pichincha Province, about in a straight line distance northeast of the city of Quito. The city, administratively a rural parish of the canton of Quito, is located in the valley of the headwaters of the Guayllabamba River, to the west of Pambamarca. It borders Cayambe Canton to the northeast.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 10, \u0026#34;title\u0026#34;: \u0026#34;Tuva\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Tuva (; Russian: Тува́) or Tyva (Tuvan: Тыва), officially the Tyva Republic (Russian: Респу́блика Тыва́, tr. Respublika Tyva, IPA: [rʲɪˈspublʲɪkə tɨˈva]; Tuvan: Тыва Республика, Tyva Respublika [tʰɯˈʋa resˈpʰuplika]), is a federal subject of Russia (a republic, also defined in the Constitution of the Russian Federation as a state).The Tuvan republic lies at the geographical center of Asia, in southern Siberia. The republic borders the Altai Republic, the Republic of Khakassia, Krasnoyarsk Krai, Irkutsk Oblast, and the Republic of Buryatia in Russia and Mongolia to the south. Its capital is the city of Kyzyl. It has a population of 307,930 (2010 census).From 1921 to 1944, Tuva constituted a sovereign, independent nation under the name of Tannu Tuva, officially, the Tuvan People\u0026#39;s Republic, or the People\u0026#39;s Republic of Tannu Tuva. The independence of Tannu Tuva, however, was recognized only by its neighbors: the Soviet Union and Mongolia.A majority of the population are ethnic Tuvans who speak Tuvan as their native tongue, while Russian is spoken natively by the Russian minority; both are official and widely understood in the republic. Tuva is governed by the Great Khural, which elects a chairman for a four-year term.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 11, \u0026#34;title\u0026#34;: \u0026#34;Russian language\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The language was first introduced in North America when Russian explorers voyaged into Alaska and claimed it for Russia during the 1700s. Although most colonists left after the United States bought the land in 1867, a handful stayed and preserved the Russian language in this region to this day, although only a few elderly speakers of this unique dialect are left. Sizable Russian-speaking communities also exist in North America, especially in large urban centers of the U.S. and Canada, such as New York City, Philadelphia, Boston, Los Angeles, Nashville, San Francisco, Seattle, Spokane, Toronto, Baltimore, Miami, Chicago, Denver and Cleveland. In a number of locations they issue their own newspapers, and live in ethnic enclaves (especially the generation of immigrants who started arriving in the early 1960s). Only about 25% of them are ethnic Russians, however. Before the dissolution of the Soviet Union, the overwhelming majority of Russophones in Brighton Beach, Brooklyn in New York City were Russian-speaking Jews. Afterward, the influx from the countries of the former Soviet Union changed the statistics somewhat, with ethnic Russians and Ukrainians immigrating along with some more Russian Jews and Central Asians. According to the United States Census, in 2007 Russian was the primary language spoken in the homes of over 850,000 individuals living in the United States.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 12, \u0026#34;title\u0026#34;: \u0026#34;Russian Soviet Federative Socialist Republic\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The Russian Soviet Federative Socialist Republic (Russian SFSR or RSFSR; Russian: Российская Советская Федеративная Социалистическая Республика, tr. Rossiyskaya Sovetskaya Federativnaya Sotsialisticheskaya Respublika listen (help·info)) commonly referred to as Soviet Russia or simply as Russia, was a sovereign state in 1917–22, the largest, most populous, and most economically developed republic of the Soviet Union in 1922–91 and a sovereign part of the Soviet Union with its own legislation in 1990–91. The Republic comprised sixteen autonomous republics, five autonomous oblasts, ten autonomous okrugs, six krais, and forty oblasts. Russians formed the largest ethnic group. To the west it bordered Finland, Norway and Poland; and to the south, China, Mongolia and North Korea whilst bordering the Arctic Ocean to the north, the Pacific Ocean to the east and the Black sea and Caspian Sea to the south. Within the USSR, it bordered the Baltic republics (Lithuania, Latvia and Estonia), the Byelorussian SSR and the Ukrainian SSR to the west. To the south it bordered the Georgian, Azerbaijan and Kazakh SSRs.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 13, \u0026#34;title\u0026#34;: \u0026#34;Estonia\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Militarization was another aspect of the Soviet state. Large parts of the country, especially the coastal areas, were closed to all but the Soviet military. Most of the sea shore and all sea islands (including Saaremaa and Hiiumaa) were declared \\\u0026#34;border zones\\\u0026#34;. People not actually residing there were restricted from travelling to them without a permit. A notable closed military installation was the city of Paldiski, which was entirely closed to all public access. The city had a support base for the Soviet Baltic Fleet\u0026#39;s submarines and several large military bases, including a nuclear submarine training centre complete with a full-scale model of a nuclear submarine with working nuclear reactors. The Paldiski reactors building passed into Estonian control in 1994 after the last Russian troops left the country. Immigration was another effect of Soviet occupation. Hundreds of thousands of migrants were relocated to Estonia from other parts of the Soviet Union to assist industrialisation and militarisation, contributing an increase of about half a million people within 45 years.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 14, \u0026#34;title\u0026#34;: \u0026#34;Lesozavodsk\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Lesozavodsk () is a town in Primorsky Krai, Russia, located on the Ussuri River (Amur\u0026#39;s tributary), from the Sino–Russian border and about north of Vladivostok, the administrative center of the krai. Population: 37,000 (1972). It was formerly known as Ussuri ().\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 15, \u0026#34;title\u0026#34;: \u0026#34;Tucson, Arizona\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Tucson is located 118 mi (190 km) southeast of Phoenix and 60 mi (97 km) north of the United States - Mexico border. The 2010 United States Census puts the city\u0026#39;s population at 520,116 with a metropolitan area population at 980,263. In 2009, Tucson ranked as the 32nd largest city and 52nd largest metropolitan area in the United States. A major city in the Arizona Sun Corridor, Tucson is the largest city in southern Arizona, the second largest in the state after Phoenix. It is also the largest city in the area of the Gadsden Purchase. As of 2015, The Greater Tucson Metro area has exceeded a population of 1 million.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 16, \u0026#34;title\u0026#34;: \u0026#34;Southern Scientific Center RAS\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Southern Scientific Center of the Russian Academy of Science (SSC RAS) is a regional unit of the Russian Academy of Science, which includes research groups from a number of cities located in the Southern Federal District of Russia. It has a staff of about 260 people, including 2 Academicians and 2 Corresponding Members of the Russian Academy of Science, 59 Doctors of Science and 118 PhDs.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 17, \u0026#34;title\u0026#34;: \u0026#34;Estonia\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The Oeselians or Osilians (Estonian saarlased; singular: saarlane) were a historical subdivision of Estonians inhabiting Saaremaa (Danish: Øsel; German: Ösel; Swedish: Ösel), an Estonian island in the Baltic Sea. They were first mentioned as early as the second century BC in Ptolemy\u0026#39;s Geography III. The Oeselians were known in the Old Norse Icelandic Sagas and in Heimskringla as Víkingr frá Esthland (Estonian Vikings). Their sailing vessels were called pirate ships by Henry of Latvia in his Latin chronicles written at the beginning of the 13th century.\u0026#34;, \u0026#34;is_supporting\u0026#34;: true }, { \u0026#34;idx\u0026#34;: 18, \u0026#34;title\u0026#34;: \u0026#34;Oklahoma City\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;The third-largest university in the state, the University of Central Oklahoma, is located just north of the city in the suburb of Edmond. Oklahoma Christian University, one of the state\u0026#39;s private liberal arts institutions, is located just south of the Edmond border, inside the Oklahoma City limits.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false }, { \u0026#34;idx\u0026#34;: 19, \u0026#34;title\u0026#34;: \u0026#34;Rēzekne\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;Rēzekne (Latgalian \\\u0026#34;Rēzekne\\\u0026#34; or \\\u0026#34;Rēzne\\\u0026#34; , ; see other names) is a city in the Rēzekne River valley in Latgale region of eastern Latvia. It is called \\\u0026#34;The Heart of Latgale\\\u0026#34; (Latvian \\\u0026#34;Latgales sirds\\\u0026#34;, Latgalian \\\u0026#34;Latgolys sirds\\\u0026#34;). Built on seven hills, Rēzekne is situated east of Riga, and west of the Latvian-Russian border, at the intersection of the Moscow – Ventspils and Warsaw – Saint Petersburg Railways. It has a population of 31,216 (2016) making it the 7th largest city in Latvia.\u0026#34;, \u0026#34;is_supporting\u0026#34;: false } ], \u0026#34;question\u0026#34;: \u0026#34;Which major Russian city borders the body of water in which Saaremaa is located?\u0026#34;, \u0026#34;question_decomposition\u0026#34;: [ { \u0026#34;id\u0026#34;: 28482, \u0026#34;question\u0026#34;: \u0026#34;Where is Saaremaa located?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;the Baltic Sea\u0026#34;, \u0026#34;paragraph_support_idx\u0026#34;: 17 }, { \u0026#34;id\u0026#34;: 46077, \u0026#34;question\u0026#34;: \u0026#34;which major russian city borders #1\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Saint Petersburg\u0026#34;, \u0026#34;paragraph_support_idx\u0026#34;: 4 } ], \u0026#34;answer\u0026#34;: \u0026#34;Saint Petersburg\u0026#34;, \u0026#34;answer_aliases\u0026#34;: [ \u0026#34;Petersburg\u0026#34; ], \u0026#34;answerable\u0026#34;: true } 测试集格式 #\r{ \u0026#34;id\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;paragraphs\u0026#34;: [ { \u0026#34;idx\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;str\u0026#34;, \u0026#34;paragraph_text\u0026#34;: \u0026#34;str\u0026#34; } //... ], \u0026#34;question\u0026#34;: \u0026#34;str\u0026#34;, } 测试集只保留id、paragraphs(去掉is_supporting) 和 question 字段。提交方式见github。\n整合格式 #\r因为这三个数据集一般都是同时出现，所以在模型测试时，转换成同一种格式还是很有必要的，这样方便对模型的测试，这里参考Efficient_RAG(Zhuang et.al, 2024)给出一种转换格式：\n{ \u0026#34;id\u0026#34;: \u0026#34;sample_id\u0026#34;, //# 样本的唯一标识符 \u0026#34;hop\u0026#34;: 2, //# 问题跳数 \u0026#34;type\u0026#34;: \u0026#34;sample_type\u0026#34;, //# 样本类型，例如 \u0026#34;compose\u0026#34;, \u0026#34;compare\u0026#34;, \u0026#34;inference\u0026#34;, \u0026#34;bridge_compare\u0026#34; \u0026#34;question\u0026#34;: \u0026#34;What is the question text?\u0026#34;, //# 问题文本 \u0026#34;answer\u0026#34;: \u0026#34;This is the answer text.\u0026#34;, //# 答案文本 \u0026#34;chunks\u0026#34;: [ { \u0026#34;id\u0026#34;: 0, //# 文本块的唯一标识符 \u0026#34;title\u0026#34;: \u0026#34;Chunk Title 1\u0026#34;, //# 文本块标题 \u0026#34;chunk\u0026#34;: \u0026#34;Text content of chunk 1.\u0026#34; //# 文本块内容 }, //# ... 更多文本块 ], \u0026#34;supporting_facts\u0026#34;: [ { \u0026#34;id\u0026#34;: 0, //# 支持事实的唯一标识符 \u0026#34;title\u0026#34;: \u0026#34;Supporting Fact Title 1\u0026#34;, //# 支持事实标题 \u0026#34;chunk\u0026#34;: \u0026#34;Text content of supporting fact 1.\u0026#34; //# 支持事实内容 }, //# ... 更多支持事实 ], \u0026#34;decomposition\u0026#34;: [ //# 对于 MuSiQueDataset，这是问题的分解信息 //# 对于其他数据集，可能为空或具有不同的结构 \u0026#34;decomposed_part_1\u0026#34;, \u0026#34;decomposed_part_2\u0026#34;, //# ... 更多分解部分 ] } 答案格式\n{ \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;问题的ID值\u0026gt;\u0026#34;, \u0026#34;original_question\u0026#34;: \u0026#34;\u0026lt;原始问题的文本内容\u0026gt;\u0026#34;, \u0026#34;ground_truth\u0026#34;: \u0026#34;\u0026lt;原始问题的答案\u0026gt;\u0026#34;, \u0026#34;final_answer\u0026#34;: \u0026#34;\u0026lt;最终答案的文本内容，如果解析或获取失败则为\u0026#39;Unavailable\u0026#39;\u0026gt;\u0026#34;, \u0026#34;Inference_process\u0026#34;: \u0026#34;\u0026lt;推理过程的描述，如果解析或获取失败则为\u0026#39;Unavailable\u0026#39;\u0026gt;\u0026#34;, \u0026#34;sub_questions\u0026#34;: [ { \u0026#34;sub_question\u0026#34;: \u0026#34;\u0026lt;第一个子问题的文本内容\u0026gt;\u0026#34;, \u0026#34;relevant_chunks\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;\u0026lt;相关文本块的标题\u0026gt;\u0026#34;, \u0026#34;chunk\u0026#34;: [ \u0026#34;\u0026lt;相关文本块的内容片段1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;相关文本块的内容片段2\u0026gt;\u0026#34;, //... ] }, //... ], \u0026#34;answer\u0026#34;: \u0026#34;\u0026lt;第一个子问题的答案文本内容，如果获取失败则为None\u0026gt;\u0026#34; }, ... ] } 四种问题类型： #\r比较问题（Comparison question） 定义：对同一组中的两个或多个实体在实体的某些方面进行比较的问题。 示例：如 “Who was born first, Albert Einstein or Abraham Lincoln?”（阿尔伯特・爱因斯坦和亚伯拉罕・林肯谁先出生？）。 推理过程：需要理解问题中的属性（如出生日期），并对两个实体进行定量或逻辑比较来得出答案。 推理问题（Inference question） 定义：基于知识库中的两个三元组和，利用逻辑规则获取新的三元组，然后根据新三元组创建问题，其答案为。 示例：已知三元组 (Abraham Lincoln, mother, Nancy Hanks Lincoln) 和 (Nancy Hanks Lincoln, father, James Hanks)，可得到新三元组 (Abraham Lincoln, maternal grandfather, James Hanks)，问题为 “Who is the maternal grandfather of Abraham Lincoln?”（亚伯拉罕・林肯的外祖父是谁？），答案是 James Hanks。 推理过程：要求系统理解多个逻辑规则，例如要找到 “grandchild”（孙辈），需先找到 “child”（子女），再基于此继续寻找下一级 “child”。 组合问题（Compositional question） 定义：由知识库中的两个三元组和创建，但与推理问题不同，两个关系和不存在推理关系。 示例：对于三元组 (La La Land, distributor, Summit Entertainment) 和 (Summit Entertainment, founded by, Bernd Eichinger)，问题是 “Who is the founder of the company that distributed La La Land film?”（发行《爱乐之城》电影的公司的创始人是谁？），答案是 Bernd Eichinger。 推理过程：系统需要回答多个原始问题并将它们组合起来，如回答上述示例问题，需先回答 “Who is the distributor of La La Land?”（《爱乐之城》的发行商是谁？），再回答其创始人是谁。 桥接比较问题（Bridge - comparison question） 定义：将桥接问题与比较问题相结合，需要找到桥接实体并进行比较以获得最终答案。 示例：“Which movie has the director born first, La La Land or Tenet?”（《爱乐之城》和《信条》哪部电影的导演出生更早？） 推理过程：模型需要找到连接两个段落（一个关于电影，一个关于导演）的桥接实体，获取出生日期信息，然后进行比较得出最终答案。 ","date":"2024年12月4日","externalUrl":null,"permalink":"/posts/%E4%B8%89%E5%A4%A7%E5%A4%9A%E8%B7%B3qa%E6%95%B0%E6%8D%AE%E9%9B%86/","section":"文章","summary":"简单介绍HotPotQA、2WikiMultihopQA和MuSiQue数据集的数据形式和构建理念","title":"【速览】三个经典多跳QA数据集","type":"posts"},{"content":"\r原文题目：Shi 等 - 2024 - Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering\n原文链接：\rGenerate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering - ACL Anthology\n1. 多跳RAG #\r​\t传统RAG大家都知道，把问题拿过来先用检索器找到最相关的一个或几个文本片段，把这些文本片段放到大模型上下文中，让大模型按照上下文提供的知识回答问题嘛。\n​\t但是呢，这种一次检索，一次回答的方法可能并不总是那么好用。\n​\t比如下面这个问题：\n周星驰的母亲和吴孟达谁更大？ ​\t你在任何资料里面（除了本文），都不会找到关于这个问题的直接回答（因为很少有人总结这么无聊的问题）。大模型的世界知识中，也基本不会有这么无聊的信息。\n​\t那抛开大模型，你如果想要通过搜索引擎回答这个问题，你要通过几步完成？\n首先，你得先对问题有个整体的理解，在一般的中文语境里面，两个人比较“谁更大”应该等同于“谁的年龄更大”。\n然后，你要对问题进行分解，对于上面这个问题，你可以分解成三个子问题\n1. 周星驰的母亲是谁？获得一个人名，比如”凌宝儿“。 2. ”凌宝儿今年多大？“但是由于年龄这种信息有时效性并不适合直接检索，我们可以转而检索“凌宝儿的出生日期？“ 3. 吴孟达的出生日期？ 可以看到，问题2和问题1是递进的两个问题，没有问题1的答案我们也没法对问题2进行提问。\n最后，在得到上述子问题的答案后，我们还需要一个简单的推理判断，比较周星驰母亲和吴孟达的出生日期，得到最终答案。\n​\t那么大模型处理上面问题的逻辑也是类似的，根据原问题分解为可以直接检索得到答案的子问题，再通过这些子问题检索文本块得出部分事实，以此迭代重复直到收集到的部分事实能够通过推理得出原问题的回答。\n图1 Self-Ask论文中的例子\r​\t那么在实际操作中如何对原问题进行分解呢？当然是直接扔给万能的大模型辣！相比大家都听说过思维链这东西，给大模型一定格式化的提示，就能激发大模型的问题分解与分段推理能力（如图1）。所以，借助提示工程的方法，让大模型自己借助现有资料对问题进行分解，也是解决多跳RAG问题早期研究的主要思路。\n推荐阅读：\nCoT：\rChain-of-Thought Prompting Elicits Reasoning in Large Language Models (neurips.cc) ReAct：[\r2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models (arxiv.org) Self-Ask：\rMeasuring and Narrowing the Compositionality Gap in Language Models - ACL Anthology IRCoT：\rInterleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions - ACL Anthology 2. 现有问题 #\r​\t对于刚才提到的这种分解问题先检索再根据上下文回答的RAG范式，其实是存在一些问题的：\nRAG的有效性其实是受到检索性能的约束的，这种范式完全是利用检索器返回的文档进行回答，而LLM本身自有的世界知识是被忽略的。而在多跳问答中这个问题会更加严重，因为只要在一个子问题的回答中检索器没有返回正确的文档，可能整个问题就会回答错误，导致该范式的性能下降。 检索到的文档不可避免地包含不相关或看似合理的语句，直接将它们纳入LLM的推理链可能会误导大模型产生不正确或不相关的响应。 图2 检索性能很大程度上影响最后回答问题的能力\r3. 基本思路 #\r​\t那么本文就提出了一个基于生成后再矫正策略的检索框架，在这个框架中，LLM交替执行下面两个步骤：\n答案推导：根据原问题和现有资料制定一个更简单的单跳问题，并直接生成答案;\n指令下知识验证：在检索到的文档中建立问题-答案对，修正答案中的错误预测，并将本轮修正后的问答对整合到现有资料中。\n图3 GenGround基本思路\r4. 具体方法 #\r4.1 答案推导 #\r在这一步，GenGround 将构建好的提示、示例、需要分解的问题、之前分解出的所有子问题和答案塞给大模型，让大模型输出这次分解后的子问题和他根据自己世界知识回答的答案。公式化表示为： $$ q_i,a_i=M_\\theta(I_A,Q,H_i) $$\n其中，$q_i,a_i$分别表示生成的子问题和答案，$M_\\theta$ 表示参数为 $\\theta$ 的大模型，$I_A$表示构建的提示，$\\mathcal{Q}$ 表示原问题，$\\mathcal{H}_\\mathcal{i}$ 表示之前之前积累的修改后的问答对。因为本论文没有开源代码，而且附录中也没有详细的示例，所以具体他们构建的提示可以参考图4。\n图4 答案推导部分的提示构建\r4.2 指令性知识验证 #\r​\t大模型直接生成的答案可能生成非事实性陈述或”幻觉“（不然我们也不会研究RAG），所以需要进一步根据现有资料对答案进行修正。具体来说，在第 $i$ 次迭代中，进行如下操作：\n使用检索器根据推导出的子问题 $q_i$ 检索相关文档集 $D$ 。 $$ D = Retrieval(q_i) $$\n通过引用从检索文档 $ D_i $ 中找到的最相关内容 $\\tilde{d_i} $ ，指令大模型 $ M_{\\theta} $ 将问答对 $ (q_i, a_i)$ 基于现有资料进行验证，得到修正后的答案，公式化表示为： $$ \\tilde{a_i} = M_{\\theta}(\\mathcal{I}_\\mathcal{G},\\mathcal{Q}, q_i, a_i) $$ 其中 $\\tilde{a_i} $ 表示修正路径，包括引用的证据和修正后的答案。其格式为${证据},{修改后的答案}$。\n如果模型找不到可以修正问题的证据，则输出\\\u0026lt;ref\\\u0026gt;Empty\\\u0026lt;ref\\\u0026gt;，并不对回答进行修改。\n$\\mathcal{I}_\\mathcal{G}$ 表示在零样本环境下的验证指令，这部分内容可以参考图5。 其他输入还有原问题 $\\mathcal{Q}$ 和答案推导环节生成的问答对 $ (q_i, a_i)$ 。\n图5 指令性知识验证的提示构建\r这里有一个问题，原文公式表示和上图中都没有展示相关文档块是怎么处理的，但是我们可以从上图的提示中可以推断他们应该是把相关文档作为上下文输入给大模型了的。\n最后，把本次迭代检索的子问题$q_i$、本轮修正路径 $\\tilde{a_i} $ 并入现有资料 $H_i$ ，供大模型在下一次迭代中使用，即 $$ \\mathcal{H}_{i+1} = \\mathcal{H}_i \\cup {(q_i, \\tilde{a_i})} $$\n文章中也没有给出明确的迭代终止条件，我们只能从下图给出的论文中的唯一示例中看到整个问答流程的输出信息（MD发论文不开源代码真恶心）。\n图6 整体流程输出示例\r4.3 批量验证策略 #\r​\t传统的先检索再生成的RAG方式有一个问题，就是不管这个检索出的文档块是不是真的包含能够回答问题的信息，当大模型生成答案时，能获得的额外知识输出就这一个了。这样很容易导致一个问题，就是大模型很容易被检索出的无关信息误导导致回答错误。（当然也有应对的办法，比如对文档有效性进行判断：\rSelf-RAG、束搜索等：\rEnd-to-End Beam Retrieval）\n图7 批量验证策略示例\r​\t本文先生成再检索的框架有个优势，就是可以根据回答去批量比对文本，直到找到可以对指定的答案进行修改的文档。系统规定一个批处理大小 $b$ ，按照相关性排序每次喂给大模型 $b$ 个相关文档，检查能否对已经生成的答案进行修改，如果可以，修改回答停止迭代。否则，继续送进去下一批 $b$ 个文档进行检查，直到能够对答案进行修改，或者找不到相关文档，直接将生成的答案作为备用，如图7所示。\n5. 验证过程蒸馏的泛化推广 #\r​\t上述框架在ChatGPT之类的闭源大模型上表现出色，但是在参数量较小的开源模型上表现不佳，主要在于在知识验证阶段”小“模型无法遵循指令去引用相关证据。于是作者使用知识蒸馏的方法将ChatGPT输出的验证过程蒸馏到一个小参数的学生模型中去。\n5.1 数据构建 #\r​\t作者找了个单跳QA数据集（Natural Questions数据集）抽了50000个问题，每个问题 $q$ 都与相应的真实文档 $\\tilde{d}$以及噪声文档 $D$ 配对。用个小模型（如Mistral-7B）随便生成一个什么回答当作修正前的答案 $a$，修正过程的就交给ChatGPT进行生成，还采用大模型去过滤低质量输出，包括输出格式不对的、没找到证据的、没有修改回答的和修改回答错了的（原数据集也是有标准答案的）。最终获得了如表1所示的一个用于小模型训练的增强数据集。\n表1 增强数据集相关指标\r​\n5.2 模型训练 #\r​\t使用上一节获得的数据，作者用极大似然损失函数对Mistral-7B进行训练，使用3张A100训练了18小时。具体损失函数如下：\n6. 实验 #\r6.1 数据集选择与设置 #\r实验在四个常用的多跳问答基准数据集上进行：\nHotpotQA：需要复杂的推理链条来获取答案，主要涉及开放域问题。 MuSiQue：通过将多个单跳问题合成多跳问题来评估模型的推理能力。 2WikiQA：包含来自多个维基百科页面的问题，评估跨文档的推理能力。 StrategyQA：主要测试模型在策略性推理问题中的表现。 这些数据集覆盖了从事实推理到复杂的多跳推理，能全面验证模型的能力。\n6.2 评估指标 #\r准确率 (Accuracy, Acc)：检验生成的答案是否包含正确的目标答案。\nF1 值：基于词汇重叠来评估模型输出与参考答案之间的相似度。\n语义准确率 (Acc†)：通过 GPT-3.5 的自动评估来检验生成的答案与参考答案在语义上的一致性，具体提示如下。\nIn the following task, you are given a Question, a model Prediction for the Question, and a Ground-truth Answer to the Question. You should decide whether the model Prediction implies the Ground-truth Answer. Question {question} Prediction {model output} Ground-truth Answer {answer} Does the Prediction imply the Ground-truth Answer? Output Yes or No 这三项指标能多角度衡量模型在生成答案中的性能，尤其是考虑了语义上的准确性，避免仅基于词汇相似度。\n6.3 实验方法对比 #\r作者利用gpt-3.5-turbo作为GenGround和所有Baseline的核心模型，解码温度设置为0以实现确定性生成，批处理策略中的批大小设置为3，使用ColBERTv2作为检索器，为每个问题检索前10个文档。通过比较生成-然后-校正框架与其他两类方法进行对比：\n不带检索的生成方法： CoT（Chain of Thought）：通过链式推理步骤来生成答案。 CoT-SC（Self-Consistency）：采样多种推理路径，并选择一致性最高的答案。 GenRead：模型通过阅读生成的上下文来生成答案。 带检索的生成方法： ReAct：将问题生成、文档检索和知识整合相结合进行多跳推理。 DSPy：基于编程框架的多跳推理方法。 RetGen：通过迭代检索和生成的协同策略来回答多跳问题。 6.4 实验结果 #\r实验结果表明，GenGround 框架在所有数据集和指标上均优于现有方法，尤其在 HotpotQA 和 MuSiQue 等复杂推理数据集上，其准确率提升了显著的百分点。具体表现为：\n在 HotpotQA 数据集上，GenGround 取得了 47.27% 的准确率，比基线方法高出 6 个百分点。 在 MuSiQue 数据集上，F1 值达到了 27.36%，同样显著优于其他方法。 表2 主要实验结果\r泛化实验 #\r​\t作者同时实验了指令知识蒸馏的有效性，将主干大语言模型替换为开源模型（Mistral-7B），并在相同条件下重复实验，实验表明：\n直接使用GenGround方法提示（Vanilla）的模型性能已经优于Baseline。\n**指令蒸馏（IDG）**后的模型进一步显著提升了整体性能。\n表3 小参数量模型在指令式知识蒸馏后的表现\r更换检索器 #\r​\t作者将ColBERTv2替换为BM25和Google搜索，使用ChatGPT作为核心模型，实验显示无论使用何种检索器， GenGround方法均展现出最佳性能。表明其在低召回（BM25）和高召回（Google搜索）场景中的适应性。这可能得益于其中的答案推导阶段，该阶段利用LLM的参数知识来补充检索到的知识。此外，指导性知识基础阶段通过引用最相关的证据有效地整合了检索文档，从而减轻了嘈杂文档的负面影响。\n表4 更换检索器的泛化实验\r6.5 消融实验 #\r为了验证框架中不同模块的有效性，实验还进行了消融实验，分别去除了答案推导阶段、知识校正阶段以及批量校正策略，结果表明：\n去掉答案推导阶段后，SQA上模型的准确率降低了 10 个百分点，表明这一阶段对复杂问题的推理至关重要。 去掉知识校正阶段后，HQA上模型的表现下降，F1 值减少了 7%，这说明直接生成的答案可能包含幻觉或错误，必须通过校正提升准确性。 去掉批量验证阶段后，性能也都有下降，说明当参考文档列表冗长且包含无关信息时，大模型在生成正确答案方面存在困难。 表5 消融实验\r6.6. 分析与讨论 #\r作者在本节用独立重复实验证明了他们方法的稳定性、token的消耗量相较于没有问题分解的方法也有减少。\n图8 协同整合实验\r作者还探索大模型自有知识和外部文档知识两种知识来源的协同整合。具体做法包括计算以下三个指标：\n成功率：LLMs直接生成正确答案或准确修正错误答案的比例。 失败率：LLMs生成错误答案并未能修正的比例。 错误率：LLMs生成正确答案但错误修正的比例。 由于现有数据集缺乏即时解答轨迹的标准答案，研究邀请了三位标注者对Hotpot QA数据集中的100个随机样本进行评估。研究发现LLMs整体成功率为53.2%，其中28.7%的问题由LLMs直接正确回答，24.5%的问题则先生成错误答案，后通过外部文档修正。此外，误差率仅为5.6%，说明LLMs在使用外部文档进行修正时通常是有效的。\n7. 局限性 #\r​\t文章最后分析了本框架的一些局限性，包括：\n初始答案生成的依赖性：框架的第一步是生成初始答案，但其效果高度依赖于具体任务。在不同任务中，模型可能难以生成有意义或有用的初始答案，限制了其在多样问题领域的适用性。\n问题分解的挑战：该方法假设复杂问题可以被分解为更简单的问题，但问题分解本身是一个具有挑战性的任务，尚未在当前框架中得到充分探索。\n外部文档的局限性：该方法假设外部文档可以用于修正模型最初生成的非事实性陈述。然而，如果外部文档缺乏必要的修正信息或包含错误信息，框架的有效性可能会受到影响。\n8. 总结 #\r​\t本文最主要的创新点还是在传统基于提示工程的RAG检索生成迭代的基础上添加了先让模型生成答案再进行修改的步骤，使得答案语义变成一种筛选候选文档的方法，知识蒸馏训练小模型的做法也是非常的典。\n​\t当然提示工程的活就这么多，22、23年整的也差不多了，后面纯提示工程的工作还能不能受到认可也是个问题。现在一个新的研究范式是去训练一个小参数模型作为外挂插件去协同大模型进行检索增强生成，类似多模块智能体的思路感觉还可能有点搞头，比如\rSlimPLM、\rEfficientRAG。\n​\t最后，不开源代码差评。\n","date":"2024年10月11日","externalUrl":null,"permalink":"/posts/genground%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"组会报告文字版，介绍一个基于生成后再矫正策略的RAG框架","title":"【组会】Generate-then-Ground：在检索增强多跳问答中的生成后再矫正策略","type":"posts"},{"content":"论文标题：Trivedi 等 - 2023 - Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n论文链接：\rInterleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions - ACL Anthology\n解决问题 #\r传统的一步式RAG难以满足多步问答需求，即提问一次需要多次进行查询才能得到最终答案，因为要进行检索的查询取决于已经得到的内容，而已经得到的内容是根据之前检索的内容生成的。如图一。\n基本思路 #\r基本思路即是使用检索来指导思维链，然后思维链来指导检索，两个步骤交错进行。\n首先，扩展COT：使用问题、当前检索到的段落和之前的COT语句来生成下一个语句。\n然后，扩展检索信息：使用上一步生成的COT语句进行查询，将检索到的段落进行收集。\n以此重复，直到COT得到答案或者达到最大限制的推理步数。\n上述步骤停止后，将所有收集到的段落作为检索结果，作为问题或思维链的提示上下文来回答问题。\n具体方法 #\r思维链推理的交叉检索 #\rIRCoT检索器由三个要素构成：\n一个可以通过查询从语料库或知识源获得给定数量文本段落的基本检索器 一个具有zero/few-shot思维链生成能力的语言模型 少量带注释的问题，带有推理步骤，解释如何用自然语言（思维链）得出答案。以及来自知识源的，能够支持推理链和答案的一组文本片段。 检索器工作方法如图二所示，首先使用问题Q检索K个段落，收集起来作为一组基本段落。然后交错执行思维链推理和检索两个步骤，直到满足终止标准。\n检索指导的思维链推理 #\r检索指导的思维链推理使用问题、到目前为止收集的段落以及到目前为止生成的 CoT 句子生成下一个 CoT 句子。该任务的提示模板如下所示：\n在上下文能力的验证中，作者使用上述格式的完整CoT，在实际测试中，则只向模型展示到目前为止生成的 CoT 句子，然后让它完成其余部分。即使模型可能会输出多个句子，但对于每个推理步骤，只获取第一个生成的句子并丢弃其余的句子。\n在上下文能力验证中，作者将事实段落和M个随机采样段落按照上图格式打乱并连接在一起。在实际测试中，使用之前收集的检索到的全部段落。如果生成的 CoT 句子具有“answer is:”字符串或已达到最大步骤数，则终止该过程并返回所有收集的段落作为检索结果。\n使用在上一步中生成的CoT语句作为查询来检索更多的段落并收集起来。最多段落总数需要限制以免超出LLM的上下文长度限制。\nQA阅读器 #\r本文通过两种提示策略实现基于上下文的文本问答。\nCoT提示：使用之前相同的CoT模板，但是在实际测试中要求模型从头开始生成完整的 CoT。 CoT 的最终句子预计采用“答案是：\u0026hellip;”的形式，以便可以通过编程方式提取答案。如果不是这种形式，则返回全部的生成内容作为答案。 直接提示：对于直接提示，作者使用与 CoT 提示相同的模板，但答案字段（“A：”）仅包含最终答案而不是 CoT。 实验 #\r数据集 #\rHotpotQA：\n​\t现有的问答（QA）数据集无法训练可执行复杂推理和提供答案解释的 QA 系统。HotpotQA创建了一个新型问答数据集 HotpotQA，该数据集包含 11.3 万个基于维基百科的问答对，具备以下四个特点：\n问题的答案必须要基于多个支持文档；\n问题多样化，不局限于任何已有的知识库或知识模式；\n提供句子级别的支持推理线索（supporting fact），允许 QA 系统用强大的监督进行推理，并对预测结果进行解释；\n提供了新型模拟比较型问题，来测试 QA 系统提取相关线索、执行必要对比的能力。\n论文展示了 HotpotQA 数据集对最新 QA 系统是有难度的，支持推理线索帮助模型提升性能、做出可解释的预测。\narxiv.org/pdf/1809.09600\n2WikiMultihopQA：\n​\t多跳问答 （QA） 数据集旨在通过要求模型阅读多个段落来回答给定问题来测试推理和推理技能。然而，当前的数据集并没有为从问题到答案的推理过程提供完整的解释。此外，以前的研究表明，现有多跳数据集中的许多示例不需要多跳推理来回答问题。在这项研究中，我们提出了一个新的多跳QA数据集，称为2WikiMultiHopQA，它使用结构化和非结构化数据。在我们的数据集中，我们引入了包含多跳问题推理路径的证据信息。证据信息有两个好处：\n（i） 为预测提供全面的解释。\n（ii） 评估模型的推理技能。\n​\t在生成问答对时，我们精心设计了一个管道和一组模板，以保证多跳步骤和问题的质量。我们还利用了 Wikidata 中的结构化格式，并使用逻辑规则来创建自然但仍需要多跳推理的问题。通过实验，我们证明了我们的数据集对于多跳模型具有挑战性，并且它确保需要多跳推理。\naclanthology.org/2020.coling-main.580.pdf\nMuSiQue（可回答部分）：\n​\t多跳推理仍然是一个难以实现的目标，因为现有的多跳基准测试在很大程度上可以通过捷径解决。我们能否创建一个从结构上要求进行正确多跳推理的问答（QA）数据集？为此，我们引入了一种自下而上的方法，系统地选择可以组合的单跳问题对，这些问题是相互关联的，即其中一个推理步骤严重依赖于另一个步骤的信息。这种自下而上的方法让我们能够探索大量的问题空间，并添加严格的筛选条件以及其他针对关联推理的机制。它提供了对构建过程和生成的k跳问题的特性进行细粒度控制的能力。我们使用这种方法创建了MuSiQue-Ans，一个包含25K个2到4跳问题的新多跳QA数据集。相较于现有数据集，MuSiQue-Ans整体上更难（人机差距增加了3倍），并且更难通过不相关的推理来作弊（例如，单跳模型的F1分数下降了30分）。我们还添加了无法回答的对比问题，以生成一个更严格的数据集MuSiQue-Full。我们希望我们的数据集能够帮助NLP社区开发能够进行真正多跳推理的模型。\narxiv.org/pdf/2108.00573\nIIRC（可回答部分）：\n​\t人类通常需要阅读多篇文档来满足他们的信息需求。然而，大多数现有的阅读理解（RC）任务仅关注于上下文中提供了回答问题所需的全部信息的情况，因此无法评估系统识别信息不足的潜在情况以及定位这些信息来源的能力。为填补这一空白，我们提出了一个名为IIRC的数据集，该数据集包含超过13,000个问题，问题来自英文维基百科的段落，这些段落只提供了部分解答所需的信息，缺失的信息存在于一个或多个链接文档中。这些问题是由不访问任何链接文档的众包工人编写的，这导致问题与答案所在上下文之间的词汇重叠极少。这个过程还生成了许多无答案的问题，以及那些需要离散推理的问题，增加了任务的难度。我们参考了最近在各种阅读理解数据集上的建模工作，构建了一个用于该数据集的基线模型，发现其在这一任务上的F1得分为31.1%，而人类的预估表现为88.4%。该数据集、基线系统的代码以及一个排行榜可以在https://allennlp.org/iirc找到。\naclanthology.org/2020.emnlp-main.86.pdf\n构建\n对于HotpotQA，使用其自带的维基百科语料库进行开放域设置。对于另外三个原本属于阅读理解或混合设置的数据集，作者使用相关上下文构建了其的开放域设置的语料库（附录A）。对于每个数据集，从原始开发集随机抽取100个问题用于调优超参数，并随机抽取另外500个问题作为测试集。\n模型 #\r使用BM25作为基础检索器，构建两种检索系统：\n一步检索器（OneR）：直接使用问题检索K个段落。K选择在开发集上表现最好的 K ∈ {5, 7, 9, 11, 13, 15}。 IRCoT检索器：使用BM25作为检索器，结合GPT3、Flan-T5作为不同规模的CoT生成器进行实验。 在这些段落中，作者通过以下步骤构建示例和进行超参数搜索：\n构建示例： 作者为每个数据集创建了三个演示（“训练”）集，每个演示集包含15个从数据集中随机抽取的问题。 这些演示集被用于在不同的实验中，向模型展示上下文示例。 在测试时，作者在模型的上下文长度限制内打包尽可能多的演示。 GPT3的上下文限制是 8K tokens。 Flan-T5 的上下文限制为 6K tokens，这是 80G A100 GPU 内存所能容纳的最大值。 对于IRCoT的推理模块的训练示例，作者使用了黄金段落（即包含正确答案的段落）以及数量较少的干扰段落（M ∈ {1, 2, 3}），以生成训练示例。 超参数搜索： 对于每个实验，作者首先使用第一个演示集对开发集（dev set）进行超参数搜索，以确定最佳的超参数设置。 对于IRCoT Retriever，作者的关键超参数是K（即每个步骤检索的段落数，K ∈ {2, 4, 6, 8}）和干扰段落的数量M。 在超参数搜索过程中，作者允许所有检索系统最多检索15个段落，并通过衡量这些段落中黄金段落的召回率来选择最佳的超参数。 找到最佳的超参数后，作者使用这些超参数在测试集上进行评估，并报告了不同实验中三个演示集的结果的平均值和标准差。 通过这种方法，作者确保了在固定预算下每个系统的最优召回率，并将这些结果用于进一步的实验和分析。\n在qa阅读器的选择上，Flan-T5在直接提示策略下表现更好，而GPT3在CoT提示策略下表现更好，于是后面就这么用的。\n作者还构建了开放域的问答模型，将检索器和阅读器结合在一起。此外，作者还实验了没有检索器的问答阅读器 NoR QA，以评估语言模型能否仅凭其参数知识回答问题。为了选择最佳的 ODQA 模型超参数，作者搜索了能在开发集上最大化答案 F1 值的超参数 K 和 M。IIRC 的结构与其他数据集略有不同，其问题基于一个主要段落，并且其他支持段落来自该段落中提到的实体的维基百科页面。论文也稍微修改了检索器和阅读器以适应这种情况。\n操作 #\r直接性能 #\r图三展示了IRCoT检索器的的f1性能是优于一步检索器的，不论是在T5和GPT3的环境下。\n图四展示了无检索器、一步检索器、和IRCoT的性能差异。值得关注的最后一个在GPT3下IIRC数据集中没有检索器直接问答反而是最好的，应该是拿这个训练过。\n总的来说，IRCoT在少样本多步ODQA方面达到了当时的SOTA效果。如表1，展示了EM和F1的相关情况。\n泛化能力 #\r作者实验了三种问答系统的泛化能力，使用一个数据集的提示示例来评估另一个数据集，图5图6说明IRCoT的性能还是高一些。\n事实错误 #\r作者使用三种问答系统为每个数据集随机抽取的40个问题生成CoT，如果生成的CoT中有一环是错误的，那就认为CoT存在事实错误。如图7，IRCoT事实错误最少。\n表2展示了不同方法的CoT预测在质量上的差异。由于NoR完全依赖于参数知识，它经常在第一句话就犯事实性错误，从而导致整个CoT出错。OneR能够检索到最接近问题的相关信息，因此较少在初期犯错，但在CoT后期仍然会犯错。另一方面，IRCoT通常能够在每一步中防止这种错误。\n对于小型化的作用 #\r图8图9分别展示了在不同模型规模下，一步检索和IRCoT召回率和F1的变化情况。可以看到大多数情况下，小规模LM的IRCoT表现比更大规模LM的一步检索效果还要好。\n当前的限制 #\r依赖基础语言模型的能力：\nIRCoT 依赖于基础语言模型具备零样本或少量样本的链式思考（CoT）生成能力。\n大型语言模型（超过100B参数）通常具备这种能力，但较小的模型（低于20B参数）较少具备，这限制了IRCoT的广泛应用。\n随着对小型语言模型的关注增加，它们将逐步具备链式思考生成能力，使IRCoT能兼容更多的模型。\n对长输入的要求：\nIRCoT 依赖于基础语言模型支持长输入，因为需要将多个检索到的段落与问答示例一起输入模型。 未来研究可以探索重新排序和选择检索段落的策略，以减少对长输入支持的需求。 计算成本： IRCoT 相比基线方法（OneR 和 ZeroR）在检索和问答上有性能提升，但也增加了计算成本，因为它对链式思考的每个句子都需要单独调用语言模型。 实验重现性： 部分实验使用了OpenAI的商用API（code-davinci-002），但由于该模型已被弃用，实验的重现性受到影响。 使用 Flan-T5-* 的实验结果仍可重现，因为其模型权重是公开的。 ","date":"2024年10月10日","externalUrl":null,"permalink":"/posts/ircot%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"使用思维链解决多跳问答的典型论文，提示工程还能整出多少活？","title":"【笔记】IRCOT: 交错检索与链式思考推理在知识密集型多跳问答中的应用","type":"posts"},{"content":"\r原文题目：Sarthi 等 - 2024 - RAPTOR Recursive Abstractive Processing for Tree-Organized Retrieval\n原文链接：[\r2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval (arxiv.org)\n基础思路 #\r使用递归嵌入、聚类和总结文本块的新方法，从下到上构建了一棵具有不同摘要级别的树。在推理时，RAPTOR 模型从这棵树中检索，在不同抽象级别的冗长文档中集成信息。\n具体方法 #\r思路基础：一个长文本一般都会分章节，或者至少有个分层的逻辑（什么总分总之类的）。\n首先，将文本按照100的长度切片，并保证没有句子会被中间截断（如果一个句子超过 100 Token的限制，将整个句子移动到下一个块，而不是在句子中间切掉它。这保留了每个块中文本的上下文和语义连贯性。）。并使用SBERT进行文本嵌入，构成树的叶节点。\n然后使用聚类算法对刚才切出的文本块进行分组，每次分组后，使用LLM对分组后的文档进行一次摘要，然后对摘要进行一次嵌入。以此循环不断进行聚类、摘要和嵌入，直到无法再进行聚类。以此产生原始文档的结构化、多层树状表示。这种方式有一个很大的优势是计算复杂度是线性的，适合大文本处理。\n聚类算法 #\r特点：软聚类，一个节点可以属于多个组，也不需要规定分组的数量。作者认为单个文本段通常包含与多个主题相关的信息，所以需要保证将它们可以包含在多个摘要中。\n聚类算法基于[高斯混合模型 (GMM)](#1. 高斯混合模型（GMM）)，这种方法既提供了灵活性，又提供了概率框架。GMM 假设数据点是由几种高斯分布的混合生成的。\n给定一组 $N$ 个文本片段，每个文本片段都表示为一个 $d$ 维稠密向量嵌入，假设文本向量 $x$ 属于第 $k$ 个高斯分布的概率表示为$P(x|k)=\\mathcal{N}(x;u_k,\\Sigma_k)$ 整体概率分布是加权和$P(x) = \\sum_{k = 1}^{K}\\pi_k\\mathcal{N}(x;u_k,\\Sigma_k)$，其中 $\\pi_k$ 表示第k个高斯分布的混合权重。\n但是由于距离度量在计算高维空间中向量相似性时可能表现不佳，高维向量嵌入对传统的GMMs产生了挑战。作者引入[均匀流形近似和投影(UMAP)](#2. 均匀流形近似和投影（UMAP）)，并通过最近邻参数n_neighbors调整局部结构和全局结构之间的平衡。作者的算法通过变化n_neighbors来创建层次聚类结构：首先识别全局聚类，然后在这些全局聚类中执行局部聚类。这种两步聚类过程捕捉了文本数据之间的广泛关系，从广泛主题到具体细节。\n如果局部聚类的组合上下文超过了总结模型的token限制，作者的算法会在该聚类内递归应用聚类，确保上下文保持在token限制内。\n作者使用[贝叶斯信息准则(BIC)](#3. 贝叶斯信息准则（BIC）)来确定最佳聚类数。BIC不仅惩罚模型复杂性，还奖励拟合优度。对于给定的GMM，$\\text{BIC} = k \\cdot \\ln(N) -2 \\cdot \\ln(\\hat{L})$，其中 $N$ 为文本片段(或数据点)的数量，$k$ 为模型参数的数量，$\\hat{L}$ 为模型似然函数的最大化值。在GMM的情境中，参数数量 $k$ 是输入向量的维度和聚类数量的函数。\n得到由BIC确定的最佳聚类数量后，使用\rEM算法来估计GMM参数，即均值、协方差和混合权重。虽然GMM中的高斯假设可能与文本数据的性质不完全符合，后者通常表现出稀疏和偏斜的分布，但实证观察表明，它是一种有效的模型。\n模型摘要 #\r在每次使用高斯混合模型进行聚类后，每个组的节点使用GPT3.5-Turbo模型生成摘要。\n所有数据集的摘要长度与子节点长度之和的平均比率为 0.28，表明压缩率为 72%。平均而言，摘要长度为 131 个token，平均子节点长度为 86 个token。\n使用的prompt：\n查询 #\rRAPTOR使用两种查询机制：树遍历（tree traversal）和折叠树（ collapsed tree）。所有的向量嵌入都是通过SBERT完成的。\n树遍历 #\r树遍历先根据查询嵌入的余弦相似度找出TopK个最相关根节点。然后在这些节点的所有子节点中再选出TopK个最相关节点。重复这个步骤直到选到叶节点。最后所有被选出的节点（不止最后的叶节点，包括之前经过的所有节点）连接起来作为最后检索出的文本。\n即：\n在RAPTOR树的根层开始。计算查询嵌入与该初始层中所有节点的嵌入之间的余弦相似度。 基于最高余弦相似度得分选择前k个节点，形成集合$S_1$ 继续处理集合$S_1$ 中元素的子节点。计算查询向量与这些子节点的向量嵌入之间的余弦相度。 在这些子节点中选择余弦相似度最高的TopK个，构成集合$S_2$ 对树的d层递归的进行此操作，形成集合$S_1,S_2,..,S_d$ 把集合$S_1,S_2,..,S_d$连起来，作为最后检索出的文本进行上下文回答。 这个方法通过调整检索深度 $d$ 和每层检索TOP $K$ 控制检索的精度和广度（层数越深越精细，每层越多越广泛嘛）。这是一种自上而下的，从广泛到具体的检索方法（从最概括的根节点向文本片段的叶节点进行逐级检索）。\n折叠树 #\r折叠树的方法就比较简单粗暴，直接把之前构建的所有结点（根节点、叶节点和中间节点）看作相同的等级，全部计算与查询的余弦相似度，选取 TopK 构成检索出的文本。\n即：\n把构建的RAPTOR树看成一层（折叠？拍扁？），表示为 $C$ ，包含之前树中的所有节点。 计算查询向量和 $C$ 中所有节点嵌入向量的余弦相似度 选择余弦相似度最高的TopK节点。==（一直放的话TopK有啥意义？）==往检索结果中放节点，知道达到预定的最大Token数，，确保不超过模型的输入限制。 效果 #\r作者在QASPER数据集上的20篇文章中测试上面两种方法，图3展示了不同TopK下的树遍历和不同最大token数限制下的树折叠方法的性能曲线。可以看到在相同的最大token数限制下，树折叠性能多数更好。其实不难理解，原因有二：\n树折叠相当于直接对全树的所有节点进行了相似度计算，而树遍历只对树的某一部分进行了计算。 树折叠没有对概括性信息（靠近根节点）和精确性信息（靠近叶节点）的比率进行限制，而在树遍历方法中，当遍历层数 $d$ 与 Top $K$ 被设定好后，概括性信息和精确性信息的比例也被固定死了。 所以总体来看，树折叠更加的全面灵活，但是计算消耗肯定相对更大，作者提到使用快速 k 最近邻库可以提高效率，如 FAISS（Johnson et al., 2019）。\n定性来看，相较于稠密段落检索（DPR），RAPTOR的优势在于其可以根据问题的深入度匹配不同精确程度的文本片段，这种方法通常为下游任务提供更相关、更全面的信息。\n实验 #\r数据集 #\rNarrativeQA：\n基于书籍和电影记录全文的问答对组成，共计 1,572 个文档。\nNarrativeQA-Story 任务需要对整个文章有全面的理解，以便准确回答其问题，从而测试模型在文学领域中理解较长文本的能力。作者使用标准 BLEU （B-1， B-4）、ROUGE （R-L） 和 METEOR （M） 指标来衡量此数据集的性能。\nQASPER：\n包括 1,585 篇 NLP 论文中的 5,049 个问题，每个问题都基于全文的信息。\nQASPER 中的答案类型分为可回答/无法回答、是/否、摘要和提取。精度使用标准 F1 测量。\nQuALITY：\n由多项选择题组成，每个选择题都附有平均长度约为5,000个tokens的上下文段落。该数据集要求对整个文档进行推理以执行 QA 任务，使我们能够衡量检索系统在中等长度文档上的性能。该数据集包括一个具有挑战性的子集 QuALITY-HARD，其中包含大多数人类标注者在快速回答中回答错误的问题。文章同时对整个测试集和 HARD 子集测试准确性。\n具体操作 #\r作者先使用UnifiedQA 3B作为阅读器，SBERT、BM25和DPR作为嵌入模型在三个数据集生进行受控对比，结果表明，当RAPTOR与任何检索器结合时，在所有数据集上均表现出优于各自检索器的一致性（论文中表1表2）。同时RAPTOR与SBERT结合具有最佳性能，后续实验也将使用这个组合。\n然后，作者使用三种不同的LLM：GPT-3、GPT-4 和 UnifiedQA，在三个数据集上将将 RAPTOR 与 BM25 和 DPR 进行比较。\n在QASPER数据集中RAPTOR表现尤其出色 ，三种LLM上都超过了BM25和DPR（表3），而且结合GPT-4超过当时SOTA结果（CoLT5 XL，表5）。看来RAPTOR的分级别摘要节点是十分适合该数据集的全文信息提问的。\n在QuALITY数据集中，如表4所示，RAPTOR的准确率达到62.4%，比DPR和BM25分别提高了2%和5.1%。当使用UnifiedQA时也观察到类似的趋势。与GPT-4的结合大幅领先于当时的SOTA结果（CoLISA，表7）\n在 NarrativeQA 数据集上，RAPTOR + UnifiedQA 在 METEOR 分数上超过了其他模型。（？）\n树结构贡献的探究 #\r定性分析 #\r作者使用关于1500字童话灰姑娘故事的，关于主题的和多跳的问题，将RAPTOR检索的上下文与稠密段落检索(DPR)检索的上下文进行了比较。如图4，RAPTOP检索的段落加框，DRP检索段落为箭头指示。可以看到，RAPTOR根据手头问题所需的粒度级别从不同层中选择节点。此外，由DPR检索的信息通常包含在RAPTOR检索的上下文中，或者直接作为叶节点，或者间接作为来自更高层的摘要的一部分。\n文章给出两个具体例子，第一个是“灰姑娘如何找到一个幸福的结局?”，这是一个多跳问题，需要通过综合各种文本片段的信息来回答。第二个问题是“故事的中心主题是什么?”，这是一个主题问题，需要对全文进行整体理解。表13展示了两种模型返回的检索相关文档，其实结果很明显，在这种需要结合多段文本的问题上，限制RAPTOP的是摘要的质量，而DPR很可能返回一个片面的片段用于回答问题。\n定量分析： #\r作者对所有三个数据集和三种不同的检索器进行了消融研究，使用RAPTOR和折叠树检索来检查检索到的节点对应的层，观察到18.5%到57%的检索节点来自非叶节点。\n如图7所示，跨层的检索模式揭示了RAPTOR多层树结构的重要性。值得注意的是，RAPTOR使用DPR检索器为NarrativeQA数据集检索的节点中有很大一部分来自树的第一层和第二层，而不是叶子节点。这种模式在其他数据集和检索器中是一致的，尽管百分比不同。\n聚类机制的消融实验 #\r对于RAPTOR，采用了典型的聚类和摘要过程。相比之下，另一种设置涉及通过递归编码和总结相邻文本块来创建平衡树。选择了7个节点的窗口大小，折叠树方法应用于两种模型的检索。消融研究的结果见表9。该消融研究的结果明确显示在利用RAPTOR的聚类机制时，准确性有所提高。\n幻觉分析 #\r作者主要分析了在摘要生成时产生幻觉的概率，并判断这种幻觉是否会对QA任务产生影响。\n通过随机抽样和人工比对的方式，得出幻觉产生比例约为4%（6/150），而且据作者观察，幻觉不会传播到更高层的摘要中。所以作者得出结论：一般来说，幻觉是轻微的，不会改变文本的主题解释，幻觉对QA任务的表现也没有明显的影响，幻觉并不是RAPTOR架构中总结组件的主要关注点。\n总结 #\r本文主要提出了一种基于树结构的检索系统——RAPTOR，其亮点在于通过递归聚类和摘要模型构建了一个分层次总结文本的查询树，可以满足多种细粒度的查询需要，在根据全文整体内容进行提问的问题中表现突出。（就是实验除了与SOTA比较的部分外主要比较的都是直接检索某一精确文本的方法，要是能更多比较针对全文问题的方法就更好了）\n补充信息 #\r1. 高斯混合模型（GMM） #\r高斯混合模型（Gaussian Mixture Model，GMM）是一种基于概率统计的聚类模型。它假设数据是由多个高斯分布（正态分布）混合而成，每个高斯分布代表数据的一个聚类。GMM 可以用于对数据进行聚类分析，也常用于概率密度估计。\n关键概念 #\r高斯分布: 高斯分布是一个连续概率分布，通常呈钟形曲线（正态分布）。它由两个参数控制：\n均值: 决定分布的中心位置。 方差: 控制分布的宽度。 混合模型: GMM 是多个高斯分布的加权组合，每个分布对应一个潜在类别。每个分布有不同的均值和方差，并且被赋予一个权重（表示该分布在总体中的比例）。\n软分类: 与传统的聚类算法（如 K-means）不同，GMM 进行的是软分类。每个数据点属于每个簇的概率不同，而不是硬分配给某个单一簇。\nEM 算法: GMM 通常使用期望最大化（Expectation-Maximization，EM）算法进行参数估计。EM 算法迭代进行以下步骤：\nE 步骤: 计算每个数据点属于每个高斯分布的概率（即后验概率）。 M 步骤: 根据 E 步骤的结果，更新每个高斯分布的参数（均值、方差、权重）。 GMM 的应用 #\r聚类: 通过识别数据中的不同高斯分布，GMM 可以进行聚类分析。 密度估计: GMM 可以用于建模复杂的概率密度函数，从而进行异常检测或生成新数据。 语音识别、图像处理: GMM 广泛用于这些领域进行建模和特征提取。 由于 GMM 能够捕捉不同形状、大小和方向的簇，它比 K-means 更灵活，适用于更复杂的数据分布。\n向量嵌入的高维度的挑战 #\r1. 维度灾难（Curse of Dimensionality） #\r高维数据往往会导致维度灾难问题。GMM 假设数据的每个聚类都服从多元高斯分布，而在高维空间中，多元高斯分布的参数（如均值向量和协方差矩阵）需要估计的数量随着维度的增加急剧增长。这种情况会导致以下问题：\n参数估计困难: 在高维空间中，需要估计的协方差矩阵的参数数量是维度的平方，容易出现过拟合或不稳定的估计。 数据稀疏性: 高维空间中，数据点之间的距离趋于均匀，导致难以区分不同的聚类。 2. 协方差矩阵的计算复杂性 #\r在高维度下，GMM 的每个分布都包含一个协方差矩阵。对于 n 维数据，协方差矩阵是一个 (n \\times n) 的矩阵，需要估计 (n(n + 1)/2) 个参数。计算协方差矩阵的逆矩阵的复杂度是 (O(n^3))，这在高维情况下会导致计算效率显著降低。此外，协方差矩阵在高维度下也容易出现数值不稳定性，影响模型的准确性。\n3. 数据量与维度的比例问题 #\r在高维空间中，为了准确估计 GMM 的参数，通常需要大量的数据。然而，在实际应用中，数据量往往不足以支撑高维度下精确的参数估计，这可能导致模型泛化能力差、过拟合或收敛到局部最优解。\n4. 高维数据的可分性和聚类效果 #\rGMM 在高维数据中可能难以识别实际存在的复杂结构，因为高维数据的可分性变差。GMM 假设数据符合多元高斯分布，但在高维嵌入空间中，数据的分布通常不符合这一假设，导致聚类效果较差。此外，高维嵌入向量可能具有非线性结构，而 GMM 作为线性模型难以捕捉这种结构。\n5. 计算时间和内存开销 #\r高维度增加了 GMM 的训练时间和内存需求。对于大规模高维数据集，模型训练和预测的计算复杂度显著上升，这可能使 GMM 不适用于实时或大规模数据处理任务。\n解决方案和改进 #\r为了应对高维度带来的挑战，可以考虑以下方法：\n降维: 在应用 GMM 之前，使用降维技术（如 PCA 或 UMAP）将高维嵌入向量映射到较低维度的空间。 稀疏或对角协方差矩阵: 通过简化协方差矩阵（如对角协方差或稀疏协方差）来降低计算复杂度和参数数量。 贝叶斯 GMM: 通过引入先验分布，贝叶斯 GMM 可以减少过拟合，并更稳健地估计参数。 高维向量嵌入在传统 GMM 中确实构成了显著挑战，通常需要结合其他技术进行优化和改进。\n相关资料 #\r高斯混合模型（GMM） - 知乎 (zhihu.com)\n使用高斯混合模型，让聚类更好更精确（附数据\u0026amp;代码\u0026amp;学习资源） | 机器之心 (jiqizhixin.com)\n2. 均匀流形近似和投影（UMAP） #\rUniform Manifold Approximation and Projection（UMAP）是一种用于降维和可视化的非线性技术。它能将高维数据投影到低维空间，同时尽量保留数据的局部和全局结构关系，常用于数据探索和可视化任务。\nUMAP 的关键概念 #\r流形学习（Manifold Learning）: UMAP 基于流形假设，即高维数据实际上存在于一个低维的流形结构中。UMAP 尝试将数据的这种低维流形嵌入揭示出来，并在降维过程中保留这些结构特征。\n邻域图（Neighborhood Graph）: UMAP 首先构建一个邻域图，表示数据点在高维空间中的邻居关系。它使用 k 最近邻（k-NN）方法来确定每个数据点的邻居，并计算邻居之间的距离权重。\n非线性投影: 在降维过程中，UMAP 通过优化一个目标函数来保持邻域图在低维空间中的结构，即保留原始高维空间中的相似性和局部结构。\n均匀化和拓扑学方法: UMAP 使用拓扑学中的概念（如单纯复形、1-skeleton）进行数据建模，同时通过均匀化调整来控制投影的形状和分布。\nUMAP 的主要特点 #\r保留局部和全局结构: UMAP 不仅关注局部邻居关系，还能保留全局数据结构，因此在高维数据映射到低维空间时表现良好。 高效性和可扩展性: UMAP 在大数据集上的表现优异，能够处理数百万个数据点。 可调节性: UMAP 允许用户通过调整超参数（如邻域大小、最小距离）来控制降维的效果和数据在低维空间中的分布密度。 在 UMAP (Uniform Manifold Approximation and Projection) 中，n_neighbors 是一个关键参数，它决定了在构建邻域图时，每个数据点要考虑的最近邻数据点的数量。这个参数对 UMAP 的性能和降维效果有重要影响。\nn_neighbors参数作用 #\r局部 vs. 全局结构: n_neighbors 控制 UMAP 在降维过程中是更关注局部结构还是全局结构。较小的 n_neighbors 值让 UMAP 更加关注数据的局部邻域，保留局部的细节结构；较大的 n_neighbors 值则让 UMAP 更加关注全局结构，将数据更均匀地分布在低维空间。\n流形假设的范围: n_neighbors 还决定了 UMAP 在数据中假设流形结构的局部性。流形假设认为数据嵌入在一个低维的流形中，这个假设在一定的局部范围内更容易成立。n_neighbors 的大小直接影响 UMAP 如何定义这个“局部”的范围。\n参数选择的影响 #\r较小的 n_neighbors（如 5-15）: UMAP 会保留数据的局部结构，适合用于揭示数据中的小尺度模式或局部簇结构。这种设置适合有较多小簇的数据，但可能会丢失全局结构。 较大的 n_neighbors（如 50-200）: UMAP 会更关注全局结构，适合用于展现数据的整体分布模式，适合探索更大的群体或全局趋势。但过大的 n_neighbors 可能会使得局部细节被平滑，影响局部簇的分辨能力。 UMAP 与 t-SNE 的比较 #\rUMAP 和 t-SNE 都是常用于可视化高维数据的降维算法，但有一些关键区别：\nUMAP 通常在处理大数据集时速度更快，并且能够保留更多的全局结构信息。 t-SNE 在展示局部结构方面表现出色，但可能导致聚类间隔不均匀，难以解释全局关系。 UMAP 的应用 #\rUMAP 被广泛应用于各类高维数据的分析和可视化，例如：\n生物信息学: 分析基因表达数据、细胞数据等。 图像和文本处理: 可视化嵌入向量。 聚类分析: 降维后进行聚类或探索数据中的潜在模式。 UMAP 在保持数据结构、处理大规模数据集以及灵活性方面具有显著优势，因此在数据科学和机器学习领域得到了广泛应用。\n相关资料 #\r[\r1802.03426] UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction (arxiv.org)\nUMAP:均匀流形逼近和投影的降维方法 - 朱晓旭的博客 (zhuxiaoxuhit.github.io)\n3. 贝叶斯信息准则（BIC） #\r贝叶斯信息准则（Bayesian Information Criterion, BIC）是一种用于模型选择的准则，广泛应用于统计建模和机器学习中。BIC 平衡了模型的拟合程度和复杂度，用来选择在给定数据集上表现最优的模型。\nBIC 的定义 #\rBIC 的计算公式为： $$ \\text{BIC} = -2 \\cdot \\ln(L) + k \\cdot \\ln(n) $$ 其中：\n$L$ 是模型的最大似然估计，对应模型在给定数据下的拟合优度。 $k$ 是模型中的参数数量，反映模型的复杂度。 $n$ 是样本的数量。 BIC 的含义 #\r第一项：拟合度 ( $-2 \\cdot \\ln(L)$ ): 这一项反映模型对数据的拟合程度，拟合越好，BIC 值越小。 第二项：惩罚项 ( $k \\cdot \\ln(n)$ ): 这一项是对模型复杂度的惩罚，参数越多，BIC 值越大。通过引入这一项，BIC 可以避免过拟合。 BIC 的使用 #\rBIC 在模型选择时遵循“越小越好”的原则。在多个候选模型中，BIC 值最小的模型通常被认为是最优的，因为它在平衡模型拟合和复杂度方面表现最好。\nBIC 的优点和局限性 #\r优点:\nBIC 考虑了模型复杂度，能够在模型拟合和过拟合之间取得平衡。 在大样本情况下，BIC 的表现通常优于其他信息准则（如 AIC，Akaike Information Criterion），因为 BIC 中的惩罚项随样本量增加。 局限性:\nBIC 假设数据独立同分布，且模型中使用的似然估计是准确的。 BIC 更倾向于选择简单的模型，尤其是在样本量较大时，可能会忽略更复杂但更准确的模型。 应用场景 #\rBIC 广泛应用于以下场景：\n模型选择: 在回归分析、聚类分析（如 GMM 中选择最佳簇数）等任务中，用于评估不同模型。 特征选择: 在选择变量时，BIC 可以帮助确定最佳特征组合。 BIC 是一个强有力的模型选择工具，能够在不同模型之间进行有效的权衡，并且在统计建模和机器学习中的广泛应用表明其在实践中的价值。\n","date":"2024年10月7日","externalUrl":null,"permalink":"/posts/raptor%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"使用树形结构存储不同级别的摘要信息以应对不同粒度的RAG查询，思路新颖有趣","title":"【笔记】RAPTOR: 用于树形组织检索的递归摘要处理","type":"posts"},{"content":"","date":"2024年10月7日","externalUrl":null,"permalink":"/tags/%E5%85%A8%E5%B1%80%E9%97%AE%E7%AD%94/","section":"Tags","summary":"","title":"全局问答","type":"tags"},{"content":"由于一些不可抗力，童鞋们在下载huggingface上的模型时可能面临无法下载、速度贼慢等问题，这时候就需要使用一些国内的镜像站进行下载，比如\rHF-Mirror。\nHF-Mirror官网给出了几种下载的方法，我试了几种，感觉hfd还比较好用，\rhfd 是\rHF-Mirror开发的 huggingface 专用下载工具，基于成熟工具 git+aria2，据说可以做到稳定下载不断线（其实我下载时还是断线了……）。\n本次主要记录下载 meta-llama/Llama-3.2-11B-Vision-Instruct 的过程，这个仓库还有访问限制，应该比较有代表性。\n1. 获取Access Token #\r先进入hf主站的token设置页面（当然你首先得有一个hf主站的账号密码）：\rHugging Face – The AI community building the future.\n添加一个新的access token\n设置一个名字，把能勾选的都勾选上，点击create token按钮生成密钥hf-****，复制下来备用。\n另外，有的模型需要单独申请下载（比如llama系列模型），需要在相关仓库中填写表单申请通过后才能下载。\n2. 下载并配置hfd #\r下载hfd脚本：\nwget https://hf-mirror.com/hfd/hfd.sh 还需要安装aria2c下载器，如果是linux系统，使用：\napt install aria2c git-lfs 如果是Windows系统，在cmd中使用:\nwinget install aria2 3.下载模型 #\rhfd.sh脚本在linux系统下应该可以直接运行，可能需要添加一下执行权限chmod a+x hfd.sh。\nwindows系统可以在git bash中执行sh脚本。\n如果你的git设置过代理，可能需要先把代理解除掉\n查看git代理设置：\ngit config --global --get http.proxy git config --global --get https.proxy git解除代理绑定\ngit config --global --unset http.proxy git config --global --unset https.proxy 一切准备就绪后，可以直接下载：\nHF_ENDPOINT=https://hf-mirror.com ./hfd.sh \u0026lt;hf模型路径\u0026gt; --hf_username \u0026lt;hf主站的用户名\u0026gt; --hf_token \u0026lt;第一步获取的access token\u0026gt; hf模型路径可看链接获取：\rhf用户名点击主站头像即可查看：\r示例：\nHF_ENDPOINT=https://hf-mirror.com ./hfd.sh meta-llama/Llama-3.2-11B-Vision-Instruct --hf_username Hugging-Leg --hf_token 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 ","date":"2024年10月7日","externalUrl":null,"permalink":"/posts/%E4%BD%BF%E7%94%A8hfd%E9%80%9A%E8%BF%87%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BDhugging-face%E4%B8%8A%E7%9A%84%E6%A8%A1%E5%9E%8B/","section":"文章","summary":"本次主要记录下载 Llama-3.2-11B-Vision-Instruct的过程，这个仓库还有访问限制，应该比较有代表性。","title":"使用hfd通过镜像下载hugging Face上的模型","type":"posts"},{"content":"\r原文题目：Zhuang 等 - 2024 - EfficientRAG Efficient Retriever for Multi-Hop Question Answering\n原文链接：\rEfficientRAG: Efficient Retriever for Multi-Hop Question Answering - ACL Anthology\n项目地址：\rNIL-zhuang/EfficientRAG-official: Code Repo for EfficientRAG: Efficient Retriever for Multi-Hop Question Answering\n解决问题 #\r之前解决多跳推理思路一般在于多轮检索重写或生成查询、交叉多个检索和推理步骤，让模型多次自我询问逐步提出问题。而上述方法普遍存在两个问题：\n1)它们需要多次LLM调用，涉及为下一轮检索重写或生成新的查询，从而增加了延迟和成本。\n2)针对不同的场景可能需要专门去设计prompt和few-shot例子。\n基本思路 #\r作者认为，多跳问题中的关系类型是有限的，或者与实体的数量相比要少得多。有论文证明小模型也具有一定的推理能力，那么可以通过小模型而不是LLM来有效地管理从检索信息中识别关系及其关联实体。（即，多跳问题的类型没有那么多，小模型也可完成相应的关系分解任务，没有必要上大模型，这样节省资源而且效率高）\n因此，作者使用小模型训练了一个标签器和一个过滤器来迭代生成新的检索查询，同时保留最相关的检索信息，与其他RAG方法相比，提高了效率。\n具体方法 #\rEfficientRAG核心在于加了两个两个轻量级组件:标签器（Labeler \u0026amp; Tagger）以及过滤器。他们模型结构是一样的，Labeler 和 Tagger 通过同一模型的不同输出头生成结果，过滤器是另外训练的一个模型。Labeler和过滤器都作为token级别的二分类器，将token分类为真或假，Tagger是文档块级别的分类器。\n在给定一个问题后，检索器在数据库中找到一些相关的文本块。然后Labeler在文档中标记一些token序列，表示这些token对回答问题有作用。\nTagger用于标记整个文本块对回答问题是有用还是没用，如果在这个文本块的基础上还需要更多的信息来回答问题，就标记为\u0026lt;Continue\u0026gt; 并把该块放到候选池中。如果文档被标记为无用或不相关，将停止搜索此查询的后续分支。\n过滤器模块接受Labeler标记的token和当前查询，为下一轮检索构造一个新的查询。\n以此循环，直到获得足够的信息来回答初始问题，它就会停止并将所有这些信息传递给最终生成器（LLM）以获得最终回答。除了最后一次调用外，其他的操作都是小模型完成的，提高了性能。\n训练 #\r数据准备 #\r至于定制化训练的数据，当然还是使用LLM构建。作者利用 LLM 来合成Labeler和过滤器的训练数据。该过程包括多跳问题分解、标记标记、下一跳问题过滤和负采样。综合数据详见表1。\n多跳问题分解：给定一个多跳问题和相关块，作者首先提示LLM将原始问题分解为几个单跳问题。每个单跳问题对应一个块。然后，要求LLM解析子问题的依赖关系。\n**Labeler token标记：**对于每个子问题和相应的块，作者提示LLM标记与子问题答案相关的块中的重要单词。使用一个二进制标签对数据块中的每个单词进行注释，以确定它是否重要，是否应该由EfficientRAG Labeler保留。\n**过滤器的下一跳问题：**给定一个单跳问题及其相关问题的标记令牌，提示LLM生成一个下一跳问题，这是理想的下一个检索查询，提取下一跳问题的token即可。\n**Tagger 负采样：**对于每个过滤后的下一跳问题，作者检索最相似但不相关的块作为硬负样例。这些否定的数据块将被标记为\u0026lt;Terminate\u0026gt;，而其他相关的数据块将被标记为\u0026lt;Continue\u0026gt;。\n训练任务 #\r作者训练了 EfficientRAG Labeler 用于两个任务：token标注和块过滤，因为它们使用相同的输入。训练使用自动编码器语言模型作为编码器，以获得连接的查询序列和块中每个词元的嵌入。随后，作者使用一个全连接层将词元嵌入投射到二维空间，用以区分“有用token”和“无用token”。另一个全连接层则用于将序列嵌入的平均池化投射到二维空间，表示块的tag \u0026lt;Continue\u0026gt; and \u0026lt;Terminate\u0026gt;。EfficientRAG 过滤器训练方式类似，其输入序列是查询和标注token的拼接。该过滤器提取单词并将其拼接，以形成下一跳查询。\n实验 #\r数据集 #\r数据集使用 HotpotQA、MuSiQue 和 2WikiMultihopQA，具体数据集的介绍可以参考之前的文章。\nBaseline #\r不用检索直接使用大模型自身回答，使用直接提示、思维链提示和问题分解等方法。 使用采用前10相关的文档块的朴素RAG 之前的一些方法，如 Iter-RetGen (Shao et al., 2023) 和 SelfAsk (Press et al., 2023)。 验证模型 #\r作者基于 DeBERTa-v3-large (He et al., 2021，24层，参数量340M)微调 Labeler 和 Filter ，用Contriever-MSMARCO(Izacard et al.,2022)作为数据合成和推理阶段的检索器。采用lama-3- 8b - instruction回答问题和并作为其他所有基准。\n实际训练方面，作者使用Llama-3-70B-Instruct构建了训练数据(附录B.2)。在4× Nvidia A100 gpu上分别训练了大约10个gpu小时，使用AdamW (Loshchilov和Hutter, 2019)优化器，学习率为5e-6。\n结果 #\r检索与问答 #\r首先表2对比了EfficientRAG与之前方法的检索性能，可以看到在HotpotQA与2WikiMQA上效果不错，但是在MuSiQue上表现一般，作者认为是可能归因于检索的块数量较少和数据集的复杂性增加。\n在实际检索问答上，EfficientRAG在准确度上都能接近最佳效果，如果将问答模型换为GPT3.5，还可以进一步提高性能，在HotpotQA上达到了SOTA效果（表5）。\n推理效率 #\r作者从MusiQue数据集中随机选择了200个样本进行研究，计算了四个指标:LLM调用次数、迭代次数、回答延迟和GPU利用率。可以看到EfficientRAG在回答延迟方面有较高优势，因为只需要调用一次大模型。\n域外泛化 #\r作者提出了一个跨HotpotQA和2WikiMQA数据集的域外实验，在一个数据集上训练模型，并在另一个数据集上测试它。从表6可以看出，EfficientRAG对不同的数据集有很好的适应性，在某些情况下甚至超过了在原始数据上训练的模型。这表明，EfficientRAG不依赖于特定领域的知识，表现出一定程度的可移植性。\n讨论 #\rRAG本质上的两个方面：IR+RC\nIR的实际上就是说我如何去将最准确、最相关的信息提供给大模型而过滤掉不相关的干扰信息 ，在保证包含关键文档的情况下，信息“纯度”越高，RAG性能越好。而IR现在结合大模型相较原来传统的研究也多了很多方法，很多RAG的研究本质上都是在卷IR（包括这一篇）。\nRC就是让LLM在现在检索到的文本上自己去进行整合和理解，当然这个东西随着LLM自己的迭代与规模的扩大本来性能就会慢慢提升，这也导致了一个问题是你直接去调LLM提升其RC性能基本不太可能，计算资源不允许，现在我看论文能做的就是用一些提示的方法去引导大模型进行逻辑思考和推理，可能本文提到的外挂插件也可以是一种思路。\n读完本篇文章，有以下感受：\n这个方法只算提升了回答速度，只要需要调用LLM，显存和GPU占用是下不来的。 虽然本文主打的是高效率，但我还是想看一看在不考虑效率，将外部插件全部使用LLM的情况，准确率等是否会有进一步提升（话说为啥没有消融实验？）。但其实这样和self-rag没啥区别了。 感觉大小模型协同的方式，工作量是上去了，因为Scaling law的问题，最终效果上可能很难说能做出碾压纯大模型的方法，最后只能在问答效率上做文章，这也是高校研究大模型缺少算力的无奈之举吧。 补充 #\rRecall@k #\rRecall@k 是信息检索和推荐系统中常用的评价指标，用于评估系统在前 k 个返回结果中检索到相关项的能力。具体来说，Recall@k 衡量的是在返回的前 k 个结果中有多少是相关的。\n公式 Recall@k = (在前 k 个结果中相关项的数量) / (总相关项的数量)\n举例\n真实情况： 疾病A的治疗药物是：药物1，药物2，药物3，药物4. 疾病B的治疗药物是：药物5，药物6，药物7 模型预测情况： 疾病A的治疗药物是：药物2，药物5，药物1，药物6，药物3，药物7（按照推荐的可能性排序） 疾病 B的治疗药物是：药物7，药物8，药物5，药物6（按照推荐的可能性排序） 若K=3,则选取模型推测结果的前三个，即： 疾病A：药物2，药物5，药物1 疾病B：药物7，药物8，药物5 所以： 对于疾病A：药物2，药物1是相关项 对于疾病B：药物7，药物5是相关项 recall@k = (2/4 + 2/3) /2 = 0.583 参考：\r分类模型、回归模型的常见评价指标_预测分类模型评价指标代码-CSDN博客\n","date":"2024年10月6日","externalUrl":null,"permalink":"/posts/efficientrag%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"介绍了一个大小模型协同的高效RAG方法，使用BERT量级的模型辅助RAG任务，节省了资源的开销。","title":"【笔记】EfficientRAG：多跳问题回答的高效检索器","type":"posts"},{"content":"","date":"2024年9月25日","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024年9月25日","externalUrl":null,"permalink":"/series/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","section":"Series","summary":"","title":"博客搭建","type":"series"},{"content":"","date":"2024年9月25日","externalUrl":null,"permalink":"/tags/%E5%BB%BA%E7%AB%99/","section":"Tags","summary":"","title":"建站","type":"tags"},{"content":"","date":"2024年9月25日","externalUrl":null,"permalink":"/tags/%E5%89%8D%E7%AB%AF/","section":"Tags","summary":"","title":"前端","type":"tags"},{"content":"一个博客怎么能没有评论系统！Giscus是利用 GitHub Discussions 实现的评论系统，让访客可以借助 GitHub 在网站上留下评论和反应。部署起来还是比较简单的，其具有一下特点（来自官网）：\n开源。🌏 无跟踪，无广告，永久免费。📡 🚫 无需数据库。所有数据均储存在 GitHub Discussions 中。 支持\r自定义主题！🌗 支持\r多种语言。🌐 高可配置性。🔧 自动从 GitHub 拉取新评论与编辑。🔃 可自建服务！🤳 1. 创建并配置Github仓库 #\rGiscus需要依靠一个Github仓库来进行部署，仓库需要满足以下三个条件：\n该仓库是\r公开的，否则访客将无法查看 discussion。 giscus app 已安装，否则访客将无法评论和回应。 Discussions 功能已\r在你的仓库中启用。 1.1 创建一个公开的Github仓库 #\r直接在Your repositories 下new一个就可以，记得需要选择public\n1.2 为仓库启动Discussions功能 #\r找到Settings -\u0026gt; General -\u0026gt; Features ，勾选Discussions\n1.3 为仓库安装Giscus #\r进入这个网址：\rGitHub Apps - giscus，点击安装\n点击安装后，选择Only select repositories，选择刚才创建的仓库\n2. 从官网获取配置信息 #\r评论区的html代码与配置信息可以在官网进行自动配置获取，首先进入官网：\rgiscus\n2.1 填写仓库名 #\r如图所示\n其他配置先默认\n2.2 配置Discussion 分类 #\r选择 Announcements 类型即可\n2.3 获取html代码与配置信息 #\r在启用giscus栏中，即可看到相应代码。\n要记下data-repo，data-repo-id，data-category，data-category-id，data-mapping这几个值。\n3. hugo配置 #\r这里使用的 Hugo 一定要是最新的版本，不然是不支持 Giscus 的\n3.1 创建html文件 #\r在hugo项目下 themes/\u0026lt;你的主题文件夹\u0026gt;/layouts/partials/目录下新建 comments.html 文件，填入刚才在官网获取的script脚本代码内容。\n如果使用blowfish主题，具体路径应该是\\themes\\blowfish\\layouts\\partials\\\n在hugo项目下themes/\u0026lt;你的主题文件夹\u0026gt;/layouts/_default/single.html文件下，加入{{ partial \u0026quot;comments.html\u0026quot; . }}\n{{ partial \u0026#34;header.html\u0026#34; . }} {{ .Content }} \u0026lt;footer class=\u0026#34;footline\u0026#34;\u0026gt; {{with .Params.LastModifierDisplayName}} \u0026lt;i class=\u0026#39;fas fa-user\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;a href=\u0026#34;mailto:{{ $.Params.LastModifierEmail }}\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{with $.Date}} \u0026lt;i class=\u0026#39;fas fa-calendar\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; {{ .Format \u0026#34;02/01/2006\u0026#34; }}{{end}} \u0026lt;/div\u0026gt; {{end}} \u0026lt;/footer\u0026gt; {{ partial \u0026#34;comments.html\u0026#34; . }} {{ partial \u0026#34;footer.html\u0026#34; . }} 如果使用blowfish主题，则无需进行这项配置（原本文件中就有）\n3.2 toml配置修改 #\r进入\\config\\_default\\params.toml，按照如下格式，填入在官网获取到的配置：\n[giscus]\rdata-repo=\u0026#34;[自动生成]\u0026#34;\rdata-repo-id=\u0026#34;[自动生成]\u0026#34;\rdata-category=\u0026#34;[自动生成]\u0026#34;\rdata-category-id=\u0026#34;[自动生成]\u0026#34;\rdata-mapping=\u0026#34;pathname\u0026#34;\rdata-strict=\u0026#34;0\u0026#34;\rdata-reactions-enabled=\u0026#34;1\u0026#34;\rdata-emit-metadata=\u0026#34;0\u0026#34;\rdata-input-position=\u0026#34;top\u0026#34;\rdata-theme=\u0026#34;preferred_color_scheme\u0026#34;\rdata-lang=\u0026#34;zh-CN\u0026#34;\rdata-loading=\u0026#34;lazy\u0026#34;\rcrossorigin=\u0026#34;anonymous\u0026#34; 如图：\n如果你是blowfish主题，还需要在[article]配置项中加入一行： showComments = true\n这样就可以显示评论啦\n参考文献 #\rHugo 添加 Giscus 评论 (stilig.me)\n在Hugo上配置giscus评论_hugo 评论 giscus-CSDN博客\ngiscus\nGitHub Apps - giscus\n配置 · Blowfish\n局部模板(Partials) · Blowfish\n","date":"2024年9月25日","externalUrl":null,"permalink":"/posts/%E4%B8%BAhugo%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0gisus%E8%AF%84%E8%AE%BA%E5%8A%9F%E8%83%BD/","section":"文章","summary":"总算是有评论功能了！","title":"为hugo博客添加Gisus评论功能","type":"posts"},{"content":"","date":"2024年9月4日","externalUrl":null,"permalink":"/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/","section":"Tags","summary":"","title":"课程笔记","type":"tags"},{"content":"","date":"2024年9月4日","externalUrl":null,"permalink":"/tags/%E6%95%B0%E5%AD%A6/","section":"Tags","summary":"","title":"数学","type":"tags"},{"content":"\r第一章：预备知识 #\r1. 什么是最优化？ #\r说白了：就是目前已知条件下的最优/最小/极值问题（如下例子）\n复习：拉格朗日乘子法 #\r对于 $f(x_1,x_2,\u0026hellip;)$，在等式$g(x_1,x_2,\u0026hellip;) = 0$ 的约束下，使用 $\\lambda$ 转化为求拉格朗日函数的极值问题\n即： $$ L = f(x_1,x_2,\u0026hellip;)+ \\lambda g(x_1,x_2,\u0026hellip;) $$ 分别对$x_1,x_2,\u0026hellip;,和\\lambda$ 求偏导数，得到方程组\n解出极值点即可\n经典极值问题一般可以有以下分类：\n在后面学习中，我们会认识到更多的最优化问题。具体来说，最优化问题可以有以下分类：\n我们一般研究静态问题，指的是问题的条件不会随着问题的发展而变化的问题。\n动态问题一般指问题的条件会随着问题的发展而变化的问题，即动态规划。\n2. 最优化问题的基本概念 #\r2.1 最优化问题的向量表示法 #\r在变量十分多的时候，无论是问题的抽象表示和运算都会变得十分麻烦。那么，我们就需要引入向量。\n那么，原理一大串的函数就被简化为一个向量+一个实值函数的形式了。\n向量的序关系 #\r在单纯的数值变量时，我们可以简单的直接比较两个值的大小。在引入向量后，我们需要定义向量之间的大小关系：\n**（定义1.1）**对于m维实向量：\n如果对应每一位，a都和b相等 $(a_i=b_i)$，则两向量相等。\n如果对应每一位，a都小于等于b, $(a_i \\leq b_i)$，则向量a小于等于向量b。\n如果对应每一位，a都严格小于b, $(a_i\u0026lt;b_i)$，则向量a小于向量b。\n对应位有大有小，无法比较\n向量值函数 #\r当我们讨论函数时，通常有两种主要类型：向量值函数和数量值函数。它们之间的区别在于函数的输出类型。\n数量值函数（Scalar-Valued Function）： 数量值函数是一种函数，其输出是一个标量或数量值，即单个数字。 例如，如果你有一个函数 f(x)，其中 x 是一个实数（或者是一个实数向量），并且 f(x) 返回一个实数，那么这就是一个数量值函数。例如，f(x) = x^2 是一个数量值函数，因为它将实数 x 映射到一个实数。 向量值函数（Vector-Valued Function）： 向量值函数是一种函数，其输出是一个向量，即多个数字组成的有序集合。 例如，如果你有一个函数 g(t)，其中 t 是一个实数，而 g(t) 返回一个包含两个分量的向量，例如 g(t) = [sin(t), cos(t)]，那么这就是一个向量值函数。在这个例子中，g(t) 输出一个包含两个实数（sin(t) 和 cos(t)）的向量。 总之，区分数量值函数和向量值函数的关键在于输出类型。数量值函数返回单个数字，而向量值函数返回一个向量，其中可以包含一个或多个数字。\n2.2 最优化问题的一般形式（考点：实际问题建模） #\r其中：\n“s.t.”是subject to 的缩写，表示 “满足于”。 f(x) 称为目标函数，s(x)称为不等式约束，h(x)称为等式约束 满足所以约束的向量称为 容许解或容许点。所有容许点的集合称为容许集 2.3 最优化问题的求解 #\r关于严格与非严格极小值，具体来说：\n严格极小：附近的都比它大\n非严格极小：附近的没有比它大的（但是可以有跟它一样小的）如下图：\n3. 二维问题的图解法（考点） #\r高维函数的等值面 #\r4. 梯度与Hesse矩阵（重点） #\r4.1 前置知识 #\r复习：向量内积 #\r向量内积的性质：\n4.2 多元函数的可微性 #\r考虑一个向量值函数 F: ℝⁿ → ℝᵐ，其中 ℝⁿ 表示 n 维实数向量的空间，而 ℝᵐ 表示 m 维实数向量的空间。函数 F 在某一点 x₀ ∈ ℝⁿ 处可微，如果存在一个线性变换 A: ℝⁿ → ℝᵐ，使得对于该点附近的所有向量 Δx，以下极限成立： $$ \\lim\\limits_{Δx→0} [F(x₀ + Δx) - F(x₀) - AΔx] = 0 $$ 其中，Δx 是一个小的向量，Δx→0 表示Δx趋近于零向量，A 是一个从 ℝⁿ 到 ℝᵐ 的线性变换（通常表示为一个矩阵），F(x₀ + Δx) 表示函数 F 在点 x₀ + Δx 处的值，而 F(x₀) 表示函数 F 在点 x₀ 处的值。\n简而言之，一个向量值函数在某一点可微，意味着在该点附近存在一个线性变换（矩阵 A），使得函数的改变与输入向量的改变之间的关系可以用该线性变换来近似描述。这类似于标量值函数可微的概念，但在这里我们处理的是向量而不是标量。\n4.3 梯度 #\r即各个分量的偏导数组成的向量，也称为函数 $f(\\vec{x})$ 关于 $\\vec{x}$ 的一阶偏导数\n梯度的性质 #\r当梯度$\\nabla f(\\vec{x})$ 连续时\n$\\textcolor{red}{函数在某点的梯度若不为零，则必与过该点的等值面\u0026rsquo;\u0026lsquo;垂直\u0026rsquo;\u0026rsquo;}$\n$\\textcolor{red}{梯度方向是函数具有最大变化率的方向}$\n梯度方向是函数值上升最快的方向，负梯度方向是函数值下降最快的方向，垂直梯度方向在函数等值面上\n以上两条分别说明了梯度的几何性质和数值性质\n方向导数 #\r这个定义可以理解为，我们在点 $x_0$ 处开始，然后沿着方向 $\\vec{e}$ 走一个微小的步长 $t$，然后观察函数值的变化。方向导数告诉我们，当我们沿着方向 $\\vec{e}$ 移动时，函数值$ f(x)$ 的变化率是多少。\n函数值升降的快慢则是由方向导数绝对值的大小决定的。绝对值越大，升或降的速度就越快；绝对值越小，升或降的速度就越慢。而梯度方向是函数值变化最快的方向\n方向导数的意义在于：\n确定最速增加方向：方向导数可以帮助找到函数在某一点上增加最快的方向。如果我们希望最大限度地增加函数值，我们可以沿着具有最大方向导数的方向前进。 优化问题：在优化问题中，方向导数对于确定梯度下降法中的步进方向很有用。梯度下降法是一种用于最小化函数的算法，它在每一步中选择梯度的负方向作为前进方向，以减小函数值。方向导数可以帮助确定更一般的前进方向。 判断函数在某一点的增长趋势：方向导数可以用于判断函数在某一点的增长或减小趋势。如果方向导数为正，说明函数在该点沿着所选方向增加；如果为负，说明函数在该点沿着所选方向减小。 常用梯度公式 #\r其中 $C$ 是常数，$Q$ 是对称方阵\n4.4 Hesse矩阵 #\r前面说过，梯度$\\nabla f(\\vec{x})$ 是函数$ f(\\vec{x})$ 关于$\\vec{x}$ 的一阶导数，那么，$ f(\\vec{x})$ 关于$\\vec{x}$ 的二阶导数是什么？\n由上文可知，梯度$\\nabla f(\\vec{x})$与变量 $\\vec{x}$ 维度相同的向量，那么，向量值函数的导数是如何定义的？\n由此可见，对于$\\mathbb{R}^n-\u0026gt;\\mathbb{R}^m$的向量值方程$g(\\vec{x})$ （即含有n个自变量，由m个方程输出m维向量的方程组），其一阶导数为一$n \\times m$ 的矩阵，元素为每个方程组分别对自变量求偏导。\n将对$\\mathbb{R}^n-\u0026gt;\\mathbb{R}^1$的$f(\\vec{x})$求梯度的的操作看作一向量值方程（将求偏导操作看作方程组的联立？）即可借助向量值函数导数的定义，得到对于n元函数 $f$ 的二阶导数的定义：\n具体来看，就是自变量两两组合 用 $f(\\vec{x})$ 依次对齐求偏导。\n求 Hesse矩阵步骤：\n对每个自变量求偏导，组成向量 刚才求偏导组成的方程组，再对每个自变量求偏导，组成矩阵 特殊函数导数公式 #\r复习：单位矩阵 #\r从左上角到右下角的对角线（称为主对角线）上的元素均为1。除此以外全都为0的方阵\n（3）的例子：\n5. 极值点判定定理（考点） #\r5.1 基本概念 #\r邻域：$x_0$内很小的一块区域\n局部极小点：有个邻域内我是值最小的（别管邻域多小，只要有个邻域就行）【高数中的极值点】\n严格局部极小点：周围都比我大\n非严格局部极小点：周围都没有比我小的\n注意，邻域表示要在定义域内\n全局极小点：在整个定义域内我值是最小的【高数中的最值点】\n严格全局极小点：我一定最小 非严格全局极小点：我是最小之一 当然，也存在极大值定义，但是我们可以加负号转成求极小\n驻点：可微情况下，梯度为零的点 5.2 极值的条件 #\r首先明确：$\\textcolor{red}{普通极值判断没有充要条件！}$\n（其实就是一维情况导数那一套的推广）\n一阶必要条件：局部极值点一定是驻点（反过来不一定） 二阶必要条件：定义域内的局部极值点，其 Hesse 矩阵一定是半正定的 p.s.驻点：一阶梯度为零向量的点\n复习：正定矩阵 #\r正定矩阵（Positive Definite Matrix）： 一个实对称矩阵 A 被称为正定矩阵，如果对于任意非零实向量 x，都有 x^T * A * x \u0026gt; 0，其中 x^T 表示 x 的转置。换句话说，一个正定矩阵的所有特征值都是正数。正定矩阵在优化、数值计算和统计学中具有重要应用，它们表示一个二次型函数的极小值点。\n半正定矩阵（Positive Semidefinite Matrix）： 一个实对称矩阵 A 被称为半正定矩阵，如果对于任意非零实向量 x，都有 x^T * A * x ≥ 0。半正定矩阵的特征值可以是非负数，但至少有一个特征值为零。半正定矩阵在最小化问题的约束条件、信号处理和统计学中有广泛应用。\n负定矩阵（Negative Definite Matrix）： 一个实对称矩阵 A 被称为负定矩阵，如果对于任意非零实向量 x，都有 x^T * A * x \u0026lt; 0。换句话说，负定矩阵的所有特征值都是负数。负定矩阵在一些特殊的数学问题中出现，但相对较少见。\n正定矩阵的判断\n要快速判断一个矩阵是否是正定、半正定或负定，可以使用以下方法：\n对称性检查： 首先，确保矩阵是对称矩阵，即 A = A^T，因为正定、半正定和负定的定义都是基于对称矩阵的。 特征值检查： 正定矩阵的所有特征值都应为正数。 半正定矩阵的所有特征值都应为非负数（可以包括零）。 负定矩阵的所有特征值都应为负数。 你可以使用计算工具或软件来计算矩阵的特征值，然后检查它们的符号。如果所有特征值都符合上述条件，那么你可以得出相应的结论。然而，这种方法在大型矩阵上可能会比较耗时。\n另一种更快速的方法是使用主元子矩阵的顺序主子式（leading principal minors）。对于一个 n x n 的对称矩阵 A，它的顺序 k 主子式是取前 k 行和前 k 列组成的子矩阵的行列式。根据顺序主子式的规律：\n如果所有的顺序主子式的行列式都大于零，则矩阵 A 是正定的。 如果所有的顺序主子式的行列式都大于等于零，则矩阵 A 是半正定的。 如果所有的顺序主子式的行列式交替符号（从正到负），则矩阵 A 是负定的。 这种方法在判断矩阵性质时更高效，因为你只需要计算一系列小的子矩阵的行列式，而不需要计算全部特征值。\n二阶矩阵正定快速判断\n二阶充分条件：①Hesse矩阵正定的②驻点一定是严格局部极小点\n二阶充分条件：当前点为驻点，且当前点的邻域内，处处半正定\n**注意：$\\textcolor{red}{在以后的大部分算法中，都是以局部极小点作为终止准则的}$ ** （甚至有可能终止到拐点，考试一般不会出）\n例题（一）：\n注：当Hesse矩阵只有2阶时，可以通过顺序主子式进行正定判断，再高就得求特征值判断\n例题（二）：\n复习：求特征值与特征向量 #\r注意：$\\textcolor{red}{对于对角阵，对角线上的元素就是其特征值}$\n6. 凸集与凸函数 #\r6.1 凸集 #\r（定义、性质不会考，但是要知道是啥）\n想象你有一个透明的容器，里面装满了水。这个容器的外形是一个没有突起或凹陷的形状，就像一个圆球或一个凸透镜一样。现在，你在这个水里选择两个点，不论它们在哪里，你都可以在容器内找到一条线段，将这两个点连在一起，这条线段的每个点都在容器内部。\n这个水装满的透明容器就代表了一个凸集。无论你选择容器内的哪两个点，连接它们的线段都完全在容器内，不会突破容器的边界。这种性质使得凸集在数学、优化和几何学中非常有用，因为它们具有良好的结构和可预测的性质。\n凸组合 #\r当我们谈论凸组合时，可以将其理解为混合不同事物的方法，但有两个关键规则：\n非负权重：每个事物都有一个权重，这个权重必须是非负的（大于等于零）。 权重总和为1：所有事物的权重加在一起必须等于1。 举个例子，假设有两种水果，苹果和橙子，你想创建一种新的混合水果，这就是凸组合。如果你用0.7的权重来表示苹果（0.7表示70%的苹果）和0.3的权重来表示橙子（0.3表示30%的橙子），那么这个混合就是一个凸组合。这意味着混合水果的总量为1，且每种水果的权重都是非负的。\n实质：系数全为正且和为一的线性组合\n人话：$\\textcolor{red}{对于凸集，如果在这个集合中任意取两点连成线段，那这线段一定还在集合内部（注意：空集也算凸集）}$（没有坑，没有洞的点集）\n凸集的性质 #\r注意，$\\textcolor{red}{\u0026lsquo;\u0026lsquo;和集\u0026rsquo;\u0026lsquo;与\u0026rsquo;\u0026lsquo;并集\u0026rsquo;\u0026lsquo;不是一个概念}$，显而易见，凸集的并集不是凸集\n超平面与半空间 #\r超平面 #\r在数学和几何学中，超平面（Hyperplane）是一个具有比空间维度低一维的平坦子空间。通俗地说，超平面是用于将空间划分为两个不相交部分的一个平面或子空间。\n具体来说，超平面在n维空间中是一个n-1维的子空间。在三维空间中，一个超平面就是一个平面；在二维空间中，一个超平面就是一条直线。超平面通常可以表示为一个线性方程的解，其一般形式可以写成：$a^Tx=b$\n其中，(a1, a2, \u0026hellip;, an) 是法向量（也称为法线），(x1, x2, \u0026hellip;, xn) 是超平面上的点，b 是常数。法向量确定了超平面的方向，而线性方程决定了它的位置。\n半空间 #\r由一个超平面（hyperplane）将空间分成两个部分而产生的一个部分空间。\n具体来说，一个n维空间中的半空间是由一个(n-1)维的超平面划分的空间的一部分。\n比如，在二维空间中，一个半空间就是由一条直线将平面分成两个部分的区域；在三维空间中，一个半空间由一个平面将空间分成两个部分。在更高维度的空间中，半空间的定义也是类似的。\n超平面和半空间都是凸集\n与此相近的还有“超球”的概念，指的是在4维空间以上，与$x_0$的距离小于半径的点集。\n（可以看出，“超XX”，一般指的是低维概念在高维空间的映射）\n极点 #\r例子：\n如图中所示，在第一个凸多边形中红圈圈起的顶点即是极点，它们可以作为线段的顶点表示凸集中其他的非极点，但是不能在凸集中任何其他点表示的线段中。\n6.2 凸函数 #\r几何意义：函数图形上连接任意两点的线段处处都在函数图形的上方\n​\n性质 #\r性质3的解释：对于凸函数，规定其小于某一值限制出的定义域还是凸集（还是连续的）（别忘了空集也是凸集）\n其实可以看出：对于凸函数的驻点，其一定是极小值点，且是全局极小值点\n凸函数的判定条件 #\r一阶要求任意性，根本用不了\n二阶条件：\n凸函数的开凸集定义域内的 Hesse 矩阵处处半正定，反之亦然（充要条件） 若 Hesse 矩阵正定，这其在定义域内为严格凸函数（充分条件）p 3之例：\n可以证明，对称矩阵 Q 就是正定二次函数的 Hesse 矩阵\n复习：二次函数化二次型 #\r对于一个多元齐次多项式，其一定可以写成$\\vec{x}^TQ\\vec{x}$的二次型形式\n以三元二次函数为例\n有$f(\\vec{x}) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{13}x_1x_3 + a_{22}x_2^2 ++ a_{23}x_2x_3 + a_{33}x_3^3$\n其中，对称矩阵Q可以写成：\n不难看出，可以将对称矩阵Q的行列位置可以与多项式中的x项一一对应，对角线上是对应平方项的系数，其他位置则是对应系数的一般，如一行二列和二行一列都是$x_1x_2$项系数的一半。\n由此可以推出n元其次多项式的二次型形式\n凸函数极值 #\r凸规划问题 #\r如果你求一个函数的极小值，这个函数正好是个定义域是凸集的凸函数，这就是凸规划（不考定义）\n凸函数极值的性质 #\r凸函数的局部极小点一定是全局极小点 凸函数的驻点一定是全局极小点 严格凸函数一定有且只有一个全局极小点 P.s.$\\textcolor{blue}{在实际问题中，当算法遇到近似驻点停止时，如果可以证明目标函数是凸函数，就可以断定找到了极值}$\n特殊的凸规划 #\r为啥凹函数大于等于零就是能限制到凸集呢？因为两边加负号就变成了 $凸函数 \\leq 0$ 就是“水平集” （参见前面凸函数的性质）\n而线性函数其实就是“超平面”，也可限制凸集，凸集的交集还是凸集。\n二次函数不考\n7. 下降迭代算法 #\r核心思想：每求一次迭代点，极值都要更小\n7.1 步骤 #\r关键步骤：\n确定下降方向 ${\\vec{p}}$\n确定步长因子 ${t_k}$\n注意，没说 $t_k$ 不能是负的\n考试的时候求驻点的方程不会太复杂\n7.2 直线搜索 #\r性质 #\r示例：由图可知，函数在直线搜索到的最优点 ${\\vec{z}}$ 上的梯度方向一定与搜索方向垂直\n收敛速度 #\r以后再说\n计算终止准则 #\r终止误差设置需要结合实际情况\n一般用这个，具体看考题要求\n第二章：线性规划 #\r1. 数学模型 #\r1.1 线性规划的标准型 #\r单纯形法需要用到标准型\n繁写形式是最常用的形式\n化标准型，**$\\textcolor{red}{第一步先把右边常数项先化成正的！}$**别忘了\n1.2 线性规划的典范形式 #\r单纯形法需要将标准型进一步化成典范形式，即使系数矩阵 A 中有 m 个列向量构成 m 阶单位矩阵。\n以m = 3 为例，形如：\n但是不是说一定是前m个变量构成单位阵，只要能找出任意m个变量能构成单位阵即可\n注意，典范形式跟变量顺序也没关系\n以上皆是典范形式\n1.3 任意模型化标准型 #\r对于极大化目标函数，加负号转成求极小\n对于约束条件，首先如果右边常数项有负数，两边加负号转成正的\n检查是否存在不等式约束，添加附加变量\n​\t当左边小于右边，左边加一个非负变量，使得等式成立\n​\t当左边大于右边，左边减去一个非负变量，使得等式成立\n数数还有没有没有限制正负性的自由变量\n​\t两个正数做差可以构造任意数，引入两个变量表示自由变量\n若变量存在常数界限限制，使用换元的方法替换成无界限变量 ​\t就是简单的移项+换元\n例\n注意\n2. 线性规划解的性质 #\r2.1 基本概念 #\r容许解 #\r基 #\r在 n 维向量空间中，n 个线性无关的向量组成的极大线性无关组称为这个向量空间的基\n典范形式中的单位阵就是标准基\n复习：矩阵的秩 #\r矩阵的秩（Rank）是一个重要的线性代数概念，它用于衡量矩阵中的线性无关列（或行）的最大数量。矩阵的秩提供了关于矩阵的结构和性质的重要信息。\n以下是关于矩阵秩的基本概念和性质：\n列秩（Column Rank）：矩阵的列秩是指矩阵中线性无关列的最大数量。它表示矩阵的列空间的维度。如果矩阵的列秩等于它的列数，那么它被称为“列满秩矩阵”。\n行秩（Row Rank）：矩阵的行秩是指矩阵中线性无关行的最大数量。行秩等于列秩，因为对于任何矩阵，它的行秩和列秩相等。\n矩阵秩：矩阵的秩是指它的列秩（或行秩）的最大值。矩阵的秩通常用符号 \u0026ldquo;rank(A)\u0026rdquo; 表示，其中 A 是矩阵。秩可以看作是矩阵所包含的最大线性无关列（或行）的数量。\n基础变量和非基础变量：在线性方程组的上下文中，秩也可以用来确定系统的基础变量（basic variables）和非基础变量（non-basic variables）。基础变量是方程组中的变量，非基础变量是通过线性组合基础变量得到的变量。\n零矩阵的秩：零矩阵的秩总是 0，因为它没有任何线性无关的列或行。\n求矩阵的秩（高斯消元法）\n步骤 1：将矩阵化为行阶梯形矩阵（Row Echelon Form）或行最简形矩阵（Row Reduced Echelon Form），也称为行最简形（Reduced Row Echelon Form）。\n从第一行开始，找到第一个非零元素（主元素），并将该元素所在的行作为当前的主元行。 使用行变换操作，将当前主元行的主元素变为 1，同时将该主元所在列的其他元素变为 0。这通常涉及到将当前主元行的所有元素除以主元素。 将下一行的第一个非零元素作为新的主元素，继续上述操作。 重复这个过程，直到所有的主元素都处理完毕，得到行阶梯形矩阵或行最简形矩阵。 步骤 2：统计非零行的数量。这个数量就是矩阵的秩。\n复习：可逆矩阵 #\r可逆矩阵（Invertible Matrix）也被称为非奇异矩阵（Non-singular Matrix）或满秩矩阵（Full Rank Matrix），是指具有逆矩阵的方阵，也就是说，它可以通过矩阵乘法找到一个矩阵，使得原始矩阵与其逆矩阵相乘等于单位矩阵。具体来说，一个 n x n 的矩阵 A 是可逆的，如果存在一个 n x n 的矩阵 B，使得以下等式成立：\nA * B = B * A = I\n其中，I 是 n x n 的单位矩阵。矩阵 B 被称为 A 的逆矩阵，通常用 A^(-1) 表示。\n可逆矩阵的性质和重要特点包括：\n存在唯一性：如果一个矩阵 A 是可逆的，那么它的逆矩阵 A^(-1) 存在且唯一。这意味着只有一个矩阵可以与 A 相乘得到单位矩阵。 可逆矩阵的行列式：一个矩阵 A 是可逆的，当且仅当它的行列式（det(A)）不等于零。如果 det(A) = 0，那么 A 不可逆。 逆矩阵的性质：如果 A 和 B 都是可逆矩阵，则它们的乘积 AB 也是可逆矩阵，且 (AB)^(-1) = B^(-1) * A^(-1)。 转置矩阵的逆：如果 A 是可逆矩阵，那么它的转置矩阵 A^T 也是可逆矩阵，并且 (A^T)^(-1) = (A^(-1))^T。 可逆矩阵的秩：一个 n x n 的矩阵 A 是可逆的，当且仅当它的秩（rank）等于 n。这意味着所有的列都是线性无关的，且行列式不为零。 可逆矩阵的逆：要找到矩阵 A 的逆矩阵 A^(-1)，可以使用各种方法，如高斯-约当消元法、伴随矩阵法等。 相关概念\n基本解（掌握） #\r基本解：令所有非基变量为 0，求出的满足约束（2）的解。\n基本容许解：满足约束（3）的基本解。\n最优基本容许解：满足约束（1）的基本容许解。\n在上例子中，我们假设系数矩阵的前m个向量正好能够构成一个基 B （在一般情况下不一定是正好前几个按顺序就组成基了），\n那么我们将剩下的 m+1 到 n 的变量全部设置成 0，只求解前 m 个变量，因为此时B是满秩的，所以此时前 m 个变量有唯一解 $B^{-1}b$，这个解加上后面那一堆 0 （m+1到n的变量）就被称为基本解。\n但是你在算$B^{-1}b$时，没法保证这个算出来是大于0的，如果真都大于0，那么上述基本解就可以被称为基本容许解。\n要是你这个基本容许解是所有基本容许解中使目标函数最小的，就称之为最优基本容许解\n容许基（一般不考） #\r容许基：若 B 是基，且存在关于 B 的基本容许解，称 B 是容许基。 若容许基 B 是单位矩阵称为标准容许基。\n退化的基本解：在上例中，如果你解 $X_B$ 里面有 0 ，这样组成的基本解称为退化的基本解（考试不会考，你算出来说明你做错了）\n2.2 线性规划问题的基本定理 #\r如果一个线性规划问题有最优解，那么直接找它容许域上的极点就一定能找到最优基本容许解\n那么，我们就有了单纯性法的基本思路——沿着边找顶点，直到找到最优解：\n3. 单纯形法 #\r3.1 步骤 #\r化典范形式，构造标准容许基 何为人工变量：\n如果化标准型后基变量不够，我人为在某个约束方程上硬加一个变量，使其满足系数为一且不在其他方程中存在的条件，让它当基变量。（但是这样相当于改变了约束条件）\n$\\textcolor{red}{注意：考试时化基变量最好不要对约束方程直接用乘除法}$（只是因为老师不好改所以不给全分？做几道看看最后答案对不对）\n在约束中引入人工变量 $x_6$ 后，还需要在目标函数中加入$x_6$，并配一个无穷大的系数 M\n此时的系数矩阵：\n注意：基变量和基B里面变量的顺序就按照实际构成基的顺序来写（比如如果$x_6$是$[1,0]^T$，那么$x_6$就要写在$x_1$前面）\n引入M的意义：\n只要人工变量\u0026gt;0，使前后约束条件不等价，但由于目标函数的修改，同时也使所求的目标函数最小值是一个很大的数，也是对“篡改”约束条件的一种惩罚，因此，M 叫做罚因子，大 M 法也叫做罚函数法。\n如果想消除 M 的影响，就需要通过换基向量使得人工变量转为非基变量，这样人工变量变为 0 ，M的影响也就消除了，问题整体的求解也不受影响。\n$\\textcolor{blue}{一般来说，考试时会有三个约束，引入两个人工变量}$\n3.2 求解 #\r得出判别数 #\r接上例，我们通过移项，用非基向量表示基向量，回代目标函数，并整理成\n$Z = Z_0 - \\sigma_{m+1}x_{m+1} - \\sigma_{m+2}x_{m+2} - \u0026hellip; - \\sigma_{n}x_{n}$ 的形式，称 $ \\sigma $ 为判别数，即写为\n$Z = 常数- \\sum判别数 \\times 非基向量$\n(注:目标函数式(*)中,各非基变量的负系数称为该变量的判别数(或检验数)，规定各基变量的判别数为 0)。\n而我们可以通过公式推导，得到上述表达式快速计算公式\n其中，$C_B$ 是基向量的价值系数，随基的改变而改变。\n简单来说，即：\n$常数项Z_0 = [基向量的在目标函数中的系数组成的向量]^T \\cdot约束的常数组成的向量b$\n$非基变量项j的判别数 \\sigma_j=Z_j-C_j = [基向量的在目标函数中的系数组成的向量]^T \\cdot当前非基向量x_j在约束中的系数组成的向量 - 当前非基变量x_j在目标函数的系数C_3$\n例子：\n可以看到，和通过移项的方法获得的常数和判别数一样。算的时候一定注意向量顺序。\n得到容许解 #\r令非基向量皆为零，即可得到基向量的取值，组成一个基本容许解，其对应的目标函数的值就是当前基本容许解的值\n3.3 最优性检验 #\r退出迭代的条件：所有判别数都 $\\leq$ 0\n若人工变量为零，则有解，非基变量判别数都小于0，则为最优解，否则有无穷多解\n若人工变量为不为零，则无解\n3.4 基变换 #\r基本容许解的改进定理 #\r找一个判别数大于零的非基变量，替代原基中的某一个基变量，产生一个新的基本容许解，其值要比原来的要小。\n在几何意义上：就是沿着边换一个顶点\n换入变量的确定 #\r若有多个判别数大于零的非基变量，选判别数最大的那个（其实也不一定下降的更快，选哪个无所谓）\n接上例，可以看到 $\\sigma_3 = 2M-4$ 是唯一正数，选择$x_3$ 作为换入变量\n换出变量的确定——最小非负比值规则 #\r接上例，在选择 $x_3$ 作为换入变量后，我们把约束的系数矩阵 A 拿过来，找到 $x_3$ 对应系数的那一行依次比较常数项与其系数的比值，选择最小的那一行，而那一行对应哪个基变量，就把哪个基变量换出，这里我们换出 $x_1$\n$\\textcolor{red}{注意换出哪个坑，换入变量放到哪个坑里！别调顺序！}$\n（你换出变量是第一个基变量，换入的变量也就是新基的第一个基变量）\n3.5 单纯性法的矩阵描述 #\r以上例题为例，有：\n经过一波运算，有：\n3.6 单纯形解极大化和极小化问题的区别 #\r4. 单纯形法的表格形式 #\r5. 单纯形的两阶段法 #\r两阶段法的思路：当人工变量离基后，消除人工变量的影响，简化计算\n5.1 第一阶段——判断原线性规划问题是否有容许解 #\r即只保留目标函数中的人工变量，约束条件不变，用单纯形法求解\n第一阶段最优解判别定理 #\r对于某个基本容许解，所有判别数 $\\sigma_j \\leq 0$，则该基本容许解是第一阶段线性规划的最优解。\n此时\n5.2 第二阶段——求原线性规划问题的最优解 #\r以第一阶段的最终单纯形表为基础，去掉其中的人工变量列，把目标函数换成原问题的目标函数，于是得到第二阶段的初始单纯形表，继续迭代下去，得到最优解即为原问题的最优解。\n第二阶段最优解判别定理 #\r例题 #\r接上例 第三章 无约束最优化方法 #\r目标函数没有约束\n方法——迭代法：\n直接法（只用到目标函数的值，迭代速度慢，计算量较大）：步长加速法（考点）， 方向加速法，单纯形替换法\n解析法（借助目标函数的解析性质[即导数]，速度较快，但是要求导数）:最速下降法， 牛顿法，共轭梯度法， 拟牛顿法\n搜索方式：直线搜索 $$ min \\phi(t) = f(\\vec{x}_k+t\\vec{p}_k) $$ 但是你对非线性函数直接求驻点可能求不出来，采用特殊的直线搜索方法\n直线搜索方法：黄金分割法（重点，但是也没咋考过），抛物线插值法（计算量太大不适合考），平分法（），牛顿法\n1. 直线搜索的直接方法 #\r1.1 单谷函数 #\r由此可见，单谷函数有唯一极小值点\n总结：谁大谁可以当端点（ ）\n那么，搜索区间 [a,b] 如何确定？\n外推内插法 #\r核心思想：找到三个值，使得 $ 左边 \u0026gt; 中间 \u0026lt; 右边 $\n但是一个h一个h找的太慢了，使用步长加速法——每一次的跨度都比上次大（每次步长倍增）\n例：\n1.2 黄金分割法 #\r分割原则：短比长等于长比总长\n为啥要用这个比例？：因为黄金分割点有两个（正着一个倒着一个），每次迭代都要求这俩点，而用黄金分割点可以让计算效率最大化\n由图可知，对于第一次迭代中 $ [a_1，b_1]$ 的黄金分割点 $t_1,t_1\u0026rsquo;$ ，如果 $t_1$ 成为下次迭代的新端点，$t_1\u0026rsquo;$ 在新的搜索区间里面依然是黄金分割点。$t_1\u0026rsquo;$当新端点时同理。\n==新端点的选取：按照单谷函数的性质，谁大谁当端点==\n步骤 #\r初始化 #\r迭代 #\r比较 $\\phi(t_k)与\\phi(t_k\u0026rsquo;) $ ，谁的函数值大，谁当端点，另外一个作为下一次迭代的新黄金分割点（或对称点）。\n计算下一次迭代的黄金分割点（你留了一个，算另外一个）：\n或\n终止 #\r例题 #\r中间自己补\n1.3 抛物线插值法（以前从来没考过） #\r同样只要求函数连续即可，同样是，先找到三个点，使得其值 $ 左边 \u0026gt; 中间 \u0026lt; 右边 $\n然后，==找到一个过这三点的抛物线==，作为对目标函数的一个拟合\n然后我们可以求出这个抛物线的极值点，根据极值点在x轴上的坐标 $t_4$ ，我们可以求出这个坐标下在目标函数上的值 $ \\phi(t_4) $ ，然后在已知的这四个点中取其函数值最小的点，加上它最近的左边和右边的点，构成新的三个点，有其值 $ 左边 \u0026gt; 中间 \u0026lt; 右边 $\n终止准则\n例题\n2. 最速下降法（也基本没考过） #\r2.1 基本思想 #\r对于 $t_k$ 的求解，可以使用上面讲到的直线搜索方法（但是算起来太麻烦了，下面有快速计算法）\n2.2 算法步骤 #\r其中，$ \\vec{g}() $ 代表求梯度的操作，$||\\vec{g_k}||$ 代表求梯度的范数（模）\n2.3 最佳步长因子 $t_k$ 的近似公式 #\r==而对于二次函数，其二阶泰勒展开已经是精确值，故上述公式求出的即是其 $t_k$ 的精确值==\n2.4 例题 #\r注意：\n在实际应用中，方向向量可能很大或很小，可能需要单位化 每次搜索前后两次搜索方向和梯度是正交的，故有锯齿现象 2.5 锯齿现象（考画图） #\r最速下降法的迭代点在向极小点靠近的过程中，走的是曲折的路 线：后一次搜索方向 $\\vec p_{k+1}$ 与前一次搜索方向 $\\vec p_k$ 总是相互垂直的，称它为锯齿现象。除极特殊的目标函数（如等值面为球面的函数）和极特殊的初始点外，这种现象一般都要发生。\n所以你会发现：对于“非正圆形”的目标函数，最速下降法是“越靠近收敛越慢”的（最慢下降法实锤了）\n3. 牛顿法（考点） #\r3.1 基本思想 #\r对于二次函数，我们不想只按照梯度逐次迭代，希望一次迭代就能达到最优解，那么就需要引入二阶梯度。\n当我们将方向向量定于以下形式时，即可一次迭代达到最优点。\n于是，我们借助泰勒展开，将上述方法推广到一般非线性函数，即有：\n那么，我们有牛顿法的迭代公式：\n3.2 步骤 #\r3.3 主要结论 #\r判断下降方向就得用这个法\n3.4 例题 #\r复习：矩阵逆的求法 #\r初等变换法 待定系数法 行列式 + 伴随矩阵 分块对角.反三角.上三角的逆 4. 修正牛顿法 #\r4.1 传统牛顿法的问题 #\r步长因子 $t$ 固定为1，你按照下降方向走可能得到的点更大了 你算出的下降方向 $\\vec p$ 可能并不下降（跟梯度同向或正交），甚至你有可能压根解不出 $\\vec p$ 4.2 对牛顿法的修正 #\r下降方向点更大：在下降方向上改做直线搜索 算不出 $\\vec p$ 或 $\\vec p$ 与梯度正交：本次迭代直接用负梯度做最速下降搜索 算出来 $\\vec p$ 是上升方向：对 $\\vec p$ 加负号改反向进行搜索 正常情况继续算就行 4.3 例题 #\r5. 共轭方向法和共轭梯度法 #\r5.1. 共轭方向法 #\r（1）共轭 #\r推广：如果在一个向量组中任意两个向量两两 Q 共轭，那么称这个向量组中Q共轭\n==**特殊：**当 $Q= I$ （单位矩阵），Q 共轭即为正交==\n（2）共轭的性质 #\r定理1：Q 共轭的向量组线性无关\n推论：（其实都是线性无关的性质 ）\n在n维向量中，n个线性无关的向量构成一个极大线性无关组（再多肯定有线性相关的了），也能构成n维向量空间的一组基\n推论3没啥用\n（3）共轭方向法的特点 #\r（4）步骤 #\r在下降迭代法的步骤的基础上，要求每次下降方向都得与之前的的搜索方向 Q 共轭\n迭代思想：\n（5）二次终止性 #\r我们知道 n 维向量 Q 共轭的数量最多有 n 个，也就是说共轭方向法最多能迭代 n 次，而二次函数在这 n 次之内一定能达到极小值点\n(1)第k+1点的梯度和之前的搜索方向都正交\n（6）共轭方向形成 #\r初始方向选择梯度负方向，在直线搜索之后，按照红框公式计算下一步方向，可以满足 i) 、ii) 、iii)、 iv) 四条性质\ni) 的证明可以通过判断 $\\vec{p}_k$ 与 $\\vec{g}_k$ 的内积是否为正来判断\niv) 的证明可由 iii)的结论 + 上图框式实现。\n5.2 共轭梯度法（送分题必会） #\r（1）系数 $\\alpha_k$ 的其他形式 #\r因为正定阵Q只在二次函数中出现，对于其他函数，我们借助式子：\n得到\n（2）二次函数的最佳步长因子 #\r同样由a式，得到二次函数的最佳步长因子\n==最速下降法、共轭梯度法、逆牛顿法，都能用这个公式==\n$\\textcolor{red}{非二次函数，求导求驻点}$\n（3）步骤 #\r首先，初始方向使用负梯度方向\nLoop:\n直线搜索，找到$x_{k+1}$ 终止判断，如果没有达到终止条件，且迭代次数小于 n 次，继续迭代。大于等于 n 次，计数归零，重新使用当前点负梯度方向进行共轭迭代 计算下一次迭代共轭方向，计算$p_{k+1}$ 特殊情况判断：共轭方向因为误差累积不是下降方向了：计数归零，重新迭代（考试时不会考）\n5.3 例题 #\r==注意：考试的时候，这块计算中间步骤需要保留分数！！不然会损失精度==\n解题思路：\n初始方向是逆梯度方向，作个垂线 二元二次函数，顶多迭代两次，直接连到最优点，结束 6. 拟牛顿法（变尺度法） #\r6.1 基本思想 #\r牛顿法需要求二阶导的Hesse矩阵$G(\\vec{x}_k)$，太麻烦了，我们换个简单的矩阵进行替换\n6.2 DFP算法（必考） #\r（1）校正矩阵 #\r修正矩阵有多种取法，以下是比较简单的一种取法：\n考试时这个公式会给\n（2）步骤 #\rDFP法考试时一般就迭代一次（肯定是算$H_k$的才算一次），计算量太大了\nDFP法也是一种共轭方向法，所以也存在二次终止性（所以步骤其实和共轭梯度法一样，但是搜索方向的计算方式不同）\n（4）性质 #\r==DFP法也是一种共轭方向法，所以也存在二次终止性==\n6.3 例题 #\r+\n7. 步长加速法（考点，应该能拿分） #\r又称为模式法或模矢法，是只用到函数值的直接方法\n7.1. 基本思想 #\r7.2 步骤 #\r注意：\n正常考试只考二维，就是先探测 X 轴正、负方向有个横轴方向，再探测 Y 轴正、负方向有个纵轴方向，探测4次，找到比原来点优的点，采用该方向。\n一般来说，考试时不会出现探测失败的情况（缩减步长计算量太大）\n步长加速：按照 $b_{k+1}-b_k$ 跨度会越来越长\n注意看11 的获取，因为采取步长加速的策略，11 得到的方向不是 8-6 的方向，是 8-5 的方向。\n直接搜索：判断成功与否只与值是否比搜索前的小有关\n7.3. 例题 #\r上图标号不一定对，考试时只标基点顺序即可\n==注意画图的时候，睁大狗眼看看到底哪个点离中心的等值线更近，实在看不出来拿尺子量！==\n8. 最小二乘法 #\r最小二乘问题是对变量参数的拟合，我们将已知的现有样本点自变量的值带入估计的函数，希望得到能够使得估计函数的函数值更接近实际因变量值的参数。即：\n其中，t即是自变量，x即待定参数，具体可见下【线性最小二乘模型】例子。\n8.1. 线性最小二乘模型（考点） #\r什么是线性最小二乘模型？\n直接看例题：\n由例子可以看出：\n线性最小二乘的定义可以看成\n（1）线性最小二乘模型的解 #\r注意，上述证明会用到第一章提到的梯度计算的性质，考试也是'有可能'考的\n故，有线性最小二乘模型的解：\n==注意：这里的最小二乘解不一定是原线性方程组的解，将解回带到最小二乘模型（**）中，如果得到模型的值为0，那么说明是原线性方程组的解，但是如果值大于0，原线性方程组无解==\n（2）例题 #\r·\n8.2 非线性最小二乘模型（了解） #\r第四章 约束最优化方法 #\r==$\\textcolor{red}{记得先化标准型！！！}$== #\r1. 最优性条件 #\r1.1 等式约束问题的最优性条件 #\r（1）等式约束问题 #\r（2）最优性条件 #\r其实就是通过 $\\lambda $ 把约束式加入到方程组中去\n上面是 (*) 式，下面就是原来那些约束直接写上去\n要是半正定，就是必要条件\n其中$\\nabla h(\\vec{x}^)$ 表示约束方程组的梯度带入 $\\vec{x}^$ 得到的矩阵，也就是说，切子空间是对与每个约束方程，其在点 $\\vec{x}^$ 处的切空间（三维方程就是切平面嘛）的交集（三维就是切平面的交集）。在这上面的向量如果满足$\\vec{v}^T\\nabla^2_{xx}L(\\vec{x},\\vec{\\lambda})\\vec{v}\u0026gt;0$ ,则说明点 $\\vec{x}^$ 是严格局部极小值点。\n1.2 只含不等式约束问题的最优性条件 #\r（1）不等式约束问题 #\r（2）几何最优性条件 #\r【1】容许方向 #\r很简单，到边上的才算约束住了嘛\n没啥用，下一个\n由上图可知，容许域内的点任何方向都是容许方向，边界上的点有一部分非容许方向\n所以你能看出：==梯度方向总是指向容许集内部方向==，即：\n【2】下降方向 #\r若方向 $\\vec{p}$ 既是点 $\\bar{x}$ 的容许方向，又是下降方向，则称它 是 $\\bar{x}$ 的容许下降方向\n【3】几何最优性条件 #\r【4】例题 #\r（4）库恩-塔克条件（Kuhn-Tucker条件/K-T条件 考点） #\r由此可以看出，K-T条件是一阶必要条件。\n$\\textcolor{blue}{在做题的时候不用判断线性无关}$\n红线紫线是约束，黑线是等值线\n结论1：单个约束上的最优点，就没有容许下降方向（容许就没法下降，下降就没法容许）\n结论2：两个约束上的最优点：其上目标函数梯度方向一定在两个约束梯度方向夹角之内（都容许就没法下降，下降就没法都容许）\n例题 #\r1.3 一般约束问题的最优性条件 #\r就是把前面两部分的理论合一起\n例题 #\r1.4 凸规划问题的最优性条件 #\r==在一般约束条件的问题求最优解问题中，在得出K-T点后，一般需要判断原问题是为凸规划问题，==即：\n目标函数是凸函数 不等式约束是凹函数 等式约束是线性函数 满足以上三点，原问题是凸规划问题，则得出的K-T点即全局极小点，即最优点。\n例题 #\r2. Zoutendijk 容许方向法（Z-容许方向法） #\r一种选择容许下降方向的方法，就有一种容许方向法，但都是 选方向——直线搜索——迭代 的套路\n这里只讲线性约束的情形\n2.1. 下降容许方向的确定 #\r（1） 容许方向的确定 #\r在线性约束这边，大于等于零即可作为充分条件\n（2） 容许下降方向的确定 #\r以上规定是为了限制方向向量的长度，都无穷大就没法比了\ni) ii) iii) iv)四种限制选一种对方向向量进行限制，这里使用 $\\vec{e}$ 对其进行限制\n要是按照这个方法找不到合法的方向向量，那说明目前来到了K-T点，进入迭代终止判断\n2.2 直线搜索 #\r按照容许下降方向的性质，起作用约束对于上面的要求恒成立，所以：\n这里是把\r移项，化为 $\\vec{u}+t\\vec{v}\\geq0$ 的形式，那么，按照 $\\vec{v} $的正负性 ，对 t 就有上述约束。\n2.3. 迭代终止准则 #\r无约束：驻点 、有约束：K-T点\n2.4. 算法 #\r这里，上一撇的是起作用约束，上两撇指的是不起作用约束\n2.5 例题 #\r==易错点：二维图解画图，务必把方向看明白咯==\n3. 外部罚函数法 #\r罚函数法是解决约束优化问题的一般方法。它的基本思想是将约束问题转化为一系列无约束极值问题处理，因此又称为序列无约束极小化技术(简记为 SUMT)。罚函数法可分为两类：外部罚函数法(又称外点法)与内部罚函数法(又称内点法或围墙函数法)。外部罚函数法 适用于一般的约束优化问题，内部罚函数法只适用于不等式约束优化问题。\n3.1 问题描述 #\r3.2 罚函数的形式与性质 #\r3.3 收敛性 #\r增广目标函数是越迭代越大的，罚函数是越迭代越小的\n但是一定在原来目标函数的最优值之内\n对于$\\vec{x}_k是F(\\vec{x},\\mu_k)$的极小点，$\\vec{x}_k$ 在$\\bar x$ 固定时是关于 $\\mu_k$ 的函数，当 $\\mu_k$ 趋向无穷大时，$\\vec{x}_k$ 趋向$\\bar x$，即（NP) 的极小点\n迭代算法的理论依据，每迭代一次，看看是不是容许点，第一个找到的就是原问题的极小点\n3.4 步骤 #\r3.5 优缺点 #\r3.6 例题 #\r4. 乘子法 #\r乘子法是针对外部罚函数法的一种改进方法。Hesternes 和 Powell 于 1969 年各自独立地提出了乘子法，即在约束极值问题的 Lagrange 函数中加入相应的惩罚，使得在求解系列无约束极值问题时，罚因子 不必趋于无穷大，就能求到约束极值问题的最优解，保证了数值计算的稳定性。\n区别：\n外部罚函数法：只在目标函数上加惩罚\n乘子法：在拉格朗日函数上也加惩罚项\n4.1. 等式约束情形 #\r（1）Hesternes 乘子法（H乘子法） #\r性质 #\r二阶充分条件：在切子空间正定\nH乘子法（背） #\r多了第（4）步，使用 $\\theta$ 人为规定收敛速度，在收敛速度满意时，不增大罚因子（限制罚因子不会无限增大）\n考试时候还是取极限\nP乘子法（考试不考看看就行） #\r4.2 只含不等式约束情形 #\r1. 乘子迭代公式\n考试一般只考一个不等式的形式\n4.3 一般约束问题 #\rH乘子法 #\r4.4 例题 #\r考点统计 #\r公式部分 #\r理论部分 #\r1. 建模问题 #\r物理问题不考\n2.画等值线 #\r3.等值面的性质 #\r4. 梯度 #\r梯度公式一定会背\n5. 泰勒展开 #\r6.凸集 #\r7. 凸函数和凸规划 #\r8.极小点判断 #\r9.迭代方法 #\r10.容许集的性质 #\r11.基本解和基本容许解 #\r极点就是基本容许解\n12.单纯形法的基本思想 #\r13. 两阶段单纯形法 #\r14.锯齿现象 #\r15.对于正定二次函数，牛顿法只需迭代一次 #\r16.二次终止性 #\r17.最优步长因子 #\r对于所有直线搜索，你 t 90% 应该是正的\n非二次函数，求导求驻点\n18. F-R共轭梯度法-画图 #\r初始方向是负梯度方向\n记得二次终止性\n19 DFP #\r20 步长加速的基本思想 #\r21 最优性条件 #\r22 起作用约束 #\r不等式约束关于容许集的任意内点（例如图4—2中的点P）都是不起作用约束.因此，只有容许集的边界点才能使某个或某些不等式约束变为起作用约束.\n按照定义，任一等式约束关于任意容许点都是起作用约束.\n23 容许方向向量 #\r24 梯度方向 #\r25 K-T 条件的梯度方向 #\r26 KT条件 #\r27 Z容许下降方向的判定 #\r28 Z容许方向法终止准则 #\r29 外部罚函数法的基本思想 #\r30 乘子法基本思想 #\r31 $\\lambda$ 计算 #\r试卷统计 #\r写在最后 #\r本资料为个人整理，难免存在错漏，欢迎联系我进行改正。资料仅供学习参考，禁止用作商业用途。\n愿选到这课的人都不挂科🙏。\n","date":"2024年9月4日","externalUrl":null,"permalink":"/posts/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/","section":"文章","summary":"结合杨冬梅老师的教案梳理了知识点和一些题目，表白杨老师，讲得太好。","title":"最优化方法与理论-课程笔记","type":"posts"},{"content":"\r专题一 概率基础 （第一题） #\r1. 1 理论 #\r标准手写希腊字母：\n在本文中，定义：\n1.1.1 统计量 #\r1.1.2 四大分布与抽样分布定理 #\r==在考试时进行区间估计和假设检验时，使用的是上侧分位点！（但课本上的表大多是下侧的，记得换算）==\r1.1.3 正态运算性质 #\r标准正态下侧分位点运算公式： $$ \\Phi(-x ) = 1–\\Phi(x) $$\n1.1.4 典型题目 #\r独立性判断和查表算数 #\r例题 #\r2018 一 #\r1.2. 题目汇总 #\r1.2.1. 独立性问题 #\r2019 一 #\r2. 查表凑数题 #\r2012 一 #\r2016 一 #\r2017 一 #\r3.其他 #\r2015 一（求样本容量） #\r2020 一（核方法求条件分布） #\r专题二 贝叶斯估计（第二题） #\r2.1 理论 #\r2.1.1 贝叶斯估计基本方法 #\r2.1.2 核方法 #\r==什么是“核”：一个分布密度函数去掉与主要变量无关的系数就是“核”==\r2.1.3 三种损失函数下的贝叶斯估计 #\r2.1.4 典型题目 #\r（1）离散先验下点估计 #\r2020 二 #\r（2）连续先验下点估计 #\r2015 二 #\r2012 二 #\r课本P101（习题二）32题改 #\r2.2. 题目汇总 #\r2.2.1 离散先验下点估计 #\r见2.1.4\n2.2.2 连续先验下点估计 #\r2016 二 #\r2017 二 / 2023 二（重难点） #\r2017\n2018 二 #\r2019 二 难点（加权损失配凑分布计算） #\r2021 六 #\r2.2.3. 连续先验下区间估计 #\r课本79页\n专题三 点估计（第三题） #\r3.1 理论 #\r3.1.1. 矩估计 #\r==注意：矩估计一般使用原点矩（A）,且低阶矩优先于高阶矩。==\r均方误差MSE #\r对于估计值为 $\\hat\\theta$ 的参数估计，其均方误差为： $$ MSE = E(\\hat{\\theta} - \\theta)^2 $$ ==不难看出，对于无偏估计，其估计的均方误差就是估计量的方差==\r顺序统计量的联合分布 #\r常用计算技巧 #\r3.1.2. 极大似然估计 #\r3.2. 题目汇总 #\r2012 三 #\r2015 三 #\r2017 三 #\r2018 三 #\r2019 三 #\r2019 四 #\r2020 三 #\r2021 七 #\r专题四 区间估计（第四题） #\r4.1. 单一总体区间估计 #\r4.1.1 典型题目 #\r2012 四 #\r2015 四 #\r4.1.2 题目汇总 #\r2016 三 #\r2020 四 #\r2021 一 #\r2. 混合总体区间估计 #\r2018 四 #\r==对于多正态总体的区间估计，依然可以按照之前的计算方式，得出估计类型后，直接带入模板进行区间估计==\r2019 五 #\r专题五 参数假设检验（第五题） #\r5.1. 单一总体假设检验 #\r5.1.1 典型题目 #\r2017 五 #\r5.2. 混合总体假设检验 #\r5.2.1 典型题目 #\r2012 五 #\r2015 五 #\r5.2.2 题目汇总 #\r2016 四 #\r2020 五 #\r2021 八 #\r5.3. 两类错误问题 #\r2023 #\r==由上题可知，功效函数是固定的，就是在计算第一类错误和第二类错误时的定义域不一样。（第一类错误取原假设的参数值/端点，第二类错误计算取备择假设的参数值/端点）==\r专题六 非参数假设检验（第六题） #\r6.1. 皮尔逊卡方检验（拟合优度） #\r组合数计算 #\r2015 六 #\r6.2. 卡方独立性检验（列联表） #\r6.2.1 典型题目 #\r2017 六 #\r6.2.2 题目汇总 #\r2016 五 #\r2018 六 #\r6.3. 秩和检验 #\r6.3.1 典型题目 #\r样本数量小于10 #\r2019 六 #\r==当两组数据数量不同时，选少的那个算秩和==\r样本数量大于10 #\r2012 六 #\r算个结论吧，n\u0026gt;10的时候 $R_1$ 服从正态分布， 参数如上所示\n6.3.2 题目汇总 #\r==注意：课本上的秩和检验表有个4没印刷出来==\r2021 二 #\r专题七 单因素方差分析（第七题） #\r7.1 典型题目 #\r7.1.1. 不给出样本均值方差 #\r2016 七 #\r7.1.2. 给出样本均值方差 #\r2019 七 #\r7.2 题目汇总 #\r7.2.1. 不给出样本均值方差 #\r2015 七 #\r7.2.2. 给出样本均值方差 #\r2017 七 #\r2018 七 #\r专题八 最小二乘估计（第八题） #\r8.1 基础理论 #\r8.2. 由样本数据进行最小二乘估计 #\r8.2.1 典型题目 #\r2018 八 #\r2019 八 #\r8.3. 给出样本分布进行估计 #\r8.3.1 典型题目 #\r2012 八 #\r8.3.2 题目汇总 #\r2015 八 #\r2017 八 #\r8.4.由最小二乘求分布问题 #\r8.4.1 典型题目 #\r2016 八 #\r2020 八 #\r8.4.2 题目汇总 #\rP238第17题（习题五17题） #\r2021 #\r写在最后 #\r本资料为个人整理，难免存在错漏，欢迎联系我进行改正。资料仅供学习参考，禁止用作商业用途。\n愿选到这课的人都不挂科🙏。\n","date":"2024年8月28日","externalUrl":null,"permalink":"/posts/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E9%A2%98%E5%9E%8B%E7%BB%9F%E8%AE%A1/","section":"文章","summary":"统计学是一门伟大的学科，科学的统计揭示真理，更科学的统计掩盖事实。","title":"应用数理统计-题型整理","type":"posts"},{"content":"\r报错示例 #\rFetching package metadata ... CondaHTTPError: HTTP 000 CONNECTION FAILED for url \u0026lt;https://repo.continuum.io/pkgs/main/linux-64/repodata.json.bz2\u0026gt; Elapsed: - An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. SSLError(MaxRetryError(\u0026#39;HTTPSConnectionPool(host=\\\u0026#39;repo.anaconda.com\\\u0026#39;, port=443): Max retries exceeded with url: /pkgs/main/linux-64/repodata.json.bz2 (Caused by SSLError(SSLError(\u0026#34;bad handshake: Error([(\\\u0026#39;SSL routines\\\u0026#39;, \\\u0026#39;ssl3_get_server_certificate\\\u0026#39;, \\\u0026#39;certificate verify failed\\\u0026#39;)],)\u0026#34;,),))\u0026#39;,),) 总之就是网络连接问题，可以按照下面步骤逐步排查\n1. 网络问题 #\r首先确认你的电脑或服务器可以连网：\ncurl -I https://www.baidu.com 下面是一个网络联通的返回示例，否则就要检查你的网络连接\nHTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: no-cache Connection: keep-alive Content-Length: 227 Content-Security-Policy: frame-ancestors \u0026#39;self\u0026#39; https://chat.baidu.com http://mirror-chat.baidu.com https://fj-chat.baidu.com https://hba-chat.baidu.com https://hbe-chat.baidu.com https://njjs-chat.baidu.com https://nj-chat.baidu.com https://hna-chat.baidu.com https://hnb-chat.baidu.com http://debug.baidu-int.com; Content-Type: text/html Date: Thu, 20 Jun 2024 07:48:56 GMT Pragma: no-cache Server: BWS/1.1 Set-Cookie: BD_NOT_HTTPS=1; path=/; Max-Age=300 Set-Cookie: PSTM=1718869736; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com Set-Cookie: BAIDUID=DFDB30F27E4DAB2EEA37ED70BE16DD86:FG=1; Path=/; Domain=baidu.com; Max-Age=31536000 Set-Cookie: BAIDUID_BFESS=DFDB30F27E4DAB2EEA37ED70BE16DD86:FG=1; Path=/; Domain=baidu.com; Max-Age=31536000; Secure; SameSite=None Traceid: 1718869736282151399411655385938091250512 X-Ua-Compatible: IE=Edge,chrome=1 X-Xss-Protection: 1;mode=block 2. 数据源问题 #\r执行以下命令可以查看当前conda的数据源。\nconda config --get channels 如果只有默认数据源，可能出现下载速度慢或无法连接问题，可以添加清华源：\nconda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ conda config --set show_channel_urls yes 注意这里清华镜像源推荐使用http，而不是https，否则还有可能会报HTTP 000 CONNECTION FAILED的错误。\n如果你已经添加了https的数据源，可以通过以下两种方式补救（任选其一即可）：\n删除现有数据源并重新添加：\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ ... 如果在添加http源后还是无法连接，尝试关闭SSL验证。\n关闭SSL验证：\n在命令行中输入\nconda config --set ssl_verify false 修改设置，或者在文件~/.condarc末尾添加一行ssl_verify: false（有则修改即可）\n参考 #\rConda创建环境失败：CondaHTTPError: HTTP 000 CONNECTION FAILED-CSDN博客\n清华源连接失败原因与解决 CondaHTTPError SSLError_连接镜像源时发生了 ssl 错误-CSDN博客\n","date":"2024年6月20日","externalUrl":null,"permalink":"/posts/condahttperror%E8%A7%A3%E5%86%B3/","section":"文章","summary":"实验室服务器配环境时最常出现的问题，如果你conda虚拟环境配不上显示CondaHTTPError，可以参考这篇文章","title":"CondaHTTPError: HTTP 000 CONNECTION FAILED for url的问题解决","type":"posts"},{"content":"","date":"2024年6月20日","externalUrl":null,"permalink":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","section":"Tags","summary":"","title":"环境配置","type":"tags"},{"content":"\r摘要 #\r近年来，随着大语言模型（如GPT）的发展，人工智能在自然语言处理领域取得了显著进展。然而，这些模型在实际应用中面临幻觉问题、知识注入困难和应用落地困难等挑战。为了克服这些问题，检索增强生成技术（Retrieval-Augmented Generation, RAG）应运而生。RAG通过结合外部数据和生成模型，提高了回答的准确性和实时性。本文综述了RAG技术的发展，包括其基本流程框架、评估方法以及在不同应用场景中的表现，并展望了其未来发展方向和潜在改进。\nAbstract #\rIn recent years, with the development of large language models (such as GPT), artificial intelligence has made significant advances in the field of natural language processing. However, these models face challenges in practical applications, such as hallucinations, difficulty in knowledge integration, and challenges in implementation. To address these issues, Retrieval-Augmented Generation (RAG) technology has emerged. RAG enhances the accuracy and timeliness of responses by combining external data with generative models. This paper reviews the development of RAG technology, including its basic workflow framework, evaluation methods, and performance in various application scenarios. It also discusses future development directions and potential improvements.\n一、引言 #\r​\t近年来，随着GPT[1]等语言模型的出现，人工智能在自然语言处理领域取得了显著进展。这些模型展示了卓越的语言理解和生成能力，能够在多项人类评估基准上超越人类表现。然而，这些先进的语言模型在实际应用中仍然面临诸多挑战，需要进一步研究和改进。幻觉问题、知识注入困难和应用落地困难就是其中十分典型的难题。\n​\t1.幻觉问题：大语言模型常常会产生“幻觉”问题，即生成的回答看似合理但实际上完全不准确。这种问题不仅影响模型的可信度，还可能导致严重的误导信息传播。因此，如何让模型在面对未知问题时避免生成错误信息，成为当前研究的重点。\n​\t2.知识注入困难：传统的通过训练来注入知识的方法，在构建专业性强、时效性高的大模型时面临诸多挑战。例如，长尾知识难以有效整合；对于需要频繁更新的数据，反复训练会消耗大量计算资源；而对于保密或隐私数据，由于数据泄露的风险，这些数据通常无法用于模型训练。这样一来，模型在这些领域的表现可能会大打折扣。\n​\t3.应用落地困难：对于持有独家数据且需要定制大模型的用户，通过训练或微调方式定制大模型所需的数据量和计算资源可能难以承受。\n​ 图1 ChatGPT结合RAG技术解决问题的一个示例\n​\t为了克服这些挑战，使大语言模型在更多、更专业的领域中得到有效利用，检索增强生成技术（Retrieval-Augmented Generation, RAG）应运而生。RAG技术通过检索外部数据来补充模型知识，以响应用户查询，确保生成的内容更加准确和实时。通过将事实性知识与大语言模型的训练参数分离，RAG技术巧妙地结合了生成模型的强大功能和检索模块的灵活性，提供了一种有效解决方案，以应对纯参数化模型中知识不完整和不充分的问题。图1是ChatGPT结合RAG技术解决问题的一个示例，可以看出RAG补充了大语言模型自身所没有的知识。\r​\t自2020年由Lewis等人提出以来[2]，RAG技术已经取得了显著进展。本文旨在对检索增强生成技术进行简单综述。首先，介绍RAG技术的基本流程框架，包括数据预处理、检索、检索后处理和生成四个关键环节。接着，详细讨论RAG技术的评估方法，分析其在不同应用场景中的表现。最后，展望未来RAG技术的发展方向，探讨可能的改进和新兴应用领域。\n二、RAG 流程框架 #\r​\t检索增强生成系统通过结合外部实时事实信息和检索模型来补充大语言模型的知识库。RAG系统主要由两个核心模块组成：检索器和生成器。检索器负责从预构建的数据存储中搜索相关信息，而生成器则根据检索结果生成内容。对于基于查询的RAG系统，其核心在于高效的搜索机制，这是生成高质量结果的关键。\n​\t从整体流程来看，RAG的处理流程可以分为四个主要阶段：数据预处理、检索、检索后处理和生成。这一流程允许将不同领域的专业知识通过特定的检索方法融入到不同的语言模型中，使RAG系统既灵活又具有扩展性，能够适应各种应用需求。图2展示了RAG技术的基本处理流程和增强方法。\n​\t在实际应用中，RAG技术确实遵循这一基本框架。例如，LangChain[3]、LlamaIndex[4]和Piperag[5]等平台已经将RAG方法模块化处理。尽管这些平台在具体实现上存在差异，但它们都遵循RAG的基本工作流程。\n图2 RAG技术的基本处理流程和增强方法\n2.1 数据预处理 #\r​\t高质量的检索是RAG系统能够输出准确回答的关键，而检索预处理则是确保检索有效性的基础。检索预处理通常包括三个主要方面：数据处理、索引构建和查询处理。\n2.1.1 数据处理 #\r​\t数据处理是在检索之前对数据进行改进，以提高检索的准确性和效率。此过程包括删除不相关信息、消除歧义、更新过时文档、增加附加信息以及合成新数据等。Xia等提出的LESS[6]算法通过分析梯度信息，策略性地选择最佳数据集，提升了模型微调性能。UPRISE[7]通过自动检索预构建提示池中的提示，优化零样本任务输入的数据处理，提升了生成模型性能。GENREAD[8]采用基于聚类的提示方法，通过将问题转化为文档并对它们进行聚类以排除不相关数据，丰富输入的上下文见解。\n2.1.2 索引构建 #\r​\t索引构建是保证高效检索的重要手段。合适的索引策略能够快速准确地检索到最相关的信息。Bevilacqua等的SEAL[9]系统利用压缩的全文子串索引和自回归语言模型，提高了索引构建的效率和检索准确性。Chen等[10]提出的DSI框架利用可微函数生成文档标识符排序列表，显著提升了文本检索的有效性，同时保持了存储效率和端到端检索能力。MEMWALKER系统[11]则通过记忆树结构，打破了大语言模型的上下文窗口限制，提升了索引构建和管理效率。\n2.1.3 查询处理 #\r​\t查询处理通过修改输入查询，使用户查询更好地匹配索引数据，以增强检索结果的相关性和准确性。Yu等[12]提出了一种基于少样本的生成方法来处理对话查询重写，使语言模型能够有效捕捉任务语法并学会上下文依赖关系。Query2doc[13]和HyDE[14]首先使用查询生成伪文档，然后将该文档作为检索的关键，这样伪文档包含更丰富的相关信息，有助于提高检索结果的准确性。Kim等提出的澄清树（ToC）框架[15]，通过递归构建歧义消解树并生成长形式答案，有效处理了开放域问答中的歧义性问题。KnowledGPT[16] 和 Rewrite-Retrieve-Read [17]通过“思维程序”提示和创新的查询重写技术，引入了查询处理方法。KnowledGPT通过生成代码与知识库交互，将用户查询转换为结构化的搜索命令，而Rewrite-Retrieve-Read使用可训练的紧凑语言模型对查询进行重构，使其更有效地反映用户意图和上下文。\n2.2 检索 #\r​\t在RAG系统中，检索过程至关重要。检索内容质量的高低直接影响到大语言模型的上下文学习能力以及生成器的表现。常见的增强RAG系统检索能力的方法有检索器微调和递归检索。\n2.2.1 检索器微调 #\r​\t一个高质量的嵌入模型能够使语义上相似的内容在向量空间中更加接近，从而提升检索效果。通过使用高质量的领域数据或任务相关数据对检索器进行微调，可以显著提升其在特定领域或任务中的表现。例如，REPLUG[18]通过将检索文档预先添加到冻结的黑盒语言模型输入中，利用调优的检索模型显著提升了GPT-3[1]在语言建模上的性能。APICoder[19]使用Python文件和API名称、签名、描述对检索器进行微调，以提高其在编程相关任务中的性能。EDITSUM[20]通过微调检索器以减少检索后摘要之间的Jaccard距离，从而提升摘要的多样性和准确性。\n2.2.2 递归检索 #\r​\t一种重要的检索策略是递归检索。递归检索通过在检索前将查询拆分，执行多次搜索，以检索更多且质量更高的内容。ReACT[21]采用了思维链[23]（COT）方法，，通过结合推理路径和任务特定行动，提高了模型在语言理解和决策任务中的表现，并有效解决了传统推理方法中幻觉和错误传播的问题。ActiveRAG[22]，利用主动学习机制，将知识构建与认知调节结合，从而优化COT过程并有效辅助生成答案。RATP[24]采用蒙特卡洛树搜索技术进行多阶段决策，每个节点代表一个思路。系统根据这些思路的评分模型反馈指导动作选择，在节点扩展中，通过提示将检索文档和思路合并到语言模型中以生成新的思路。\n2.3 检索后处理 #\r​\tRAG系统的检索后处理环节是将检索到的外部文档进行优化、提取和整合，以确保生成答案的准确性、相关性和可信度。这一环节不仅扩展了模型的知识范围，还使其能够实时更新和处理复杂、具体的问题。通过有效的检索后处理，RAG系统可以更好地利用外部知识库，从而提升整体性能。检索后处理的两种主要方法是重排序和过滤。\n2.3.1 重排序 #\r​\t重排序是指对检索到的内容进行重新排列，以实现更高的多样性和更优的结果。有效的重排序能够显著减少文本向量化过程中信息丢失对检索质量的影响。例如，Re2G[25]在传统检索器之后应用了一个重排序模型，通过引入知识蒸馏变体，整合BM25和重排结果，实现了多任务场景下的性能提升。AceCoder[26]使用选择器来重排序检索到的程序，以减少冗余并获取多样化的程序。XRICL[27]在检索后使用基于蒸馏的范例重排序器，通过优化文档排序来提升检索效果。\n​\t此外，Rangan等人[28]通过量化影响度量（QIM）作为“AI法官”机制，结合用户反馈实现了高效的重排序优化，确保更相关的文档被优先选中。UDAPDR[29]通过生成大量合成查询来微调重排序模型，从而在长尾领域显著提高了零样本准确率，并减少了检索延迟。LLM-R[30]则通过迭代训练检索器，利用冻结语言模型的反馈对候选文档进行排名，并训练奖励模型，每次迭代都建立在前一次训练的基础上，不断优化检索器的性能。Hofstätter等人则介绍了FiD-Light[31]，这一模型在保持高效检索的同时，通过重排序机制优化了信息流动。\n2.3.2 过滤 #\r​\t过滤的目的是剔除未达到特定质量或相关性标准的文档，从而提高生成答案的准确性和效率。有效的过滤机制能够显著减少生成器的负担，使其更专注于高质量的信息。例如，FILCO[32]通过词汇和信息论方法识别有用上下文，并训练上下文过滤模型在测试时进行过滤，显著提高了生成器在提取式问答和对话生成等任务中的上下文质量。COK[33] 提出了渐进式理由校正技术，旨在通过检索到的知识迭代精炼理由。这种方法构成了一个持续的优化过程，显著提升了内容生成中使用的信息的相关性和质量。Self-RAG[34] 引入了一种自我反思机制来高效过滤无关内容。通过使用批评标记，这种方法评估检索到的段落的相关性、支持性和效用，确保只将高质量信息融入内容生成过程。\n2.4 生成 #\r​\t在RAG系统中，生成模块的质量直接影响最终输出结果的质量。因此，生成能力决定了整个RAG系统有效性的上限。提升生成质量的一个有效方法是利用提示工程。提示工程通过优化输入提示，能够引导大语言模型（LLM）生成更准确和相关的输出。\n​\t例如，LLM-Lingua[35]应用一个小型模型来压缩查询的总长度，在保持语义完整性的同时，提高了提示压缩效率，从而加速了大语言模型的推理过程。Toufique等人[36]的研究发现，通过在提示中添加语义事实（如输入代码、函数定义、分析结果及相关评论），可以显著提升代码生成任务的性能。CEDAR[37]设计了提示模板，将代码演示、查询和自然语言说明整合到一个提示中，以提高生成的连贯性和准确性。REPLUG[18]在最终预测前将检索到的文档预先加入输入上下文。该方法采用了一种集群策略，以并行方式处理检索到的文档，突破了语言模型上下文长度的限制，并通过增加计算资源来提升准确性。XRICL[27]针对跨语言的文本到SQL语义解析，通过检索相关的英语示例构建提示，从而在零样本迁移学习的设置下，实现了跨语言的高效语义解析。RECITE[38] 采用了一种自洽技巧，即独立生成多次回答，并通过多数投票系统来选出最合适的答案。这种方法旨在提高答案的可靠性和准确性，从而提升输出的质量和可信度。\n三、RAG 评估 #\r​\t为了探索语言模型借助外部知识生成更准确、相关且稳健的回答的有效性，RAG系统的评估已经成为一个重要的研究方向。目前，大多数研究集中在利用常见指标，如精确匹配（EM）和F1分数，来评估RAG模型在各种下游任务中的表现。此外，多种数据集，如TriviaQA[39]、HotpotQA[40]、FEVER[41]、Natural Questions[42]、Wizard of Wikipedia[43]和T-REX[44]，已被广泛用于对RAG模型进行更全面的评估。\n​\t在关键针对RAG模型的评估上，Chen等人提出了一个RAG基准[45]，从噪声鲁棒性、负面拒绝、信息整合和反事实鲁棒性四个方面进行了评估。噪声鲁棒性考察了语言模型是否能够从包含嘈杂信息的文档中提取必要的信息，这些嘈杂信息虽然与输入查询相关，但对回答查询无用。负面拒绝则衡量了在检索到的内容不足以回答查询时，模型是否能够拒绝回答。信息整合评估了模型是否能够通过整合多个检索到的内容来获取知识并生成回应。反事实鲁棒性则指语言模型辨别检索到的内容中反事实错误的能力。\n​\tRAGAS[46]和ARES[47]系统则从忠实度和答案相关性两个角度对RAG结果进行评估。忠实度关注结果中的事实是否正确，特别是当正确答案可以从检索到的内容中推断出来时。答案相关性评估生成的回答是否真正解决了问题（即查询）。此外，上下文相关性评估了检索到的内容是否包含尽可能多的与回答查询相关的知识，同时尽可能少地包含无关信息。Lyu等人针对中文环境提出了CRUD-RAG[48]，该基准通过创建、读取、更新和删除四个方面全面评估了RAG系统的各个组件和应用场景。\n四、未来发展方向 #\r4.1 RAG目前待解决的问题 #\r​\t在检索增强生成（Retrieval-Augmented Generation, RAG）模型的发展中，尽管其在整合检索与生成方面展现了巨大的潜力，但仍存在若干未解的挑战需要进一步探讨与解决。\n4.1.1 多轮问答中的检索问题 #\r​\t在多轮对话场景下，共指消解是一个显著的问题，尤其是在需要对多轮对话中的语境进行理解和延续的情况下。现有的RAG模型在处理这些情况时，往往表现出明显的不足。在多轮对话中，用户的查询可能会涉及到之前的对话内容，而这些内容的上下文关系并不是一个静态的文本，它需要动态更新和维护。这种动态语境的复杂性给检索带来了极大的挑战，因为检索系统需要能够准确理解当前的查询与之前对话的关系，并有效地将相关信息提取出来。\n​\t目前，常见的解决方案如查询重写（Query Rewrite）虽然在某些情况下能够部分缓解问题，但并未从根本上解决这一难题。查询重写主要依赖于对话上下文的浅层解析，无法有效处理复杂的共指消解和语义连接。因此，未来的研究需要探索更加智能和动态的检索机制，以提升在多轮对话中对上下文的理解和信息检索的准确性。\n4.1.2 多查询与比较 #\r​\t在实际应用中，经常会遇到一个查询指向多个候选文档的情况。例如，在电商场景下，用户可能会询问“商品A和商品B哪个更好？”这种情况下，RAG模型需要能够同时处理多个查询，进行文档之间的比较和评估，以提供用户需要的答案。这一过程不仅需要检索出相关的候选文档，还需要对这些文档进行综合分析和对比，进而生成合理的、富有参考价值的答案。\n​\t目前，现有的RAG模型在多查询处理上，通常采用独立检索然后进行拼接的方式，然而这种方式往往难以有效应对复杂的比较场景。未来的研究可以着重于如何改进模型的多任务处理能力，以更好地实现多查询场景下的高效和准确的信息检索与生成。\n4.1.3 范围查询的处理 #\r​\t对于范围查询，如“找到价格比商品A贵一百以内的商品”，是另一个需要解决的问题。这种查询通常需要在检索时进行更复杂的条件匹配，并且涉及到更高层次的逻辑推理。现有的检索系统和RAG模型在处理这种查询时，往往局限于简单的文本匹配和过滤，难以充分理解用户的需求和意图。\n​\t未来，如何提高模型在范围查询场景下的理解和处理能力，将是一个重要的研究方向。具体来说，模型需要能够更好地理解查询中的条件约束，并在海量数据中迅速找到符合条件的信息。这可能需要结合更先进的数据处理和分析技术，以及更加智能的检索算法，以实现更高效的范围查询。\n4.2 多模态RAG #\r​\t多模态检索增强生成（Multimodal Retrieval-Augmented Generation, Multimodal RAG）模型是近年来在自然语言处理和计算机视觉领域逐渐兴起的一个重要方向。相比传统的RAG模型，多模态RAG通过整合不同模态的数据（如文本、图像、音频等），进一步提升了信息检索和生成的能力，展现出广阔的应用前景和研究价值。\n​\t目前已经有一些多模态RAG的工作被推出。Chen等人提出了MuRAG[49]，这是首个多模态检索增强转换器，通过大规模的图像-文本和仅文本语料库的预训练，显著提升了模型在图像和文本交互推理上的性能。在MuRAG之后，如REVEAL[50] 和 Re-Imagen[51] 的研究专注于提升视觉问答和文本到图像生成。它们分别通过引入动态检索机制和提高图像保真度来实现这一目标。这些进展为图像字幕生成和文本到音频生成方面的进一步模型奠定了基础，扩大了RAG在不同模态中的应用范围，并提高了生成输出的质量和现实感。此外，Yang等人提出Re-ViLM[52]，一个基于Flamingo[53]构建的检索增强视觉语言模型，进行零样本和少样本图像到文本生成，在域外设置中显著提高了性能，并通过简化参数和动态更新数据库来适应新数据 。\n​\t尽管多模态RAG在应用中展现了巨大的潜力并且也开始有了一些相关研究，但它仍面临许多挑战：\n跨模态对齐问题：如何将不同模态的数据进行有效对齐和融合，是多模态RAG面临的首要问题。不同模态数据具有不同的特征和分布，如何在统一的向量空间中表示它们，并确保它们的语义一致性，是一个关键挑战。\n多模态数据的复杂性：多模态数据通常具有更高的复杂性和异构性，这增加了数据处理和模型训练的难度。特别是在处理高维度图像或视频数据时，如何保持计算效率和模型性能是一大难题。\n数据标注和训练资源：多模态模型的训练通常需要大量的标注数据和计算资源。尤其是跨模态的标注数据非常稀缺和昂贵，如何在有限的数据资源下训练出有效的多模态模型，是一个亟待解决的问题。\n模型解释性和鲁棒性：多模态模型往往更加复杂，如何解释模型的行为和决策过程，以及确保模型在不同模态和场景下的鲁棒性，也是一个重要的研究方向。\n信息融合和生成质量：如何有效地融合不同模态的信息，并在生成过程中保持高质量的内容，是多模态RAG模型的一大挑战。特别是当不同模态的信息存在冲突或不一致时，模型需要能够做出合理的选择和调整。\n五、结论 #\r​\t检索增强生成问答（RAG）技术通过将检索和生成有机结合，显著提升了大语言模型在实际应用中的表现。本文在前面的章节中详细介绍了RAG技术的基本流程框架、数据预处理、检索、检索后处理和生成等关键环节及研究现状，并探讨了其在不同应用场景下的评估方法。展示了RAG技术在应对大语言模型所面临的知识局限性和时效性挑战方面，其独特的优势和广阔的应用前景。最后，我们探讨了RAG技术面临的一些挑战和未来发展的方向，介绍了多模态RAG的一些研究工作。虽然本文进行了相对深入的调查，但由于该领域的快速发展和篇幅限制，某些方面可能未被充分分析和探索，或未涵盖最新的发展情况。综上所述，RAG技术通过将检索与生成有机结合，提供了一种有效的解决方案来应对大语言模型在实际应用中的挑战。随着技术的不断发展和优化，RAG有望在更多领域中发挥更大的作用，进一步推动自然语言处理技术的进步。未来，随着检索技术和生成模型的不断演进，RAG技术将继续在知识获取、内容生成和智能问答等方面取得更多突破，助力人工智能技术的全面发展。\n参考文献 #\r[1] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.\n[2] Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks[J]. Advances in Neural Information Processing Systems, 2020, 33: 9459-9474.\n[3] H. Chase, \u0026ldquo;Langchain\u0026rdquo;, https://github.com/langchain-ai/langchain,2022.\n[4] J. Liu, \u0026ldquo;LlamaIndex\u0026rdquo;, 11 2022. [Online]. Available: https://github.com/jerryjliu/llama_index\n[5] Jiang W, Zhang S, Han B, et al. Piperag: Fast retrieval-augmented generation via algorithm-system co-design[J]. arXiv preprint arXiv:2403.05676, 2024.\n[6] Xia M, Malladi S, Gururangan S, et al. Less: Selecting influential data for targeted instruction tuning[J]. arXiv preprint arXiv:2402.04333, 2024.\n[7] Cheng D, Huang S, Bi J, et al. Uprise: Universal prompt retrieval for improving zero-shot evaluation[J]. arXiv preprint arXiv:2303.08518, 2023.\n[8] Yu W, Iter D, Wang S, et al. Generate rather than retrieve: Large language models are strong context generators[J]. arXiv preprint arXiv:2209.10063, 2022.\n[9] Bevilacqua M, Ottaviano G, Lewis P, et al. Autoregressive search engines: Generating substrings as document identifiers[J]. Advances in Neural Information Processing Systems, 2022, 35: 31668-31683.\n[10] Chen X, Liu Y, He B, et al. Understanding differential search index for text retrieval[J]. arXiv preprint arXiv:2305.02073, 2023.\n[11] Chen H, Pasunuru R, Weston J, et al. Walking down the memory maze: Beyond context limit through interactive reading[J]. arXiv preprint arXiv:2310.05029, 2023.\n[12] Yu S, Liu J, Yang J, et al. Few-shot generative conversational query rewriting[C]//Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020: 1933-1936.\n[13] Wang L, Yang N, Wei F. Query2doc: Query expansion with large language models[J]. arXiv preprint arXiv:2303.07678, 2023.\n[14] Gao L, Ma X, Lin J, et al. Precise zero-shot dense retrieval without relevance labels[J]. arXiv preprint arXiv:2212.10496, 2022.\n[15] Kim G, Kim S, Jeon B, et al. Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models[J]. arXiv preprint arXiv:2310.14696, 2023.\n[16] Wang X, Yang Q, Qiu Y, et al. Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases[J]. arXiv preprint arXiv:2308.11761, 2023.\n[17] Ma X, Gong Y, He P, et al. Query rewriting for retrieval-augmented large language models[J]. arXiv preprint arXiv:2305.14283, 2023.\n[18] Weijia S, Sewon M, Michihiro Y, et al. REPLUG: Retrieval-augmented black-box language models[J]. ArXiv: 2301.12652, 2023.\n[19] Zan D, Chen B, Lin Z, et al. When language model meets private library[J]. arXiv preprint arXiv:2210.17236, 2022.\n[20] Li J A, Li Y, Li G, et al. Editsum: A retrieve-and-edit framework for source code summarization[C]//2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021: 155-166.\n[21] Yao S, Zhao J, Yu D, et al. React: Synergizing reasoning and acting in language models[J]. arXiv preprint arXiv:2210.03629, 2022.\n[22] Xu Z, Liu Z, Liu Y, et al. ActiveRAG: Revealing the Treasures of Knowledge via Active Learning[J]. arXiv preprint arXiv:2402.13547, 2024.\n[23] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in neural information processing systems, 2022, 35: 24824-24837.\n[24] Pouplin T, Sun H, Holt S, et al. Retrieval-Augmented Thought Process as Sequential Decision Making[J]. arXiv preprint arXiv:2402.07812, 2024.\n[25] Glass M, Rossiello G, Chowdhury M F M, et al. Re2G: Retrieve, rerank, generate[J]. arXiv preprint arXiv:2207.06300, 2022.\n[26] Li J, Zhao Y, Li Y, et al. Acecoder: Utilizing existing code to enhance code generation[J]. arXiv preprint arXiv:2303.17780, 2023.\n[27] Shi P, Zhang R, Bai H, et al. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing[J]. arXiv preprint arXiv:2210.13693, 2022.\n[28] Rangan K, Yin Y. A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge[J]. arXiv preprint arXiv:2402.17081, 2024.\n[29] Saad-Falcon J, Khattab O, Santhanam K, et al. UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers[J]. arXiv preprint arXiv:2303.00807, 2023.\n[30] Wang L, Yang N, Wei F. Learning to retrieve in-context examples for large language models[J]. arXiv preprint arXiv:2307.07164, 2023.\n[31] Hofstätter S, Chen J, Raman K, et al. Fid-light: Efficient and effective retrieval-augmented text generation[C]//Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2023: 1437-1447.\n[32] Wang Z, Araki J, Jiang Z, et al. Learning to filter context for retrieval-augmented generation[J]. arXiv preprint arXiv:2311.08377, 2023.\n[33] Li X, Zhao R, Chia Y K, et al. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources[C]//The Twelfth International Conference on Learning Representations. 2023.\n[34] Asai A, Wu Z, Wang Y, et al. Self-rag: Learning to retrieve, generate, and critique through self-reflection[J]. arXiv preprint arXiv:2310.11511, 2023.\n[35] Jiang H, Wu Q, Lin C Y, et al. Llmlingua: Compressing prompts for accelerated inference of large language models[J]. arXiv preprint arXiv:2310.05736, 2023.\n[36] Ahmed T, Pai K S, Devanbu P, et al. Automatic semantic augmentation of language model prompts (for code summarization)[C]//Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 2024: 1-13.\n[37] Nashid N, Sintaha M, Mesbah A. Retrieval-based prompt selection for code-related few-shot learning[C]//2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023: 2450-2462.\n[38] Sun Z, Wang X, Tay Y, et al. Recitation-augmented language models[J]. arXiv preprint arXiv:2210.01296, 2022.\n[39] Joshi M, Choi E, Weld D S, et al. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension[J]. arXiv preprint arXiv:1705.03551, 2017.\n[40] Yang Z, Qi P, Zhang S, et al. HotpotQA: A dataset for diverse, explainable multi-hop question answering[J]. arXiv preprint arXiv:1809.09600, 2018.\n[41] Thorne J, Vlachos A, Christodoulopoulos C, et al. FEVER: a large-scale dataset for fact extraction and VERification[J]. arXiv preprint arXiv:1803.05355, 2018.\n[42] Kwiatkowski T, Palomaki J, Redfield O, et al. Natural questions: a benchmark for question answering research[J]. Transactions of the Association for Computational Linguistics, 2019, 7: 453-466.\n[43] Dinan E, Roller S, Shuster K, et al. Wizard of wikipedia: Knowledge-powered conversational agents[J]. arXiv preprint arXiv:1811.01241, 2018.\n[44] Elsahar H, Vougiouklis P, Remaci A, et al. T-rex: A large scale alignment of natural language with knowledge base triples[C]//Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). 2018.\n[45] Chen J, Lin H, Han X, et al. Benchmarking large language models in retrieval-augmented generation. arXiv[J]. arXiv preprint arXiv:2309.01431, 2023.\n[46] Chen J, Lin H, Han X, et al. Benchmarking large language models in retrieval-augmented generation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(16): 17754-17762.\n[47] Saad-Falcon J, Khattab O, Potts C, et al. Ares: An automated evaluation framework for retrieval-augmented generation systems[J]. arXiv preprint arXiv:2311.09476, 2023.\n[48] Lyu Y, Li Z, Niu S, et al. CRUD-RAG: A comprehensive chinese benchmark for retrieval-augmented generation of large language models[J]. arXiv preprint arXiv:2401.17043, 2024.\n[49] Chen W, Hu H, Chen X, et al. Murag: Multimodal retrieval-augmented generator for open question answering over images and text[J]. arXiv preprint arXiv:2210.02928, 2022.\n[50] Hu Z, Iscen A, Sun C, et al. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 23369-23379.\n[51] Chen W, Hu H, Saharia C, et al. Re-imagen: Retrieval-augmented text-to-image generator[J]. arXiv preprint arXiv:2209.14491, 2022.\n[52] Yang Z, Ping W, Liu Z, et al. Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning[J]. arXiv preprint arXiv:2302.04858, 2023.\n[53] Alayrac J B, Donahue J, Luc P, et al. Flamingo: a visual language model for few-shot learning[J]. Advances in neural information processing systems, 2022, 35: 23716-23736.\n[54] Huang Y, Huang J. A Survey on Retrieval-Augmented Text Generation for Large Language Models[J]. arXiv preprint arXiv:2404.10981, 2024.\n[55] Zhao P, Zhang H, Yu Q, et al. Retrieval-Augmented Generation for AI-Generated Content: A Survey[J]. arXiv preprint arXiv:2402.19473, 2024.\n[56] Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for large language models: A survey[J]. arXiv preprint arXiv:2312.10997, 2023.\n","date":"2024年6月13日","externalUrl":null,"permalink":"/posts/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/","section":"文章","summary":"RAG通过结合外部数据和生成模型，提高了回答的准确性和实时性。本文综述了RAG技术的发展，包括其基本流程框架、评估方法以及在不同应用场景中的表现，并展望了其未来发展方向和潜在改进。","title":"检索增强生成技术综述","type":"posts"},{"content":"\r换行符替换导致的 JS/CSS 无法使用或文档渲染出错问题 #\r在使用git部署到Github仓库时，git bash 给我返回了这么个警告：\nwarning: in the working copy of \u0026#39;posts/补档eoj4329/index.html\u0026#39;, LF will be repla ced by CRLF the next time Git touches it 我没注意，结果部署上去后，公式渲染出大问题……（忘截图了，反正就是单行公式的渲染及其离谱，一行渲染一行原代码那种，根号飞到天上去了）\n究其原因，是Git 在不同的操作系统上使用时，可能会遇到文件换行符格式不一致的问题。\n为了保持跨平台的一致性，Git 有一个配置项叫 core.autocrlf，用于自动转换换行符。\ncore.autocrlf 设为 true 时，Git 会在检出文件时将 LF 转换为 CRLF，在提交时将 CRLF 转换为 LF。 core.autocrlf 设为 input 时，Git 只在提交时将 CRLF 转换为 LF，但在检出时不会做任何转换。 core.autocrlf 设为 false 时，Git 不会进行任何换行符转换。 而LF (\\n) 是 Unix 和 Unix-like 系统（如 Linux 和 macOS）使用的换行符，CRLF (\\r\\n) 是 Windows 系统使用的换行符。如果没有特殊设置，core.autocrlf一般是true。那么在你的Windows 系统中，你文件中的LF就会被“贴心的”换成CRLF，于是你本地渲染的好好的一上传就变了样了。\n解决方法：\n删除github上现有的.github.io的仓库，重新创建仓库。\n本地目录下，删除public目录下的.git目录。\n重新执行\ngit init -b main 之后，关键的来了，多执行一步\rgit config core.autocrlf false 保证git在上传时不会替换换行的格式。\n之后正常部署即可\ngit remote add origin git@github.com:JiBinquan/JiBinquan.github.io.git git pull --rebase origin main git add . git commit -m \u0026#34;FST\u0026#34; git push origin main ","date":"2024年6月6日","externalUrl":null,"permalink":"/posts/windows%E7%B3%BB%E7%BB%9F%E4%B8%8Bgithub_pages%E9%83%A8%E7%BD%B2%E4%B8%8Ehugo%E9%A1%B5%E9%9D%A2js%E4%B8%8Ecss%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E6%88%96%E6%96%87%E6%A1%A3%E6%B8%B2%E6%9F%93%E5%87%BA%E9%94%99%E9%97%AE%E9%A2%98/","section":"文章","summary":"这个自以为是博客部署时遇到的最大的一个坑…默认设置公式排版直接爆炸","title":"Windows系统下Github_Pages部署Hugo页面JS与CSS无法使用或文档渲染出错问题","type":"posts"},{"content":"在2022年后，没有SSH-Key无法通过SSH直接上传文件到仓库，故需要在本地生成SSH-Key上传到github。一般来说，一台设备只需要一个SSH-Key。下面是对Github官方文档的整合。\n1. 生成SSH-Key #\r步骤参见：\r生成新的 SSH 密钥并将其添加到 ssh-agent - GitHub 文档\n打开Git Bash。\n粘贴以下文本，将示例中使用的电子邮件替换为 GitHub 电子邮件地址。\nssh-keygen -t ed25519 -C \u0026#34;h******@163.com\u0026#34; 注意：如果你使用的是不支持 Ed25519 算法的旧系统，请使用以下命令：\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; 这将以提供的电子邮件地址为标签创建新 SSH 密钥。\n\u0026gt; Generating public/private ALGORITHM key pair. 当系统提示您“Enter a file in which to save the key（输入要保存密钥的文件）”时，可以按 Enter 键接受默认文件位置。 请注意，如果以前创建了 SSH 密钥，则 ssh-keygen 可能会要求重写另一个密钥，在这种情况下，我们建议创建自定义命名的 SSH 密钥。 为此，请键入默认文件位置，并将 id_ALGORITHM 替换为自定义密钥名称。\n\u0026gt; Enter file in which to save the key (/c/Users/YOU/.ssh/id_ALGORITHM):[Press enter] 在提示符下，键入安全密码。 有关详细信息，请参阅“\r使用 SSH 密钥密码”。\n\u0026gt; Enter passphrase (empty for no passphrase): [Type a passphrase] \u0026gt; Enter same passphrase again: [Type passphrase again] 示例\n$ ssh-keygen -t ed25519 -C \u0026#34;h*****@163.com\u0026#34; Generating public/private ed25519 key pair. Enter file in which to save the key (/c/Users/***/.ssh/id_ed25519): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /c/Users/***/.ssh/id_ed25519 Your public key has been saved in /c/Users/***/.ssh/id_ed25519.pub The key fingerprint is: SHA256:fN*******/c********6/b*******************Zo h*****@163.com The key\u0026#39;s randomart image is: +--[ED25519 256]--+ | * | | *****| | **** | | **********| | **********| | *********| | *******| | ******| | *****| +----[SHA256]-----+ 2. 添加 SSH Key 到 ssh-agent #\r在向 ssh 代理添加新的 SSH 密钥以管理您的密钥之前，您应该检查现有 SSH 密钥并生成新的 SSH 密钥。一般一个电脑对应一个密钥就行了。\n如果已安装 GitHub Desktop，可使用它克隆存储库，而无需处理 SSH 密钥。\n在以管理员身份运行 PowerShell 窗口中，确保 ssh-agent 正在运行。 可以使用“\r使用 SSH 密钥密码”中的“自动启动 ssh agent”说明，或者手动启动它：\n# start the ssh-agent in the background Get-Service -Name ssh-agent | Set-Service -StartupType Manual Start-Service ssh-agent 在无提升权限的终端窗口中，将 SSH 私钥添加到 ssh-agent。 如果使用其他名称创建了密钥，或要添加具有其他名称的现有密钥，请将命令中的 id_ed25519 替换为私钥文件的名称。\nssh-add /c/Users/***/.ssh/id_ed25519 P.S.如果你的路径中有中文而输入的时候一直显示乱码并提示No such file or directory，那就使用cd命令进入那个目录再运行\nssh-add id_ed25519 输入刚才设置的密码即可。\n3. 将SSH-Key上传到Github #\r参考：\r新增 SSH 密钥到 GitHub 帐户 - GitHub 文档\n可以添加 SSH 密钥并将其用于身份验证或提交签名，或同时用于这两个目的。 如果要使用相同的 SSH 密钥进行身份验证和签名，则需要将其上传两次。\n将 SSH 公钥复制到剪贴板。\n如果您的 SSH 公钥文件与示例代码不同，请修改文件名以匹配您当前的设置。 在复制密钥时，请勿添加任何新行或空格。\n$ clip \u0026lt; ~/.ssh/id_ed25519.pub # Copies the contents of the id_ed25519.pub file to your clipboard 注意：\n对于适用于 Linux 的 Windows 子系统 (WSL)，可以使用 clip.exe。 如果 clip 不起作用，你可以找到隐藏的 .ssh 文件夹，在你最喜欢的文本编辑器中打开该文件，并将其复制到剪贴板。\n在使用 Windows 终端的较新版本 Windows 中，或者在使用 PowerShell 命令行的任何其他位置，可能会收到一条 ParseError，表示 The '\u0026lt;' operator is reserved for future use.。在这种情况下，应使用以下替代 clip 命令：\n$ cat ~/.ssh/id_ed25519.pub | clip # Copies the contents of the id_ed25519.pub file to your clipboard 在任何页面的右上角，单击个人资料照片，然后单击“设置”。\n在边栏的“访问”部分中，单击 “SSH 和 GPG 密钥”。\n单击“新建 SSH 密钥”或“添加 SSH 密钥” 。\n在 \u0026ldquo;Title\u0026rdquo;（标题）字段中，为新密钥添加描述性标签。 例如，如果使用的是个人笔记本电脑，则可以将此密钥称为“个人笔记本电脑”。\n选择密钥类型（身份验证或签名）。一般Key Type选择 Authentication Key即可， 有关提交签名的详细信息，请参阅“\r关于提交签名验证”。\n在“密钥”字段中，粘贴公钥。\n单击“添加 SSH 密钥”。\n如果出现提示，请确认你的帐户是否拥有 GitHub 访问权限。 有关详细信息，请参阅“\rSudo 模式”。\nssh-keyscan github.com \u0026gt;\u0026gt; ~/.ssh/known_hosts ","date":"2024年6月5日","externalUrl":null,"permalink":"/posts/%E4%B8%BAgithub%E8%AE%BE%E7%BD%AEssh-key/","section":"文章","summary":"在2022年后，没有SSH-Key无法通过SSH直接上传文件到仓库，故需要在本地生成SSH-Key上传到github。一般来说，一台设备只需要一个SSH-Key。本文是对Github官方文档的整合","title":"为GitHub设置SSH Key","type":"posts"},{"content":"\r博客搭建记录 #\r〇、 环境安装 #\r0. 注册github和安装Git #\rGit是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。Git的作用是将本地的网页文件传到github上。\nGit\r下载地址 Git\r教程 windows： 到git官网上下载.exe文件,Download git,安装选项全部默认即可。\ngithub 给学生提供免费的代码托管服务，并且提供\rGitHub Pages 让你的网站直接托管在 GitHub 的服务器上，通过 http://xxx.github.io 来访问。更新维护网站内容也可以通过 git commit 和 git push 进行更新维护，因为 GitHub Pages 就是 GitHub 上的一个 Git 代码仓库。\nGitHub注册这里不再赘述，如果网址无法访问，你可能需要科学上网。\n1. 安装Hugo #\r下载Hugo\n​\t打开 Github 中的 Hugo 库，打开右侧的 Realeases，下载最新的版本。推荐下载extended版本，因为有些主题的需要利用进行 SCSS/SASS 构建，如果下普通版就可能会报错显示： you need the extended version to build SCSS/SASS\n安装Hugo\n​\t将下载好的压缩包解压到你喜欢的目录下，记住这个目录。然后将该目录添加到系统环境变量的 Path 下。 安装验证：\n在 cmd 或 git bash 中输入 hugo version ，如果输出版本信息，则说明安装成功。\n$ hugo version hugo v0.126.2-8f3d902ce51512931f5759e9279d93e346c9e649+extended windows/amd64 BuildDate=2024-05-30T15:19:22Z VendorInfo=gohugoio 一、新建Hugo网站 #\r找个目录，用于存放网站文件，在目录下命令行执行\nhugo new site blog 即可完成网站新建。\n二、更换主题 #\r对于一般主题更换，将对应主题的代码 拉取/下载 到博客目录下的 theme 目录即可。\n之后运行\nhugo server -t 主题名 --build 我本次使用的是 Blowfish 主题，其还提供了一种傻瓜式的安装方式，即首先安装\nnpm i -g blowfish-tools 然后在博客路径下执行\nblowfish-tools 即可进行交互式的主题安装与设置\n三、页面配置 #\rCLI 工具傻瓜是傻瓜，但是一个个配置挺慢的，我还是用文件配置吧…\n3.1 基本信息配置 #\r个人信息配置 #\r因为blowfish 默认的是英文信息，首先在博客目录下的 config\\_default 目录下，复制languages.en.toml 和 menus.en.toml ，并重命名为languages.zh-cn.toml 和 menus.zh-cn.toml\nlanguages.zh-cn.toml可以这么写\nlanguageCode = \u0026#34;zh-cn\u0026#34; # 网站使用的语言代码 languageName = \u0026#34;Simplified Chinese (China)\u0026#34; # 网站使用的语言名称 weight = 1 # 当前语言页面的排序权重 title = \u0026#34;JiBinquan\u0026#39;s blog\u0026#34; # 网页标题 # 参数配置 [params] displayName = \u0026#34;中文\u0026#34; # 显示的语言名称 isoCode = \u0026#34;zh-cn\u0026#34; # 国际标准语言代码 rtl = false # 是否是从右到左的语言 dateFormat = \u0026#34;2006年1月2日\u0026#34; # 日期格式 logo = \u0026#34;\u0026#34; # 网站标志图片路径 # secondaryLogo = \u0026#34;img/secondary-logo.png\u0026#34; # 第二标志图片路径 description = \u0026#34;JiBinquan的博客\u0026#34; # 网站的描述 copyright = \u0026#34;©️ { year } Ji Binquan\u0026#34; # 版权信息，包含动态的年份占位符 # 作者信息 [author] name = \u0026#34;Ji Binquan\u0026#34; # 作者姓名 image = \u0026#34;/img/avatar.png\u0026#34; # 作者头像图片路径 headline = \u0026#34;随便写写\u0026#34; # 作者的口号或座右铭 bio = \u0026#34;现在也没研究明白的研究僧\u0026#34; # 作者的简短传记 # 作者的社交媒体链接列表，还可以添加更多 links = [ { github = \u0026#34;\u0026#34; }, # GitHub 链接 { email = \u0026#34;\u0026#34; }, # 电子邮件链接 { qq = \u0026#34;\u0026#34; }, # QQ 链接 { zhihu = \u0026#34;\u0026#34; }, # 知乎链接 ] 头像放在assets\\img 路径下（若没有建立目录即可），如果你的links没有默认的图标，可以把同名图标放置在assets\\icons路径下\n目录配置 #\rmenus.zh-cn.toml是目录配置，可以根据你的需要自己配置目录，可以参考如下编写。\n[[main]] name = \u0026#34;文章\u0026#34; pageRef = \u0026#34;posts\u0026#34; weight = 10 [[main]] name = \u0026#34;研究\u0026#34; parent = \u0026#34;文章\u0026#34; pageRef = \u0026#34;tags/研究/\u0026#34; weight = 21 [[main]] name = \u0026#34;技术\u0026#34; parent = \u0026#34;文章\u0026#34; pageRef = \u0026#34;tags/技术/\u0026#34; weight = 22 [[main]] name = \u0026#34;友链\u0026#34; pageRef = \u0026#34;friends\u0026#34; weight = 30 [[footer]] name = \u0026#34;关于\u0026#34; pageRef = \u0026#34;about\u0026#34; weight = 10 [[footer]] name = \u0026#34;Tags\u0026#34; pageRef = \u0026#34;tags\u0026#34; weight = 30 BaseURL配置 #\r如果你要部署到GithubPages上，请在 config\\_default\\hugo.toml 中设置为你自己的.github.io域名，或者你自己绑定的其他域名，如下例所示，否则一些页面跳转功能和搜索功能将失效。\n# -- Site Configuration -- # Refer to the theme docs for more details about each of these parameters. # https://blowfish.page/docs/getting-started/ theme = \u0026#34;blowfish\u0026#34; # UNCOMMENT THIS LINE baseURL = \u0026#34;https://jibinquan.github.io/\u0026#34; defaultContentLanguage = \u0026#34;zh-cn\u0026#34; 3.2 页面布局配置 #\r主要都在config\\_default\\params.toml下，这里是主要的配置表：\n这些设置是用于配置一个名为 Blowfish 的网站主题的。每个选项控制不同的功能和显示特性，使你可以根据需要自定义网站的外观和行为。以下是每个设置的简要说明：\n主题选项 (Theme Options) #\rcolorScheme: 设置网站的配色方案。例子中为 \u0026ldquo;blowfish\u0026rdquo;。 defaultAppearance: 设置网站的默认外观，可以是 \u0026ldquo;light\u0026rdquo; 或 \u0026ldquo;dark\u0026rdquo;。 autoSwitchAppearance: 自动切换外观，依据用户的系统设置。 enableSearch: 启用或禁用搜索功能。 enableCodeCopy: 启用或禁用代码复制功能。 图像和文本设置 #\rdisableImageOptimization: 禁用图像优化。 disableTextInHeader: 禁用标题中的文本。 defaultBackgroundImage: 设置默认背景图像。 defaultFeaturedImage: 设置所有文章的默认特色图像。 页眉和页脚 (Header and Footer) #\r[header] layout: 设置页眉布局方式。 [footer] showMenu, showCopyright, showThemeAttribution, showAppearanceSwitcher, showScrollToTop: 设置页脚显示的内容和功能。 主页设置 (Homepage) #\r[homepage] layout: 设置主页布局方式。 homepageImage: 主页图像。 showRecent, showRecentItems, showMoreLink, showMoreLinkDest, cardView, cardViewScreenWidth, layoutBackgroundBlur: 控制主页上显示的最近项目及其展示方式。 文章设置 (Article) #\r[article] 下的各个设置控制文章页面的各种显示选项，例如日期、浏览次数、作者信息、面包屑导航、分页等。 列表页面设置 (List) #\r[list] 下的各个设置控制列表页面的显示选项，如是否显示摘要、卡片视图、按年份分组等。 网站地图 (Sitemap) #\r[sitemap] excludedKinds: 排除的内容类型。 分类和标签 (Taxonomy and Term) #\r[taxonomy] 和 [term] 下的各个设置控制分类和标签页面的显示选项。 分析和验证 (Analytics and Verification) #\r[firebase], [fathomAnalytics], [umamiAnalytics]: 分析工具的配置选项。 [buymeacoffee]: 支持 Buy Me a Coffee 功能的设置。 [verification]: 用于搜索引擎验证的设置，如 Google、Bing、Pinterest 和 Yandex。 如果需要更多详细信息，可以参考主题文档：\r配置 · Blowfish。\n3.3 功能修改 #\r3.3.1 内联数学公式修改 #\r​\tTypora自带的内联公式语法是 $ ... $ ，而Blowfish默认的KaTeX渲染语法是\\\\( ... \\\\) ，在这种模式下，你要不就放弃在Typora上预览内联公式，要不就放弃在网页上渲染内联公式…属实离谱。于是我们找到了以下解决方法：\njavascript - 如何使用 KaTeX 渲染 $..$ 中的所有内联公式？_Stack Overflow中文网\n来到文件\\themes\\blowfish\\assets\\lib\\katex\\auto-render.min.js，找到其中代码\nn.delimiters=n.delimiters||[{left:\u0026#34;$$\u0026#34;,right:\u0026#34;$$\u0026#34;,display:!0},{left:\u0026#34;\\\\(\u0026#34;,right:\u0026#34;\\\\)\u0026#34;,display:!1},{left:\u0026#34;\\\\begin{equation}\u0026#34;,right:\u0026#34;\\\\end{equation}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{align}\u0026#34;,right:\u0026#34;\\\\end{align}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{alignat}\u0026#34;,right:\u0026#34;\\\\end{alignat}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{gather}\u0026#34;,right:\u0026#34;\\\\end{gather}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{CD}\u0026#34;,right:\u0026#34;\\\\end{CD}\u0026#34;,display:!0},{left:\u0026#34;\\\\[\u0026#34;,right:\u0026#34;\\\\]\u0026#34;,display:!0}] 改成\nn.delimiters=n.delimiters||[{left:\u0026#34;$$\u0026#34;,right:\u0026#34;$$\u0026#34;,display:!0},{left:\u0026#34;$\u0026#34;,right:\u0026#34;$\u0026#34;,display:!1},{left:\u0026#34;\\\\begin{equation}\u0026#34;,right:\u0026#34;\\\\end{equation}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{align}\u0026#34;,right:\u0026#34;\\\\end{align}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{alignat}\u0026#34;,right:\u0026#34;\\\\end{alignat}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{gather}\u0026#34;,right:\u0026#34;\\\\end{gather}\u0026#34;,display:!0},{left:\u0026#34;\\\\begin{CD}\u0026#34;,right:\u0026#34;\\\\end{CD}\u0026#34;,display:!0},{left:\u0026#34;\\\\[\u0026#34;,right:\u0026#34;\\\\]\u0026#34;,display:!0}],n.ignoredTags=n.ignoredTags|| 那么，在你的文章需要使用公式的时候，只需在开头加上一行 即可。（我也不知道这样改会有什么问题，先这么着吧）\n找着问题了…\\\\公式换行不能用了……\n3.3.2 字数统计 #\r如果你发现页面统计的字数远远少于文章的字数，请在hugo.toml的defaultContentLanguage = \u0026quot;zh-cn\u0026quot;后加入下面一行：\nhasCJKLanguage = true 如果还是字数还是少，请来到\\themes\\blowfish\\config\\_default\\hugo.toml下，修改相同字段：\ndefaultContentLanguage = \u0026#34;zh-cn\u0026#34; hasCJKLanguage = true 四、部署到Github #\r4.1 创建仓库 #\r在Github方面，需要新建两个仓库。一个是网页源代码仓库，便于进行版本管理。一个是 Github Page 仓库，用于网页部署。\n新建仓库在 Your Repositories 下选 New 就行。\n需要注意的是 GitHub Pages 仓库：\n必须使用特殊的命名格式\u0026lt;username.github.io\u0026gt;， \u0026lt;username\u0026gt; 是你的 GitHub 的用户名，如下图所示。 需要勾选 Public，设置为公开仓库。 勾选添加 README 文件，这会设置 main 分支为仓库的默认主分支。 4.2 生成静态网页 #\r在 Hugo 网站文件夹的根目录下执行 hugo 命令构建静态 HTML 网页，生成的 HTML 文件默认存放在 public 文件夹中。\n示例：\n$ hugo Start building sites … hugo v0.126.2-8f3d902ce51512931f5759e9279d93e346c9e649+extended windows/amd64 Bu ildDate=2024-05-30T15:19:22Z VendorInfo=gohugoio | ZH-CN | EN -------------------+-------+----- Pages | 36 | 11 Paginator pages | 0 | 0 Non-page files | 28 | 0 Static files | 8 | 8 Processed images | 86 | 0 Aliases | 10 | 0 Cleaned | 0 | 0 Total in 450 ms 注意在部署前一定是使用hugo进行网页构建，而不能使用hugo server否则会将baseurl编译为本地链接导致部署后搜索等部分功能失效。\r4.3 git 初始化与部署 #\r首先在根目录下，新建一个.gitignore 文件， 写入如下内容：\npublic/ 因为public目录需要单独上传到Github Page 仓库，如果不进行忽略的话会造成嵌套无法进行初始化。\n完成后在根目录下，git bash运行（后续操作都在git bash下进行）\ngit init -b main 如果你是Windows系统，可能需要取消Git自动的换行替换\r即执行：\ngit config core.autocrlf false 具体详见\rwindows系统下github_pages部署hugo页面js与css无法使用或文档渲染出错问题\n如果你还没有申请SSH-Key，请先参考下面的文章，进行SSH-Key的申请，否则无法通过SSH进行git上传\n具体详见\r为github设置ssh-key\n将博客文件夹关联远程 GitHub Pages 仓库，使用 GitHub Pages 仓库的 SSH 链接。\n​\t首先获得SSH链接\n​\t​\t然后运行（注意把链接替换为你自己的）\ngit remote add origin git@github.com:JiBinquan/My_Blog.git 之后就是先拉取，后推送\ngit pull --rebase origin main # 拉取远端仓库现有的文件 git add . # 添加修改过得文件， . 表示所有，也可以指定文件 git commit -m \u0026#34;FST\u0026#34; # 提交内容的说明信息 git push origin main # 推送到远端main分支 之后就是进入public目录，对Github Pages进行相同的操作（.gitignore 文件可以不用了，注意这里远端SSH链接是Github Pages仓库中获取的）\ngit init -b main git config core.autocrlf false git remote add origin git@github.com:Jibinquan/Jibinquan.github.io.git # 这里是Github Pages仓库的SSH链接，不要搞混 git pull --rebase origin main git add . git commit -m \u0026#34;FST\u0026#34; git push origin main 转到 GitHub 查看相关仓库中是否存在刚刚推送的文件，存在则代表推送成功。\n如果没有问题的话，现在访问Yourusername.github.io应该就能看到你的博客啦\n五、后续文章更新与部署 #\r5.1 新建文章 #\r新建文件，在根目录下：\nhugo new posts/NewArticle/index.md 找到对应目录下找到index.md进行修改，图片可以新建一个pic目录放到里面，blowfish还支持特征图和背景图定制，只需要命名为feature.jpg/png和background.jpg/png即可\n本地预览，根目录下：\nhugo server -D 没问题后，构建静态网页\nhugo 来到public目录下，进行推送\ngit add . git commit -m \u0026#34;更新内容\u0026#34; git push origin main 5.2 删除文章 #\r在content和public直接删除相关目录即可，然后进行构建推送\nhugo git add . git commit -m \u0026#34;更新内容\u0026#34; git push origin main 结语 #\r第一次部署博客对于前端苦手的我还是太折磨了，各种bug整到崩溃，后续需要啥功能等有空了再说吧……\n参考文献 #\rGithub Pages + Hugo 搭建个人博客 - 渣渣的夏天 (zz2summer.github.io)\n如何用 GitHub Pages + Hugo 搭建个人博客 · 小绵尾巴 (cuttontail.blog)\n关于本站的建造 · 群青流星 (karlukle.site)\nHugo主题blowfish搜索功能失效的解决办法之一 - 哔哩哔哩 (bilibili.com)\n","date":"2024年6月4日","externalUrl":null,"permalink":"/posts/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/","section":"文章","summary":"本站搭建过程记录，也记录了一些踩过的坑。想要搭建一个hugo博客的朋友看这篇应该可以搭建出一个最基本的能部署的博客了。","title":"Hugo博客搭建记录","type":"posts"},{"content":"\r正在施工 #\r","date":"2024年6月2日","externalUrl":null,"permalink":"/statistics/","section":"欢迎访问","summary":"","title":"统计","type":"page"},{"content":"\rAbout Me #\r你好，我是 Ji Binquan ，硕士在读，就读于东北大学（沈阳）。\n我的研究方向是自然语言处理，主要关注于大模型研究和应用\n菜鸡一枚（研究和游戏都是）\n喜欢看电影打游戏和出去玩（除了干活其他的都挺喜欢…）\nAbout Site #\r本站生日：2022-08-30 使用 Hugo 搭建，部署在 GitHub Pages 上 您能找到这儿来真不容易…感谢访问！ Contact Me #\r如果您想与我取得联系，欢迎给我发送邮件\nEmail ","date":"2024年6月2日","externalUrl":null,"permalink":"/about/","section":"欢迎访问","summary":"","title":"关于","type":"page"},{"content":"\r我的朋友们 #\rabel\u0026#39;s blog Record abel\u0026#39;s Paper and Tech Notes here. ","date":"2024年6月2日","externalUrl":null,"permalink":"/friends/","section":"欢迎访问","summary":"","title":"友情链接","type":"page"},{"content":"\r编写博客，不只在于交流表达，也是为了抵抗遗忘。\n内容仅为个人观点，欢迎评论交流。\n如有疏漏错误之处，恳请批评指正。\n","date":"2024年6月2日","externalUrl":null,"permalink":"/","section":"欢迎访问","summary":"","title":"欢迎访问","type":"page"},{"content":"主要参考：\rMySQL详细安装教程，关于msi版和zip版详解，Windows - 知乎 (zhihu.com)\n1. MySQL 下载 #\r进入MySQL官方下载：\rMySQL :: Download MySQL Community Server\n可以看到下载页面提供了不同版本、不同系统和不同种类安装包的选择。\n对于MySQL的版本，大概分两种：\n5.x：版本更老，更稳定，比较主流\n8.x：版本更新\n安装包也提供了三种：\nMSI：微软格式的安装包，傻瓜式安装，缺点是安装过程都是默认设计的，无法直接进行个性化设计、微调，没法专项安装数据库，重新调整需要安装完成后，进入配置文件，或者使用一些mysql命令在命令界面更改。\nZIP：直接解压，”免安装“，需要手动配置一些参数。\nDebug：通常称为调试版本，它包含调试信息，并且不作任何优化，所以容量比Release大很多。（优化会使调试复杂化，因为源代码和生成的指令间关系会在优化过程中变得更复杂），不优化便于程序员进行代码调试等操作。Debug模式下生成两个文件，除了.exe或.dll文件外，还有一个.pdb文件，该文件记录了代码中断点等调试信息\n本次使用的是ZIP的Release版本安装包，选择版本和系统就可以下载。\n2.解压压缩包，设置配置文件 #\r把压缩包解压到一个合适的路径（自己的目标安装路径，最好不要有中文），然后新建一个 my.ini 配置文件。（可以新建一个文本文件my.txt，配置好后将后缀改为**.ini**）\nmy.ini是MySQL安装时候回去配置信息的配置文件。主要记录用户信息、系统信息、数据库参数设置等\n可以先配置以下参数，其他先默认：\n[client] port = 3306 [mysqld] #设置3306端口（默认3306，若被占用，换其他的） port = 3306 # 设置mysql的安装目录 basedir=D:\\\\MySQL\\\\mysql-8.1.0-winx64 # 设置mysql数据库的数据的存放目录 datadir=D:\\\\MySQL\\\\mysql-8.1.0-winx64\\\\data # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB #这个需要注意一下，不然报错 # 其原因是从 5.6开始，timestamp 的默认行为已经是 deprecated 了。 explicit_defaults_for_timestamp=true [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 写完保存关闭文件。\n另外，运行MySQL需要相应的DirectX和VC++库，不然可能运行报错（经常打游戏的应该都安装的差不多了…我也没遇到这个问题…）\n3.初始化 MySQL #\r以管理员身份运行命令提示符\ncd指令切换到安装路径下的bin文件夹，便于后续操作\n进入后，输入\nmysqld --initialize --user=mysql --console 初始化mysql数据库\n初始化结果会给你一个随机的MySQL数据库密码（上图红框部分），切记要复制下来这个临时密码，复制临时密码、复制临时密码、复制临时密码\n4.建立 MySQL 服务 #\r输入安装命令：\nmysqld --install mysql81 mysql81是服务名字，可以把名字设为mysql+版本号，或者别的啥\n安装成功后使用临时密码进入mysql数据库 :\nmysql -uroot -p 然后就报错了：\n参考：\rMySQL错误提示：“ERROR 2003 (HY000): Can\u0026rsquo;t connect to MySQL server on \u0026rsquo;localhost\u0026rsquo; (10061)”解决办法_小C博客的博客-CSDN博客\n如果不是端口被占用的情况的话，可能是因为MySQL服务没有启动，使用使用：net start 启动服务（注意后面跟的是刚才install的服务名）\nnet start mysql81 之后即可正常进入数据库\n更改一个密码，随机密码记不住：\n注意8.x版本的修改密码语句与5.x的不同，使用网上一般教程提供的\nset password=password(\u0026#39;123456\u0026#39;); 会报语法错，参考某问答模型回答可以使用以下语句\nSET PASSWORD FOR root@localhost = \u0026#39;123456\u0026#39;; 123456就是密码，当然自己随便设置\n5. 将MySQL添加至环境变量 #\r为了方便操作，将MySQL添加到环境变量中。\n右键此电脑 -\u0026gt; 属性 -\u0026gt;高级系统设置 -\u0026gt;环境变量\n添加: 变量名:MYSQL_HOME 变量值:D:\\MySQL\\mysql-8.1.0-winx64 #这里是你自己的安装路径 编辑: 变量名:path 最后添加上:D:\\MySQL\\mysql-8.1.0-winx64\\bin 6.简单的SQL指令 #\r看看能不能用了\n连接数据库\nmysql -h 127.0.0.1 -P 3306 -u root -p -h：后跟数据库服务器ip地址，本机就127.0.0.1\n-P：端口\n-u：用户\n在本机3306端口下，可以简写为\nmysql -uroot -p 查看当前数据库\nshow databases; ![在这里插入图片描述](\rhttps://img-blog.csdnimg.cn/97412ae835904771b4f8f8a99c0c14e0.png\n退出数据库\nexit; 能用说明安装完成\n","date":"2023年8月14日","externalUrl":null,"permalink":"/posts/%E8%A1%A5%E6%A1%A3mysql8.1%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85/","section":"文章","summary":"","title":"（补档）MySQL8.1版本安装","type":"posts"},{"content":"","date":"2023年8月14日","externalUrl":null,"permalink":"/tags/%E8%A1%A5%E6%A1%A3/","section":"Tags","summary":"","title":"补档","type":"tags"},{"content":"\rTransformer：Attention Is All You Need\n-1.笔记 #\r-1.1 作者研究的主要问题 #\r一种在RNN和CNN之外的，完全无需循环和卷积的序列传导模型的构建及其在机器翻译乃至更多方面的应用。\n-1.2 研究工作的动机 #\r在解决序列建模和转换问题时，当前主流的模型为RNN及其变体（LSTM、GRU）。之后人们一直在推动循环语言模型和编码器-解码器之间的结合。\n但是由于RNN每一步的隐藏状态都基于前一步的隐藏状态推出的顺序特性（RNN相关算法只能从左向右依次计算或者从右向左依次计算）这种机制带来了两个问题：\n时间片t的计算依赖t-1时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。 另外的避免循环顺序限制的思路是使用卷积神经网络。但是这种模型由于卷积窗口大小限制，这使得学习远程位置之间的依赖性变得更加困难（间隔很大的话要叠很多层卷积）。\n-1.3 解决思路 #\rattention机制可以建模依赖关系而不考虑其在输入或输出序列中的距离，已经成为序列建模和传导模型不可或缺的部分，但是在这之前还大多与循环网络一起使用。\n而Transformer的提出，避免了循环，只依赖于Attention机制，解决了上面两个问题，突破了序列化模型的限制。\n卷积神经网络的优点在于可以进行多通道的学习和输出，在transformer中，采用多头注意力机制来达到类似的效果。\n-1.4 解决方法 #\r以编码器-解码器架构为基础构建整体模型。在词向量嵌入时加入位置编码来弥补attention机制对整体序列顺序关系关注不足的问题。解码器通过多头注意力机制和前馈网络两个子层加之残差连接和正则化进行处理，提取序列整体信息。解码器先通过有遮蔽的多头注意力机制，防止第 i 个单词知道 i+1 个单词之后的信息，使之符合顺序翻译的处理过程。然后再结合解码器提取的信息进行多头注意力和前馈网络进行处理。\n-1.5 实验效果 #\r又快又好\n【attention存在的缺陷】attention在并行性和信息糅合性上表现优越，但是由于attention对整个模型的假设做的更少，所以相较于RNN和CNN需要更多的数据和更大的模型才能训练出来。\n-1.6 数据集 #\r在WMT2014数据集上进行训练（使用BPE：仅使用词根进行学习，忽略时态语态的变化，缩小字典大小，且双语共用一个字典，编码解码embedding共享权重）【英语-德语，英语-法语两个任务】\n-1.7 相关工作对比 #\r-1.8 思考 #\r本片文章由于篇幅限制，所以“干货满满”，并没有用很多的文笔来去“讲故事”，但是自己写文章还是要写一写自己的研究动机、工作价值、设计理念和进一步思考等内容，让自己的文章显得有深度一些。\nTransformer 是AI领域继RNN、CNN之后又一框架性的模型，可以在各个领域都能取得比较好的成绩（尤其在NLP领域）。\n最新的研究表明，也不是只要Attention就能All Right了，它的只要作用就是吧整个序列的信息进行整合，残差链接和MLP也是必不可少的。\nAttention能比较好的处理序列信息的原因还是采用了归纳偏置，但是由于这样假设的更加一般，其对数据信息的抓取能力变差了，所以要使用更多的数据、使用更大的模型才能训练出比较好的效果。\n更好的框架，还会出现。\n-2.名词解释 #\r前馈网络： #\r前馈神经网络（feedforward neural network，FNN），简称\r前馈网络，是\r人工神经网络的一种。前馈神经网络采用一种单向多层结构。其中每一层包含若干个神经元。在此种神经网络中，各神经元可以接收前一层神经元的信号，并产生输出到下一层。第0层叫输入层，最后一层叫输出层，其他中间层叫做隐含层（或隐藏层、隐层）。隐层可以是一层。也可以是多层 。\n整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。\n激活函数： #\rReLU函数 ReLU（rectified linear unit）函数提供了⼀个很简单的⾮线性变换。给定元素 $x$ ，该函数定义为： $$ ReLU(x) = max(x,0) $$ 可以看出，ReLU函数只保留正数元素，并将负数元素清零。\n导数：\nsigmoid函数\nsigmoid函数可以将元素的值变换到0和1之间： $$ sigmod(x)=\\frac{1}{1+exp(-x)} $$ 依据链式法则，sigmoid函数的导数：$sigmoid(x) = sigmoid(x)(1-sigmoid(x))$\n当输⼊为0时，sigmoid函数的导数达到最⼤值0.25；当输⼊越偏离0时，sigmoid函数的导数越接近0。\ntanh函数\ntanh（双曲正切）函数可以将元素的值变换到-1和1之间： $$ tanh(x) = \\frac{1-exp(-2x)}{1+exp(-2x)} $$\n当输⼊接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。\n依据链式法则，tanh函数的导数：$tanh\u0026rsquo;(x) = 1-tanh^2(x)$ 当输⼊为0时，tanh函数的导数达到最⼤值1；当输⼊越偏离0时，tanh函数的导数越接近0。\n多层感知机（MLP）： #\r多层感知机就是含有⾄少⼀个隐藏层的由全连接层组成的神经⽹络，且每个隐藏层的输出通过激活函数进⾏变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。多层感知机按以下方式计算输出： $$ H=\\phi(XW_h+b_h) \\ O=HW_o+b_o $$ 其中 $\\phi$ 表⽰激活函数。\n残差连接： #\r随着\r深度网络层数的加深，带来了一系列问题，如梯度消失，梯度爆炸，模型容易过拟合，计算资源的消耗等问题。随着网络层数的增加发生了网络退化现象，loss先是下降并趋于饱和，然后loss增加。\n针对这些问题，在模型中将输入和输入的非线性变化进行叠加。 $$ x_{l+1}=x_l+F(x_l，W_l) $$\n残差块分为两部分，直接映射部分和残差部分。为weight代表卷积，addition代表单位加操作。\n如果对函数求导，可以看出x的导数为1，另外的$F(x,w)$导数不可能一直为-1，所以不会出现梯度消失问题。\n在卷积网络中$x_l$和$x_{l+1}$可能存在维度不一样的问题，所以为了便于预算，一般对x加一个1*1卷积运算，\n损失函数 #\r来自：损失函数（Loss Function）—杰奏-知乎\n损失函数是用来评估模型对于数据集的拟合程度，即预测结果与真实标签的偏差。如果模型预测的结果是正确的，则其损失函数的值会很小；如果模型预测的结果是错误的，则其损失函数的值会很大。一言以蔽之，损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。\n损失函数使用主要是在模型的训练阶段，每个批次的训练数据送入模型后，通过前向传播输出预测值，然后损失函数会计算出预测值和真实值之间的差异值，也就是损失值。得到损失值之后，模型通过反向传播去更新各个参数，来降低真实值与预测值之间的损失，使得模型生成的预测值往真实值方向靠拢，从而达到学习的目的。即通过计算损失获得模型权重的梯度，然后通过反向传播相应地更新这些权重。\n常见损失函数\n基于距离度量的损失函数 均方误差损失函数（MSE） L2损失函数 L1损失函数 Smooth L1损失函数 huber损失函数 基于概率分布度量的损失函数 KL散度函数（相对熵） 交叉熵损失 softmax loss 损失函数（包含softmax激活函数的损失计算） Focal loss 交叉熵 #\r来自：“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”——王木头学科学-知乎\n熵\n信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。即有定义：\nH(S)表示S系统的熵，E是求期望，I(x)是求信息量，P(xi)表示xi事件的概率。\n$$ H(X) = -\\sum^{n}_{i=1}p(x_i)log(p(x_i)) $$\n至于为什么信息量要定义为log形式，因为信息量在以概率作为变量描述时，应该满足对于独立事件A、B，必须有$I(P(AB)) = I(P(A)) + I(P(B))$\n又$P(AB) = P(A) * P(B)$，即对于信息量的运算，自变量的乘法等于函数值的加法，对数运算就可以很好的满足这个条件。此外，概率越小，不确定性越强，信息量越大，所以信息量应该取负对数以变为单调减小。\nKL散度\n假如说，下面这个图表示的是两个系统的概率分布，其中系统S代表的是真实的规律，系统O代表的是机器学习模型里面猜测的那个规律。那么比较每一个事件 $x_i$ 对应的信息量。如果每一个事件的信息量都是相同的，那么两个概率分布肯定就是相同的了。\r相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)常用于描述样本的真实分布，例如[1,0,0,0]表示样本属于第一类，而q(x)则常常用于表示预测的分布，例如[0.7,0.1,0.1,0.1]。显然使用q(x)来描述样本不如p(x)准确，q(x)需要不断地学习来拟合准确的分布p(x)。\n则KL散度的定义式就有：\n即： $$ D_{KL}(p||q) = \\sum^{n}_{i=1}p(x_i)log(\\frac{p(x_i)}{q(x_i)}) $$ n表示事件可能发生的情况总数，KL散度的值越小表示两个分布越接近。\n可以注意到，这个定义本质上也是一个加权求和，求和的是两个系统中同一个事件的信息量的差值，加的那个权重是其中一个系统里这个事件的概率值。从这里也能看出来，这里的系统S和系统O，它们并不是平等的，把S和O交换之后并不能保证得到相同的值。也就是说，KL散度它相当于会在两个系统中挑选了一个作为基准（我这里用的是S系统作为基准），拿另一个系统与这个基准进行比较。因为这是用S系统的熵作为基准，去衡量另一个系统O的熵，所以KL散度也叫相对熵。\n交叉熵\n对KL散度进行变形，，发现KL散度可以被分成两个部分，其中后面的那个部分计算出来就是系统S的熵，这部分算出来是多少是与系统O无关的。所以，真正决定KL散度的其实是前面那部分，它的大小决定着KL散度的大小。\n于是这部分就可以被单独拿出来讨论，所以它就被定义成为了交叉熵。想知道系统S和系统O是否一样，不需要去计算它们的KL散度，只需要去看它们的交叉熵，交叉熵的值与系统S的熵最接近时，目标达成。从数学上可以证明，交叉熵的值一定是会大于等于系统S的熵的。所以，只需要考虑如何对交叉熵求最小值就行了。一个系统与系统S的交叉熵最小值，那么这个系统与S最接近。\nsoftmax #\r来自：softmax是为了解决归一问题凑出来的吗？和最大熵是什么关系？——王木头学科学-BiliBili\n对于二分类问题\nsigmoid函数针对两点分布提出。神经网络的输出经过它的转换，可以将数值压缩到(0,1)之间，得到的结果可以理解成“分类成目标类别的概率P”。而不分类到该类别的概率，就是(1 - P)，这也是典型的两点分布的形式。所以，对于二分类问题，sigmoid函数是很好的连续激活函数。\n对于多分类问题\n激活函数处理的数据就应该是以向量形式出现，对于真实数据 $y^{(k)}_i$ ，其在相应类别位置概率为1，其余概率为零。\n而对于预测结果，则要求对于任何输入，激活函数的输出值应该恒大于零（指数函数就可以很好的将值域映射到正数域），且不同类别加起来的总概率应该为1，（那就直接算总和权重就行）。\n故有\nNorm #\rhttps://zhuanlan.zhihu.com/p/91125751\n一、定义\n数据标准化（Normalization），也称为归一化，归一化就是将你需要处理的数据在通过某种算法经过处理后，限制将其限定在你需要的一定的范围内。\n数据标准化处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要对数据进行归一化处理，解决数据指标之间的可比性问题。\n二、优点\n从定义中我们可以得知，数据归一化的目的就是为了把不同来源的数据统一到同一数量级（或者是一个参考坐标系）下，这样使得比较起来有意义。归一化使得后面数据的处理更为方便，它有两大优点：\n（1）归一化提高梯度下降发求解最优解的速度\n如下图，蓝色的圈圈表示特征的等高线。其中左图的两个特征x1和x2区间相差较大，x1~[0,2000],x2~[1,5],期所形成的等高线在一些区域相距非常远，当使用梯度下降法求解最优解的时候，很可能垂直等高线走“之字型”路线（左图红色路径），从而导致需要迭代很多次才能收敛，也可能不收敛。而右图对两个原始特征进行了归一化处理，其对应的等高线显得很圆，在梯度下降的时候就能很快收敛。因此，如果机器学习使用梯度下降法求解最优解时，归一化往往是非常有必要的。\n（2）归一化有可能提高精度\n一些分类器（如KNN）需要计算样本之间的距离（如欧式距离）。如果一个特征值域范围非常大，那么距离计算就要取决于这个特征，如果这时实际情况是值域范围小的特征更重要，那么归一化就要起作用了。\n三、归一化方法\n（1）线性归一化，也称min-max标准化、离差标准化；是对原始数据的线性变换，使得结果值映射到[0,1]之间。转换函数如下： $$ x\u0026rsquo;=\\frac{x-min(x)}{max(x)-min(x)} $$\n这种归一化比较适用在数值较集中的情况。但是这种方法有一个缺陷，就是如果max和min不稳定的时候，很容易使得归一化的结果不稳定，易受极值影响，影响后续使用效果。所以在实际应用中，我们一般用经验常量来替代max和min。\n（2）标准差归一化，也叫Z-score标准化，这种方法给予原始数据的均值（mean，μ）和标准差（standard deviation，σ）进行数据的标准化。经过处理后的数据符合标准正态分布，即均值为0，标准差为1，转化函数为： $$ x^*=\\frac{x-μ}{σ} $$\n（3）非线性归一化，这种方法一般使用在数据分析比较大的场景，有些数值很大，有些很小，通过一些数学函数，将原始值进行映射。一般使用的函数包括log、指数、正切等，需要根据数据分布的具体情况来决定非线性函数的曲线。\nLayerNorm #\r对于三维输入\nBatchNorm是对一个batch-size样本内的每个特征做归一化，LayerNorm是对每个样本的所有特征做归一化。\n形象点来说，假设有一个二维矩阵。行为batch-size，列为样本特征。那么BN就是竖着归一化，LN就是横着归一化。\n它们的出发点都是让该层参数稳定下来，避免梯度消失或者梯度爆炸，方便后续的学习。但是也有侧重点。\n一般来说，如果你的特征依赖于不同样本间的统计参数，那BN更有效。因为它抹杀了不同特征之间的大小关系，但是保留了不同样本间的大小关系。（CV领域）\n而在NLP领域，LN就更加合适。因为它抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系。对于NLP或者序列任务来说，一条样本的不同特征，其实就是时序上字符取值的变化，样本内的特征关系是非常紧密的。\nDropout #\rhttps://zhuanlan.zhihu.com/p/38200980\n1. Dropout出现的原因 #\r在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。\n过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。\n综上所述，训练深度神经网络的时候，总是会遇到两大缺点：\n（1）容易过拟合\n（2）费时\nDropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。\n2. 什么是Dropout #\rDropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。\nDropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。\n3. Dropout具体工作流程 #\r假设我们要训练这样一个神经网络，如图2所示。\n图2：标准的神经网络\n输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：\n（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）\n图3：部分临时被删除的神经元\n（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。\n（3）然后继续重复这一过程：\n恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新） 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 不断重复这一过程。\n3. Dropout在神经网络中的使用 #\rDropout的具体工作流程上面已经详细的介绍过了，但是具体怎么让某些神经元以一定的概率停止工作（就是被删除掉）？代码层面如何实现呢？\n下面，我们具体讲解一下Dropout代码层面的一些公式推导及代码实现思路。\n（1）在训练模型阶段\n无可避免的，在训练网络的每个单元都要添加一道概率流程。\n图4：标准网络和带有Dropout网络的比较\n对应的公式变化如下：\n没有Dropout的网络计算公式： 采用Dropout的网络计算公式： 上面公式中Bernoulli函数是为了生成概率r向量，也就是随机生成一个0、1的向量。\n代码层面实现让某个神经元以概率p停止工作，其实就是让它的激活函数值以概率p变为0。比如我们某一层网络神经元的个数为1000个，其激活函数输出值为y1、y2、y3、\u0026hellip;\u0026hellip;、y1000，我们dropout比率选择0.4，那么这一层神经元经过dropout后，1000个神经元中会有大约400个的值被置为0。\n*注意：* 经过上面屏蔽掉某些神经元，使其激活值为0以后，我们还需要对向量y1……y1000进行缩放，也就是乘以1/(1-p)。如果你在训练的时候，经过置0后，没有对y1……y1000进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。\n（2）在测试模型阶段\n预测模型的时候，每一个神经单元的权重参数要乘以概率p。\n图5：预测模型时Dropout的操作\n测试阶段Dropout公式：\nwtest(l)=pW(l)\n4. 为什么说Dropout可以解决过拟合？ #\r（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。\n（2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。\n（3）**Dropout类似于性别在生物进化中的角色：**物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。\nLable Smoothing #\r标签平滑是一种正则化技术，它扰动目标变量，使模型对其预测的确定性降低。它被视为一种正则化技术，因为它限制了softmax 函数的最大概率使最大概率不会比其他标签大得多（过度自信）。\n在本文中，我们将解释标签平滑的原理，实现了一个使用这种技术的交叉熵损失函数，并评估了它的性能。\n标签平滑\n我们有一个多类分类问题。 在此类问题中，目标变量通常是一个one-hot向量，其中正确类别的位置为1，其他位置为0。 这是与二元分类不同的任务因为在二分类中只有两个可能的类，但是在多标签分类中，一个数据点中可以有多个正确的类。 因此，多标签分类问题的需要检测图像中存在的每个对象。\n标签平滑将目标向量改变少量 ε。 因此，我们不是要求我们的模型为正确的类别预测 1，而是要求它为正确的类别预测 1-ε，并将所有其他类别预测为 ε。\n带有标签平滑的交叉熵损失函数转化为下面的公式。\n$$ {cross-entropy} loss = (1-ε)ce(i)+ε\\sum\\frac{ce(j)}{N} $$\n在这个公式中，ce(x) 表示 x 的标准交叉熵损失（例如 -log(p(x))），ε 是一个小的正数，i 是正确的类，N 是类的数量。\n直观地说，标签平滑将正确类的概率值限制为更接近其他类的概率值。 通过这种方式，它被用作正则化技术和对抗模型过度自信的方法。\n-3.背景知识 #\r-3.1 序列数据处理模型 #\r在RNN出现之前，卷积神经网络CNN和普通的算法大部分都是输入和输出的一一对应，也就是一个输入得到一个输出。不同的输入之间是没有联系的，但是对于处理序列数据，比如机器翻译等，前面的输入对后面应该是有影响的，因此提出RNN，用来处理「序列数据 – 一串相互依赖的数据流」的场景。\nRNN：循环神经网络\nRNN（Recurrent Neural Network）循环神经网络是一类用于处理序列数据的神经网络，传统神经网络的结构为：输入层 – 隐藏层 – 输出层，如图1。相比于之前的神经网络RNN最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。\n但是同时也可以看出RNN也存在一定的缺陷，短期的记忆影响较大（如图2中的橙色区域），但是长期的记忆影响就很小（如图2中黑色和绿色区域），这就是 RNN 存在的短期记忆问题，不适合处理长序列问题。同时在训练时会出现梯度消失或者梯度爆炸的问题。\nLSTM：长短期记忆网络\n为了解决RNN中存在的梯度消失和梯度爆炸的问题，因为这个问题的存在，RNN没办法回忆起久远的记忆，因此在RNN基础上提出了LSTM，相比于普通的RNN，LSTM多出了三个控制器，分别是输入控制，输出控制和忘记控制，能更好的提取长期信息。\nGRU：门控循环网络\nGated Recurrent Unit (GRU)就是lstm的一个变体，它将忘记门和输入门合成了一个单一的更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，是非常流行的LSTM的变体。\n0. 摘要 #\rtransformer：相较于传统的基于循环神经网络或卷积神经网络的的主流的序列传导模型，Transformer基于Attention机制完全避免了循环和卷积。具有结构简单、训练更快和效果更好（机器翻译方面）以及普适性更强的特点。\n1. 简介 #\r在解决序列建模和转换问题时，当前主流的模型为RNN及其变体（LSTM、GRU）。之后人们一直在推动循环语言模型和编码器-解码器之间的结合。\n但是由于RNN每一步的隐藏状态都基于前一步的隐藏状态推出的顺序特性（RNN相关算法只能从左向右依次计算或者从右向左依次计算）这种机制带来了两个问题：\n时间片t的计算依赖t-1时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。 attention机制可以建模依赖关系而不考虑其在输入或输出序列中的距离，已经成为序列建模和传导模型不可或缺的部分，但是在这之前还大多与循环网络一起使用。\n而Transformer的提出，避免了循环，只依赖于Attention机制，解决了上面两个问题，突破了序列化模型的限制。\n2. 背景 #\r另外的避免循环顺序限制的思路是使用卷积神经网络。但是这种模型由于卷积窗口大小限制，这使得学习远程位置之间的依赖性变得更加困难（间隔很大的话要叠很多层卷积）。\n卷积神经网络的优点在于可以进行多通道的学习和输出，在transformer中，采用多头注意力机制来达到类似的效果。\n3. 模型架构 #\r主要框架 #\r输入数据： 通过Word2Vec等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为512。\n基本结构： Transformer的本质上是一个Encoder-Decoder的结构：\n如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入：\n详细分析 #\r编码器： #\r以“我有一只猫”为例，整体的Encoder由6个下面基础的encoder模块构成：\n（1）输入 #\r词嵌入方法得到的“我有一只猫”中每一个词的特征向量，维度为$d_{model}=512$. Position encoding：在self-attention中使用的是全局信息，没有考虑位置顺序信息，但在翻译中位置顺序信息十分重要（句子单词顺序打乱，语义大概率会发生变化）。因此考虑加入Position encoding来加入这个序列的位置信息，生成的向量维度和词的特征向量维度相同，在本文中为512。具体方法如下： $$ PE_{(ois,2i) = sin(pos/10000^{2i/d_{model}})}\\ PE_{(ois,2i+1) = cos(pos/10000^{2i/d_{model}})} $$\n其中，pos 表示单词在句子中的位置， PE 的维度d与词 Embedding 一样，也是512，i 是每个维度，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。记住 “奇数位：cos，偶数位：sin”。\n举个例子：我 有 一只 猫\n其中“一只”的pos=3，并且 $d_{model}=512$ ，因此得到的位置的向量为：\n最后，将通过词嵌入方法得到的特征向量和Position encoding得到的向量求和得到最终的输入X。\n（2）注意力模块 #\r在Transformer的encoder中，输入数据首先会经过一个叫做“multi-head attention”的模块，它是由多个‘self-attention’的模块所构成的，因此先来研究‘self-attention’.\nself-attention\n自注意力模块：Query、Key和Value向量都来自自己本来的词嵌入向量，所以是“自”注意力。\n在self-attention中，每个单词有3个不同的向量，它们分别是Query向量 Q ，Key向量 K 和Value向量 V ，长度均是64。它们是通过嵌入向量 X 乘以三个不同的权值矩阵$ W^Q,W^K,W^V $得到，其中三个矩阵的尺寸也是相同的。均是 512×64 ,注意 X, Q, K, V 的每一行都表示一个单词下图为计算过程的示意图:\n在得到Q,K,V后，‘self-attention’经过计算最终得到一个特征向量Z，这个Z就是论文中公式1中的Attention(Q,K,V)： $$ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ 下面介绍一下具体的计算过程：\n论文中的图：\n第一步：Q 乘以 K 的转置后，得到的这个矩阵可以表示单词之间的 attention 强度。下图为 Q 乘以 K 的转置，1234 表示的是句子中的单词。\n第二步：得到 $QK^T $之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1\n第三步： Softmax 矩阵之后可以和 V 相乘，得到最终的编码信息输出 Z。\n第四步：上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出$ Z_1 $等于所有单词值$ V_i $根据 attention 系数的比例加在一起得到，如下图所示：\n这里要注意：\nAttention的作用\nattention输出的是Value的加权和，权重来自于Query与Key的相似性计算。\nQ,K,V的含义： Query，Key，Value的概念取自于信息检索系统，举个简单的搜索的例子来说。当你在某电商平台搜索某件商品（年轻女士冬季穿的红色薄款羽绒服）时，你在搜索引擎上输入的内容便是Query，然后搜索引擎根据Query为你匹配Key（例如商品的种类，颜色，描述等），然后根据Query和Key的相似度得到匹配的内容（Value)。在机器翻译中value和key是一个。\nself-attention中的Q，K，V也是起着类似的作用，在矩阵计算中，内积是计算两个矩阵相似度的方法之一，因此式1中使用了$ QK^T$ 进行相似度的计算。接着便是根据相似度进行输出的匹配，这里使用了加权匹配的方式，而权值就是query与key的相似度。\n键向量，查询向量，值向量维度: 键向量，查询向量，值向量维度一般比Embedding词向量低，在原论文中是输入单词Embedding维度的 1/8，（思考：为什么维度要变成Embedding词向量1/ 8，因为论文刚好建立了 8 个 自注意力机制，每个自注意力机制的 Q, K, V 维度就是（512 / 8), 最后再拼接这 8 个自注意了机制的结果，维度也能回到 512，然后再输入到下一个encoder模块中，保证每次输入的维度都是512).\nmulti-head attention\n在论文中用到的是Multi-Head Attention，它相当于 h 个不同的self-attention的集成（ensemble），在这里我们以 $ h=8$ 举例说明。Multi-Head Attention的输出分成3步：\n​\ta) 将数据 X 分别输入到上图所示的8个self-attention中，得到8个加权后的特征矩阵$ Z_i,i=1…8$ 。\n​\tb) 将8个 $Z_i$ 按列拼成一个大的特征矩阵；\n​\tc) 特征矩阵经过一层全连接后得到输出 Z 。\n可以看到 Multi-Head Attention 输出的矩阵 Z 与其输入的矩阵 X 的维度是一样的，这也是键向量，查询向量，值向量维度设置为64的原因。\n其中 $Z_i$ 拼接的方式为:\n（3）Add\u0026amp;Norm #\r在Multi-Head Attention之后的Add\u0026amp;Norm计算公式为：\n$$ LayerNorm(X+MultiHeadAttention(x)) $$\n在Feed Forward之后的Add\u0026amp;Norm计算公式为： $$ LayerNorm(X+FeedForward(x)) $$\nAdd就是残差连接，残差连接简单说就是：计算几层输出之后再把x加进来。残差网络可以有效的解决梯度消失的问题。\n而norm是对向量进行层归一化，具体计算方法为：\n（4）Feed Forward #\r这个全连接有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为: $$ FFN(x) = max(0,xW_1+b_1)W_2+b_2 $$\nFeed Forward 最终得到的输出矩阵的维度与 X 一致，都是(m×512)。\n（5）Encoder部分的最终结果： #\r在encoder部分要经过6次上述的encoder模块，最终的输出结果为：维度为(m×512)的矩阵。\n解码器 #\r（1）输入 #\r输入分为两部分，第一部分为Masked Multi-Head Attention之前，“I have a cat”通过词嵌入方法得到的向量与位置向量的和X作为输入。第二部分为Multi-Head Attention前输入的encoder的结果。\n（解码器第一子层的Masked Multi-Head Attention 结构当作Query，Value 和 Key来自编码器：利用上一层的训练来提取编码层的有效信息）\n（2）Masked Multi-Head Attention: #\r和Multi-Head Attention的过程基本相同，但是采用了Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息，这样符合翻译的过程。\n下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。具体的操作步骤如下：\n用 0 1 2 3 4 5 分别表示 “\u0026lt; Begin \u0026gt; I have a cat \u0026lt; end \u0026gt;”。\n第一步：Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 “\u0026lt; Begin \u0026gt; I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。\n第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵 X 计算得到 $Q, K, V$ 矩阵。然后计算 $Q 和K^T 的乘积 QK^T$ 。\n第三步：在得到 $QK^T$ 之后需要进行 Softmax，计算 attention score, 但我们在 Softmax 之前需要使用 Mask 矩阵遮挡住每一个单词之后的信息，遮挡操作如下：\n得到 $Mask QK^T$ 之后在 $Mask QK^T$ 上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0, 因为它们被遮盖住了，不需要关注。\n第四步：使用 $Mask QK^T$ 与矩阵 V 相乘，得到输出 Z，则单词 1 的输出向量 $z_1$ 包含单词 0和单词1的信息。\n第五步：通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$ ，然后和Encoder 类似，通过拼接多个输出 $Z_i$ ,然后计算得到第一个 Multi-Head Attention 的输出 Z，Z与输入 X 维度一样。\n（3）Add\u0026amp;Norm #\r添加残差连接，并进行归一化，同encoder中的公式一样。\n（4）Multi-Head Attention #\r计算方法和encoder中的一致，但是输入数据有差别，因此Q,K,V的计算也有差别，具体如下：输入的数据分为两部分，一部分是从Masked Multi-Head Attention经过归一化后的结果，一部分是encoder输出的结果Z，其中K, V 矩阵使用 Encoder 的编码信息矩阵 Z 进行计算(Encoder是使用输入X或前一个encoder的输出)，而 Q 使用Masked Multi-Head Attention经过归一化后的矩阵进行计算。\n（5） Feed Forward #\r同encoder中的公式一样。\n（6）Linear和Softmax #\r经过6层的decoder后，结果进入Linear和Softmax，线性层将前面decorder模块输出的向量映射成词典维数的向量，然后经过Softmax层： $$ f(z_j)=\\frac{e^{z_j}}{\\sum^{n}_{i=1}e^{z_j}} $$ 4. 为什么选择Self-Attention机制 #\r对于一个序列传导模型中编码器或解码器的隐藏层，Self-Attention可以在以下三个方面表现良好：\n每层计算的总复杂度 可并行的计算量（以所需的最小顺序操作的数量来衡量） 网络中长距离依赖之间的路径长度（影响学习这种依赖性能力的一个关键因素是前向和后向信号必须在网络中传播的路径长度。 输入和输出序列中任意位置组合之间的这些路径越短，学习远距离依赖性就越容易） 不同图层类型的最大路径长度、每层复杂度和最少顺序操作数。 n 为序列的长度，d 为表示的维度，k 为卷积的核的大小，r 为受限self-attention中邻域的大小。\n可以看出在计算复杂性上，Self-Attention与卷积和循环神经网络的区别在于序列长度和向量维度的大小关系。在并行性和信息集合性上表现优越。\n将self-attention限制在仅考虑大小为r 的邻域。 这会将最大路径长度增加到O(n ∕ r)，但是可以提高长序列任务的计算性能，这方面还有待继续研究，\n【attention存在的缺陷】attention在并行性和信息糅合性上表现优越，但是由于attention对整个模型的假设做的更少，所以相较于RNN和CNN需要更多的数据和更大的模型才能训练出来。\n5.训练 #\r在WMT2014数据集上进行训练（使用BPE：仅使用词根进行学习，忽略时态语态的变化，缩小字典大小，且双语共用一个字典，编码解码embedding共享权重），使用8台NIVIDA P100 GPU，基础模型训练了12小时（10万步），大型模型训练了3.5天（30万步）。\n使用adam训练器\n使用三个正则化方法：\ndropout 输出层和每个隐藏层之间都使用dropout方法，$P_{dropout} = 0.1$\nlabel soomthing 只要softmax输出的概率在 0.1 以上，就认为输出正确。（作者不太关心置信度）这让模型不易理解，因为模型学得更加不确定，但提高了准确性和BLEU得分。\n6. 结果 #\r（2018年）又快又好\n模型变形\n改变attention head的数量和attention key和value的维度，保持计算量不变， 虽然只有一个head attention比最佳设置差0.9 BLEU，但质量也随着head太多而下降。\n减小key的大小*$d_k$会有损模型质量。 这表明确定兼容性并不容易，并且比点积更复杂的兼容性函数可能更有用。 我们在行（C）和（D）中进一步观察到，如预期的那样，更大的模型更好，并且丢弃对避免过度拟合非常有帮助。 在行（E）中，我们用学习到的位置嵌入来替换我们的正弦位置编码，并观察到与基本模型几乎相同的结果。\n在英语成分句法分析方面做的也不错。\n7. 总结 #\r本文中，我们提出了Transformer，第一个完全基于注意力的序列转导模型，用多头注意力取代了编码器-解码器架构中最常用的循环层。\n对于翻译任务，Transformer可以比基于循环或卷积层的体系结构训练更快。 在WMT 2014英-德和WMT 2014英-法翻译任务中，我们取得了最好的结果。 在前面的任务中，我们最好的模型甚至超过以前报道过的所有的集成模型。\n我们对基于注意力的模型的未来感到兴奋。我们计划将Transformer扩展到除文本之外的涉及输入和输出模式的问题，并研究局部的、受限的注意力机制以有效处理大规模的输入和输出，如图像、音频和视频。 让生成具有更少的顺序性是我们的另一个研究目标。\n8.代码 #\r编码器与解码器的应用：\n编码器解码器都使用：机器翻译 只使用编码器：文本分类（如BERT） 只使用解码器：文本生成 8.1 输入 #\r## 句子的输入部分， sentences = [\u0026#39;ich mochte ein bier P\u0026#39;, \u0026#39;S i want a beer\u0026#39;, \u0026#39;i want a beer E\u0026#39;] 这三个句子分别是啥？在模型里如何处理的？\n我爱你：编码端的输入\nS I LOVE YOU ：（训练时）解码端的输入（训练时翻译的结果是知道的，所以直接将真实的翻译结果拿过来进行输入，再进行mask处理模拟真实应用按从左到右的序列自回归输入。真实应用的时候，解码端是按序列从左到右一个词一个词的翻译出来再放回解码器这样自回归的进行输入的）\n(teacher forcing：正常应用的时候decoder训练是不能并行的【必须按序列顺序来】，但是直接将真实的翻译结果拿过来，加上mask操作，就可以“假装”按序列处理的每一种中间状态，就可以直接扔到一个矩阵里统一处理了)\nI LOVE YOU E：（训练时）真实的翻译结果，解码器最后得到的结果与本句子进行误差损失的计算。\n‘P’、‘S’、‘E’分别都是啥？有啥用？\nS：“start”，表示开始 E：“END”，表示结束 P：“PAD”，填充字符，在多个Batch长度不一致的时候，使用字符\u0026quot;P\u0026quot;进行占位，将句子的长短变得相同。（为了能够使用矩阵进行批量的运算，P字符在运算中不应该产生效果） 规定句子长度 max_len ，句子长的部分截去，短的部分用PAD补齐\n8.2 词表参数设置 #\r# Transformer Parameters # Padding Should be Zero ## 构建词表：设置自然语言字符与数字数据的转换（字典） src_vocab = {\u0026#39;P\u0026#39;: 0, \u0026#39;ich\u0026#39;: 1, \u0026#39;mochte\u0026#39;: 2, \u0026#39;ein\u0026#39;: 3, \u0026#39;bier\u0026#39;: 4} src_vocab_size = len(src_vocab) tgt_vocab = {\u0026#39;P\u0026#39;: 0, \u0026#39;i\u0026#39;: 1, \u0026#39;want\u0026#39;: 2, \u0026#39;a\u0026#39;: 3, \u0026#39;beer\u0026#39;: 4, \u0026#39;S\u0026#39;: 5, \u0026#39;E\u0026#39;: 6} tgt_vocab_size = len(tgt_vocab) src_len = 5 # length of source 编码端输入长度 tgt_len = 5 # length of target 解码端输入长度 ## 模型参数 d_model = 512 # Embedding Size 每个字符转换为embedding的大小 d_ff = 2048 # FeedForward dimension 前馈神经网络线性层的维度大小 d_k = d_v = 64 # dimension of K(=Q), V n_layers = 6 # number of Encoder of Decoder Layer 叠几个编码器/解码器 n_heads = 8 # number of heads in Multi-Head Attention 分几个头 8.3 模型整体架构 #\r写模型的建议规则：\n从整体到局部（由整体到细节） 搞清楚数据的流动形状 ## 1. 从整体网路结构来看，分为三个部分：编码层，解码层，输出层 class Transformer(nn.Module): def __init__(self): super(Transformer, self).__init__() self.encoder = Encoder() ## 编码层 self.decoder = Decoder() ## 解码层 self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False) ## 输出部分，用于最后和标准结果计算损失 ## 输出层 d_model 是我们解码层每个token输出的维度大小，之后会做一个 tgt_vocab_size 大小的softmax ## 把d_model 的512个维度映射到解码端的词表大小，再做 softmax 进行概率比较（找最大的那个） ## 整体的架构函数（把encoder、decoder和输出部分串起来的函数） def forward(self, enc_inputs, dec_inputs): ## 这里有两个数据进行输入，一个是enc_inputs 形状为[batch_size, src_len]，主要是作为编码段的输入，一个dec_inputs，形状为[batch_size, tgt_len]，主要是作为解码端的输入 ##[m,n] = m * n ## enc_inputs作为输入 形状为[batch_size, src_len]，输出由自己的函数内部指定，想要什么指定输出什么，可以是全部tokens的输出，可以是特定每一层的输出；也可以是中间某些参数的输出； ## enc_outputs就是主要的输出，enc_self_attns这里没记错的是QK转置相乘之后softmax之后的矩阵值，代表的是每个单词和其他单词相关性； enc_outputs, enc_self_attns = self.encoder(enc_inputs) ## dec_outputs 是decoder主要输出，用于后续的linear映射； dec_self_attns类比于enc_self_attns 是查看每个单词对decoder中输入的其余单词的相关性；dec_enc_attns是decoder中每个单词对encoder中每个单词的相关性； dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs) ## dec_outputs做映射到词表大小 dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size] return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns 8.4 编码器 #\r## 2. Encoder 部分包含三个部分：词向量embedding，位置编码部分，注意力层及后续的前馈神经网络(layer) class Encoder(nn.Module): def __init__(self): super(Encoder, self).__init__() ##一样，先列整体框架，有几个部分 self.src_emb = nn.Embedding(src_vocab_size, d_model) ## 这个其实就是去定义生成一个矩阵，大小是 src_vocab_size * d_model self.pos_emb = PositionalEncoding(d_model) ## 位置编码情况，这里是固定的正余弦函数，也可以使用类似词向量的nn.Embedding获得一个可以更新学习的位置编码 self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)]) ## 使用ModuleList对多个encoder进行堆叠，因为后续的encoder并没有使用词向量和位置编码，所以抽离出来； ## 实现函数 def forward(self, enc_inputs): ## 这里我们的 enc_inputs 形状是： [batch_size x source_len] ##（有几个句子 * 句子长度） ## 下面这个代码通过src_emb，进行索引定位，enc_outputs输出形状是[batch_size, src_len, d_model] ##（形成一个三维矩阵，把这些字符索引（词表映射的那种），转换为一个个向量，合成一个矩阵） ##（句子个数*句子长度*向量维数） enc_outputs = self.src_emb(enc_inputs) ## 这里就是位置编码，把两者相加放入到了这个函数里面，从这里可以去看一下位置编码函数的实现；3. enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) ## transpose：转置（维度1，维度2） ##get_attn_pad_mask是为了得到句子中pad的位置信息，给到模型后面，在计算自注意力和交互注意力的时候去掉pad符号的影响，去看一下这个函数 4. enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) enc_self_attns = [] for layer in self.layers: ## 去看EncoderLayer 层函数 5. enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask) enc_self_attns.append(enc_self_attn) return enc_outputs, enc_self_attns 位置编码 #\r## 3. PositionalEncoding 代码实现 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len=5000): super(PositionalEncoding, self).__init__() ## 位置编码的实现其实很简单，直接对照着公式去敲代码就可以，下面这个代码只是其中一种实现方式； ## 从理解来讲，需要注意的就是偶数和奇数在公式上有一个共同部分，我们使用log函数把次方拿下来，方便计算； ## pos代表的是单词在句子中的索引，这点需要注意；比如max_len是128个，那么索引就是从0，1，2，...,127 ##假设我的demodel是512，2i那个符号中i从0取到了255，那么2i对应取值就是0,2,4...510 self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term)## 这里需要注意的是pe[:, 0::2]这个用法，就是从0开始到最后面，步长为2，其实代表的就是偶数位置，注意这里是按照对应位置元素乘起来，放到对应位置 pe[:, 1::2] = torch.cos(position * div_term)##这里需要注意的是pe[:, 1::2]这个用法，就是从1开始到最后面，步长为2，其实代表的就是奇数位置 ## 上面代码获取之后得到的pe:[max_len*d_model] ## 下面这个代码之后，我们得到的pe形状是：[max_len*1*d_model] pe = pe.unsqueeze(0).transpose(0, 1) self.register_buffer(\u0026#39;pe\u0026#39;, pe) ## 定义一个缓冲区，其实简单理解为这个参数不更新就可以 def forward(self, x): \u0026#34;\u0026#34;\u0026#34; x: [seq_len, batch_size, d_model] \u0026#34;\u0026#34;\u0026#34; x = x + self.pe[:x.size(0), :] return self.dropout(x) ## 词向量+位置编码 形成输入 共有部分算出来就是代码里的 div_term\ntranspose（0，1）二维转置\n消除PAD影响 #\r符号矩阵，告诉后面这个地方影响要消除\n## 4. get_attn_pad_mask ## 比如说，我现在的句子长度是5，在后面注意力机制的部分，我们在计算出来QK转置除以根号之后，softmax之前，我们得到的形状 ## len_input * len*input 代表每个单词对其余包含自己的单词的影响力 ## 所以这里我需要有一个同等大小形状的矩阵，告诉我哪个位置是PAD部分，之后在计算计算softmax之前会把这里置为无穷大； ## 一定需要注意的是这里得到的矩阵形状是batch_size x len_q x len_k，我们是对k中的pad符号进行标识，并没有对k中的做标识，因为没必要 ## seq_q 和 seq_k 不一定一致，在交互注意力，q来自解码端，k来自编码端，所以告诉模型编码这边pad符号信息就可以，解码端的pad信息在交互注意力层是没有用到的； def get_attn_pad_mask(seq_q, seq_k): batch_size, len_q = seq_q.size() batch_size, len_k = seq_k.size() # eq(zero) is PAD token pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # batch_size x 1 x len_k, one is masking return pad_attn_mask.expand(batch_size, len_q, len_k) # batch_size x len_q x len_k unsqueeze()\nexpand()：拓展维度\n有两点需要注意，无论是 expand() 还是 expand_as()：\n只能在第0维扩展一个维数，比如原来是是（1,3,4）==》（2,1,3,4），而在其他维度扩展不可以（1,3,4）==》（1,2,3,4）【错误】 如果不增加维数，只是增加维度，要增加的原维度必须是1才可以在该维度增加维度，其他值均不可以 expand()函数括号中的输入参数为指定经过维度尺寸扩展后的张量的size。 expand（）函数只能将size=1的维度扩展到更大的尺寸，如果扩展其他size（）的维度会报错。 本例子中\n将【1，1，5】expend为【1，5，5】，即把第二个维度扩大5倍（把原来的那个第二维包含的第三维的元素复制5份）\nimport torch a = torch.Tensor([[1], [2], [3],[4]]) # 未使用expand（）函数前的a print(\u0026#39;a.size: \u0026#39;, a.size()) print(\u0026#39;a: \u0026#39;, a) b = a.expand(4, 2) # 使用expand（）函数后的输出 print(\u0026#39;a.size: \u0026#39;, a.size()) print(\u0026#39;a: \u0026#39;, a) print(\u0026#39;b.size: \u0026#39;, b.size()) print(\u0026#39;b: \u0026#39;, b) 输出：\na.size: torch([4, 1]) a: 1 2 3 4 [torch.FloatTensor of size 4x1]\nb.size: torch.Size([4, 2]) b: 1 1 2 2 3 3 4 4 [torch.FloatTensor of size 4x2]\n","date":"2023年3月29日","externalUrl":null,"permalink":"/posts/transformer%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","section":"文章","summary":"","title":"（补档）Transformer阅读笔记","type":"posts"},{"content":"","date":"2023年3月29日","externalUrl":null,"permalink":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"Tags","summary":"","title":"机器学习","type":"tags"},{"content":"","date":"2023年3月29日","externalUrl":null,"permalink":"/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/","section":"Tags","summary":"","title":"自然语言基础","type":"tags"},{"content":"尝试训练某古早nlp相关模型，在pytorch下需要Allen-nlp0.8.4版本的环境，踩了好多坑才配置成功（主要是pytorch安装太离谱了）于是记录一下\nconda 创建虚拟环境 #\r参考：https://zhuanlan.zhihu.com/p/376030379\nubuntu下，进入conda base 环境\nsource activate windows下，打开 Anaconda Prompt 也可\n创建环境\n声明名称为 my_env_name 版本为py3.8\nconda create -n my_env_name python=3.8 激活环境\nconda activate my_env_name 回到base环境\nconda deactivate 删除环境\nconda remove -n my_env_name --all 查看现有虚拟环境\nconda info -e 安装Pytorch #\r需要手动下载文件，本地安装！！！\n需要手动下载文件，本地安装！！！\n需要手动下载文件，本地安装！！！\n直接使用 conda 或 pip 下载安装的后果：①下载到CPU版本、②安装后检测不到显卡、③安装后执行训练段错误……\n（大部分是由于 Pytorch 和 CUDA 版本不匹配造成的）\n官网地址：https://pytorch.org/get-started/locally/\n直接进这个也行：\rdownload.pytorch.org/whl/torch/\n输入\nnvcc -V 查看CUDA版本\n可以看到本地CUDA版本为 11.6\n下载老版本的pytorch可以进入官网下载界面-选择pip下载-复制命令最后的链接\n选择torch进入历史版本的下载页面\nCtrl+F 搜索输入你的CUDA版本，如 CUDA11.6 就查找cu116， CUDA11.3 就查找cu113，可以看到该版本下可以下载的pytorch版本\n选择对应的pytorch和操作系统下载 .whl 文件\n使用pip安装，进入下载路径，执行：\npip install 下载的文件名.whl 测试torcgh是否能够正常运行\n命令行输入\npython 进入python环境后，逐行运行\nimport torch print(torch.__version__) print(torch.cuda.is_available()) print(torch.cuda.device_count()) print(torch.zeros(1).cuda()) （这里torch版本是+cu113是因为换了台机器） print(torch.cuda.is_available()) 显示为True , 最后对torch的操作可以正常执行就一般没问题\n参考：https://blog.csdn.net/weixin_41529093/article/details/109399393\n24/09/28 补充： #\r后来在实际操作中发现，其实可以直接参考 nvidia-smi 中的最高支持CUDA版本进行torch安装，即：\r命令行运行nvidia-smi，找到最高支持CUDA版本\n再去下载官网找安装包时，只要低于cu122的，满足系统和python版本的torch安装包都可以下载进行安装，安装步骤同上。因为linux系统会自动按照安装包要求在虚拟环境中自动下载安装相应的cuda工具包。\n安装allen-nlp #\rpip install allennlp -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com 指定版本\npip install allennlp==0.8.4 训练时问题 #\r代码年代久远，训练时使用的函数可能新版本不再支持，可能会报以下错误，将相应工具包降回老版本即可\nArrayField.empty_field: return type None is not a allennlp.data.fields.field.Field.\npip install overrides==4.1.2 ModuleNotFoundError: No module named \u0026lsquo;sklearn.utils.linear_assignment_\u0026rsquo;\npip3 install scikit-learn==0.19.2 ","date":"2023年1月4日","externalUrl":null,"permalink":"/posts/%E6%9F%90nlp%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEpytorch%E5%AE%89%E8%A3%85/","section":"文章","summary":"尝试训练某古早nlp相关模型，在pytorch下需要Allen-nlp0.8.4版本的环境，踩了好多坑才配置成功（主要是pytorch安装太离谱了）于是记录一下。","title":"某nlp模型训练实验环境配置（Pytorch安装）","type":"posts"},{"content":"\rNeural Machine Reading Comprehension: Methods and Trends #\r神经机器阅读理解：方法与趋势\n0.摘要 #\r机器阅读理解（MRC）需要机器根据给定的上下文回答问题，在过去几年中，随着各种深度学习技术的结合，它越来越受到关注。本文是对现有方法和最新趋势的全面调查（2019）。\n本文以以下三个方面来对MRC进行阐述：\n（1）典型的MRC任务：它们的定义、差异和代表性数据集；\n（2） 神经MRC的一般架构：每个模块的主要模块和常用方法；\n（3）新趋势：神经MRC的一些新兴领域以及相应的挑战。\n1. 介绍 #\r早期的MRC系统可以追溯到20世纪70年代。然而，由于其规模小、领域特殊，该系统无法得到广泛应用。 1999年起，MRC任务的方法主要是基于规则或基于机器学习的，但是具有以下缺点。 它们主要基于手工制作的规则或特征，需要大量人力。 这些系统无法进行泛化，并且由于大量文章的大规模数据集，它们的性能可能会下降。 一些传统方法不仅忽略了长期依赖，而且无法提取上下文信息。 2015年以来，基于深度学习的MRC，也称为神经机器阅读理解出现，在捕获上下文信息方面显示出其优越性，并显著优于传统系统。并且各种大型基准数据集的出现，如CNN/Daily Mail、（SQuAD）和MS MARCO，使得用深度神经架构解决MRC任务成为可能，并为广泛评估MRC系统的性能提供了试验台。 2015-2018年的研究方向和发文趋势\n本文结构：\n首先，将常见的MRC任务分为四种类型：完形填空、多项选择、跨度提取和自由回答。我们进一步扩展了这一分类，为每种类型提供了一个正式的定义，并在不同的维度上对这些任务进行了比较（第2节）。 其次，介绍了神经MRC系统的总体架构，包括四个模块：嵌入，特征提取、上下文-问题交互和答案预测。每个模块中使用的流行深度学习技术也有详细说明（第3节）。 第三，根据不同的任务描述了一些有代表性的数据集和评估指标（第4节）。 第四，一些新的趋势，如基于知识的MRC、带有无法回答的问题的MRC，多通道MRC和会话式MRC，通过找出它们的挑战并描述现有的方法和局限性来揭示（第5节）。 最后，讨论了几个开放的问题，希望能为未来可能的研究方向提供启示（第6节）。 2.任务 #\rMRC定义 #\r机器阅读理解（MRC）是文本问答（QA）的一项基本任务，在该任务中，每个问题都被赋予了相关的上下文，从中可以推断出答案。MRC的目标是从给定的上下文中提取正确的答案，甚至根据上下文生成更复杂的答案。\n形式化定义：\n给定上下文C和问题Q，机器阅读理解任务要求模型通过学习函数F来给出问题Q的正确答案A，使得 $$ A=F（C，Q）。$$\n2.1 完形填空 #\r在完形填空测试中，通过从文章中删除一些单词或实体来生成问题。为了回答问题，要求机器在空白处填写缺失的项目。有些任务提供候选答案，但这是可选的。完形填空测试增加了阅读障碍，需要理解上下文和词汇使用，对机器阅读理解具有挑战性。\n完形填空的特征：\n（i） 答案A是给定上下文C中的单词或实体；\n（ii）通过从给定上下文C中移除单词或实体来生成问题Q，使得Q＝C− A\n形式化定义：\n给定上下文C，单词或实体 $A(A∈C)$ 完形填空测试要求模型通过学习函数F，用正确的单词或实体A填空，使 $$ A=F(C− {A} )$$。\n数据集：\nCNN/Daily Mail、CBT、LAMNADA……\n2.2 多项选择 #\r要求根据所提供的上下文从候选选项中选择问题的正确答案。与完形填空测试相比，选择题的答案不限于上下文中的单词或实体，因此答案形式更灵活，但这项任务需要提供候选答案。\n选择题的特点是给出了一个候选答案列表 $ A={A_1，A_2，···，A_n} $，可以作为预测答案的辅助信息。\n形式化定义：\n多项选择给定上下文C、问题Q和候选答案列表 $A={A_1，A_2，··，A_n}$，多项选择任务是从a（Ai∈ A） 通过学习函数F，使得 $A_i＝F(C，Q，A)$\n数据集：\nMCTest、RACE……\n2.3 区间提取 #\r跨度提取任务可以克服单个实体无法完全回答问题以及缺少候选答案的问题。给定上下文和问题，此任务要求机器从相应上下文中提取一段文本作为答案。\n跨度提取的特点是答案A需要是给定上下文C的连续子序列。\n形式化定义：\n给定由n个标记组成的上下文C，即 $C={t_1，t_2，···，t_n}$和问题Q，跨度提取任务需要提取连续子序列 $A={t_i，t_{i+1}，··，t_{i+k}}(1≤ i≤ i+k≤ n)$ 通过学习函数F使得 $A＝F(C，Q)$。\n数据集：\nSQuAD、News QA、Trivia QA、DuoRC……\n2.4 自由作答 #\r机器需要对多段文本进行推理并总结证据。在这四项任务中是最复杂的，因为它的答题形式没有限制，而且更适合实际应用场景。\n特点：侧重于使用自由形式的自然语言来更好地回答问题\n形式化定义：\n给定上下文C和问题Q，答案可能并不来源于原文本，即$A⊆ C orA \\not⊆ C$ ，该任务要求通过学习函数F来预测正确答案A，使得$ A＝F(C，Q) $。\n数据集：\nbAbl、MS MARC、search QA、Narrative QA……\n2.5 不同任务之间的比较 #\r使用construction、understanding、 flexibility、evaluation 和 application五个维度描述不同任务的优越性和局限性：\nconstruction：这个维度衡量任务构造数据集是否容易。越容易，分数越高。\nunderstanding：这个维度评估任务对机器理解能力的测试程度。如果一项任务需要更多的理解和推理，分数就更高。\nflexibility：答案形式的灵活性可以衡量任务的质量。答案越灵活，灵活性得分越高。\nevaluation：评估是MRC任务的必要组成部分。一项任务是否容易评估也决定了它的质量。容易评估的任务在这个维度上得分很高。\napplication：一个好的任务应该接近真实的应用。因此，如果一项任务能够很容易地应用于现实世界，那么这一维度的得分就很高。\n如图所示：\n对于完形填空：构建数据集和评估完形填空是最容易的。然而，由于答案形式在原始上下文中仅限于单个单词或名称实体，完形填空无法很好地测试机器理解能力，也不符合实际应用。\n对于多项选择：其对每个问题提供候选答案，这样即使答案在原始上下文中不受限制，也可以很容易地进行评估。为这项任务构建数据集并不困难，因为语言考试中的多项选择题可以很容易地使用。然而，候选答案的设置会导致合成数据集与实际应用之间的差距。\n对于区间抽取：可以很容易地构建和评估数据集。此外，它们可以有效测试机器对文本的理解。所有这些优点都有助于对这些任务进行大量研究。跨度提取的缺点是答案被限制在原始上下文的子序列中，这与真实世界仍然有点距离。\n对于自由作答：自由回答任务在理解力、灵活性和应用性方面表现出优势，这些方面最接近实际应用。然而由于答案形式的灵活性，构建数据集有些困难，如何有效评估这些任务的性能仍然是一个挑战。\n","date":"2022年11月7日","externalUrl":null,"permalink":"/posts/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3_%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%80/","section":"文章","summary":"","title":"Neural Machine Reading Comprehension: Methods and Trends 阅读笔记（一）","type":"posts"},{"content":"\r题目链接 #\r题目大意 #\r给定 a,b,n ，算这个S $$ S = (a + \\sqrt b)^n + (a - \\sqrt b)^n $$\n解题思路 #\r其实跟\rhdu-4565是一样的题（思路完全通用，甚至AC代码都差不多）\n具体的思路就是一个求递推式，再用矩阵快速幂求解递推式的方法。\n而推递推式的方法，则是在我们已知（或大胆假设）计算可以递推并限制在一定项数之内时，使用类似逆用数学归纳法的方法去构建递推式。\n以本题为例，观察式子，由二项式定理可以知道，前后两式只有在$\\sqrt{b}$为奇数次方时互为相反数，其他情况相等，这也是最后答案一定是整数的原因。\n故，我们可以设 $$ (a + \\sqrt b)^n = X_n + \\sqrt{b}Y_n $$ $$ (a - \\sqrt b)^n = X_n - \\sqrt{b}Y_n $$\n则我们可以得到：\n$$ (a+\\sqrt{b})^n = (a+\\sqrt{b})^{n-1} * (a+\\sqrt{b}) $$ $$ =(X_{n-1} + \\sqrt{b}Y_{n-1})*(a+\\sqrt{b}) $$ $$ = aX_{n-1} + bY_{n-1} + (X_{n-1}+aY_{n-1})\\sqrt{b} ${(a-\\sqrt{b})^n}$同理，待定系数，我们可以得到以下递推式（加减两个式子推出来是一样的）： $$\n$$ X_n = aX_{n-1} + bY_{n-1} $$ $$ Y_n = X_{n-1} + aY_{n-1} $$\n那我们又轻易知 ${X_1 = a,Y_1 = 1}$，就可以用矩阵快速幂求解了，由于加减两式${Y_n}$ 符号相反，最后答案即为 ${2 * X_n}$ 。\nAC代码 #\r#include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #pragma warning(disable:4996) #define inr register int #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl using namespace std; typedef long long ll; const double pi = acos(-1.0); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 100007;//1e5+7 const ll mod = 998244353;//1e9+7 struct maritx { ll m[5][5]; }; maritx mul(maritx a,maritx b,int n = 2) { maritx res; for (int i = 1; i \u0026lt;= n; i++) { for (int j = 1; j \u0026lt;= n; j++) { res.m[i][j] = 0; for (int k = 1; k \u0026lt;= n; k++) res.m[i][j] = (res.m[i][j] + a.m[i][k] * b.m[k][j]) % mod; } } return res; } maritx ksm(maritx x, ll y,int n = 2) { maritx ans; memset(ans.m, 0, sizeof(ans.m)); ans.m[1][1] = ans.m[2][2] = 1; while (y \u0026gt; 0) { if (y \u0026amp; 1) { ans = mul(ans , x); } x = mul(x, x); y \u0026gt;\u0026gt;= 1; } return ans; } int main() { ios; ll a, b, n; cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b \u0026gt;\u0026gt; n; if (n == 1) { cout \u0026lt;\u0026lt; 2 * a \u0026lt;\u0026lt; endl; return 0; } maritx js, cs; js.m[1][1] = a; js.m[1][2] = b; js.m[2][1] = 1ll; js.m[2][2] = a; js = ksm(js, n - 1ll); cs.m[1][1] = a; cs.m[2][1] = 1ll; cs.m[1][2] = 0ll; cs.m[2][2] = 0ll; cs = mul(js, cs); ll ans = 2ll * cs.m[1][1] % mod; cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; return 0; } 参考博客 #\rhttps://blog.csdn.net/wust_cyl/article/details/77662720\n","date":"2021年10月7日","externalUrl":null,"permalink":"/posts/%E8%A1%A5%E6%A1%A3eoj4329/","section":"文章","summary":"","title":"EOJ4329 - EOJ Monthly 2021.9 A. Amazing Discovery（公式推导）（矩阵快速幂）","type":"posts"},{"content":"","date":"2021年10月7日","externalUrl":null,"permalink":"/tags/%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"算法","type":"tags"},{"content":"\r势能线段树（吉司机线段树）专题 #\r势能线段树在近期训练时遇到了好几次，但是由于本人太懒一直没补完，结果ICPC网络赛还真就出了一道势能线段树Orz……结果当然是没做出来……痛定思痛，这回把之前欠的一块儿补了。\n简要介绍 #\r​\t我们知道，传统的支持区间修改的线段树是通过lazy标记来规避掉频繁大规模的单点修改来实现对时间开销的节省的。那么对于一种区间操作，其能使用lazy标记需要满足两个条件：\n区间节点的值可以通过对当前结点lazy标记的计算来更新 多次不同的lazy标记可以实现就地的快速合并 ​\t比如经典的区间加和区间乘修改，区间求和查询，当前结点的值可以通过加上lazy乘区间长度和乘上lazy的值来进行轻易的修改，lazy标记的值也可以通过简单的相加或相乘来更新。\n​\t但是对于像区间开根号、区间位运算这样的区间操作来说，其对每个结点的修改量是在一定程度上是由叶结点本身现有的值来决定的，那么就很难实现lazy的合并和对区间值的直接更新，好像只有对所有叶结点进行单点修改这一种办法，而这种方法在时间开销上是绝对不允许的。\n​\t但是再仔细观察这其中的某些操作，区间开根号，每个点顶多被开log次，当结点的值被开到1时，以后的操作就都不会对结点的值产生影响了。区间与值运算也是同理，与运算不会使结点值中二进制1的个数增加，而当结点的值变为0时，与值操作也就不能对值产生影响了。我们发现这些操作对每个结点的操作次数都是有一个隐含的“上限”的，就像有一个固定的“势能“，只要超过了这个上限值，相应的操作便会“退化”失效，也就是势能为0的情况。而当势能为0的结点连成区间时，我们便可以一口气规避掉在这个区间上的所有操作。而一般这种势能的下降还是非常迅速的，这也是势能线段树节省时间的原因。\n​\t所以，我们可以这样构建和操作这个线段树：\n在每个线段树结点加入一个”势能函数“，来记录和维护当前区间结点的势能情况。 对于每次的区间修改，若当前区间内所有结点的势能皆已为零，直接退出递归不再修改 若当前区间内还存在势能不为零的结点，则继续向下递归，暴力修改要求区间内每一个势能不为零的结点 ​\t至于具体的时间复杂度…据说每次修改不会超过O(log2n)……具体的证明嘛……\n题目归纳 #\r2021CCPC 黑龙江省赛 A And RMQ （\rGym - 103107A） #\r题目大意 #\r给定一组序列，定义三种操作：\nAND l r v ：将从l到r的区间所有的结点和v进行与运算\nUPD x v：单点修改，将x位置的值修改为v\nQUE l r：询问区间l到r的区间最大值\n进行最大400000次操作。\n解题思路 #\r几乎是势能线段树的模板题，可以看出，如果在区间所与的v的值位权为0的位置上，待修改区间所有值在这些位置的位权皆为0，那么区间与值操作对于该区间就是无效的。那么可以使用或运算计算区间值位权为0的交集作为势能值的记录，每次区间操作先查询v值位权为0的位置在相应区间上是否存在1，若存在，接着向下修改，若不存在，直接退出修改。\nAC代码 #\r#include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #include\u0026lt;bitset\u0026gt; #pragma warning(disable:4996) #pragma GCC optimize (2) #pragma G++ optimize (2) #define inr register int #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl using namespace std; typedef long long long long; const double pi = acos(-1.0); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int 1000007 = 400007;//1e5+7 int read() { int res = 0, ch, flag = 0; if ((ch = getchar()) == \u0026#39;-\u0026#39;) flag = 1; else if (ch \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; ch \u0026lt;= \u0026#39;9\u0026#39;) res = ch - \u0026#39;0\u0026#39;; while ((ch = getchar()) \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; ch \u0026lt;= \u0026#39;9\u0026#39;) res = res * 10 + ch - \u0026#39;0\u0026#39;; return flag ? -res : res; } #define lson (o\u0026lt;\u0026lt;1) #define rson (o\u0026lt;\u0026lt;1|1) int arr[1000007]; struct node { int mx; int tag; }tree[1000007 \u0026lt;\u0026lt; 2]; inline void pushup(int o) { tree[o].mx = max(tree[lson].mx, tree[rson].mx); tree[o].tag = tree[lson].tag | tree[rson].tag;//收集区间内所有位权1，作为势能函数 } void build(int o, int l, int r) { if (l == r) { tree[o] = { arr[l] , arr[l] }; return; } int mid = (l + r) \u0026gt;\u0026gt; 1; build(lson, l, mid); build(rson, mid + 1, r); pushup(o); } void modify(int o, int l, int r, int ml, int mr, int x) { if (!((~x) \u0026amp; tree[o].tag)) { return; } if (l == r) { tree[o].mx \u0026amp;= x; tree[o].tag \u0026amp;= x; return; } int mid = (l + r) \u0026gt;\u0026gt; 1; if (ml \u0026lt;= mid) { modify(lson, l, mid, ml, mr, x); } if (mid + 1 \u0026lt;= mr) { modify(rson, mid + 1, r, ml, mr, x); } pushup(o); } void update(int o, int l, int r, int p, int x) { if (l == r) { tree[o] = { x,x }; return; } int mid = (l + r) \u0026gt;\u0026gt; 1; if (p \u0026lt;= mid) { update(lson, l, mid, p, x); } else { update(rson, mid + 1, r, p, x); } pushup(o); } int query(int o, int l, int r, int ql, int qr) { if (ql \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= qr) { return tree[o].mx; } int mid = (l + r) \u0026gt;\u0026gt; 1, res = 0; if (ql \u0026lt;= mid) { res = max(res, query(lson, l, mid, ql, qr)); } if (mid + 1 \u0026lt;= qr) { res = max(res, query(rson, mid + 1, r, ql, qr)); } return res; } char st[17]; int main() { int n = read(), m = read(); for (int i = 1; i \u0026lt;= n; i++) { arr[i] = read(); } build(1, 1, n); int opl, opr, opx; while (m--) { scanf(\u0026#34;%s%d\u0026#34;, st, \u0026amp;opl); if (st[0] == \u0026#39;A\u0026#39;) { opr = read(); opx = read(); modify(1, 1, n, opl, opr, opx); } else if (st[0] == \u0026#39;U\u0026#39;) { opx = read(); update(1, 1, n, opl, opx); } else { opr = read(); printf(\u0026#34;%d\\n\u0026#34;, query(1, 1, n, opl, opr)); } } return 0; } 2021杭电中超（8）D Counting Stars （\rHDU 7059） #\r题目大意 #\r同样是求区间和，给定两种区间修改操作\n询问区间和 区间l到r所有值加上其highbit 区间l到r所有值减去其lowbit 解题思路 #\r​\t与黑龙江省赛大同小异，可以看出对于每个值其位权1的个数是一定的，而每进行一次操作3其数量会减一，操作2不会增加其数量。于是使用cnt来记录当前区间最多的位权1数量，若cnt为0，则不需要再修改。注意将每个数的hibit和剩下值分开存储，这样操作2就变成纯粹的区间乘操作，更好维护。\nAC代码 #\r#include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #pragma GCC optimize(2) #pragma GCC optimize(\u0026#34;Ofast\u0026#34;,\u0026#34;inline\u0026#34;,\u0026#34;-ffast-math\u0026#34;) #pragma GCC optimize(3) #pragma GCC optimize(3,\u0026#34;Ofast\u0026#34;,\u0026#34;inline\u0026#34;) #pragma G++ optimize(2) #pragma G++ optimize(\u0026#34;Ofast\u0026#34;,\u0026#34;inline\u0026#34;,\u0026#34;-ffast-math\u0026#34;) #pragma G++ optimize(3) #pragma G++ optimize(3,\u0026#34;Ofast\u0026#34;,\u0026#34;inline\u0026#34;) #pragma warning(disable:4996) #define inr register int #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl using namespace std; typedef long long ll; const double pi = acos(-1.0); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 1000007;//1e5+7 const ll mod = 998244353;//1e9+7 #define lson (o\u0026lt;\u0026lt;1) #define rson (o\u0026lt;\u0026lt;1|1) ll arr[maxn]; ll m2[maxn]; inline ll lowbit(ll x) { return x \u0026amp; (-x); } inline ll hbit(ll x) { for (inr i = 30; i \u0026gt;= 0; i--) { if (x \u0026gt;\u0026gt; i \u0026amp; 1) { return 1 \u0026lt;\u0026lt; i; } } } inline void init() { m2[0] = 1; for (inr i = 1; i \u0026lt; maxn; i++) { m2[i] = (m2[i - 1] * 2) % mod; } } struct node { ll hb, lb, cnt, lazy; }tree[maxn \u0026lt;\u0026lt; 2]; inline void pushup(int o) { tree[o].lb = (tree[lson].lb + tree[rson].lb) % mod; tree[o].hb = (tree[lson].hb + tree[rson].hb) % mod; tree[o].cnt = max(tree[lson].cnt, tree[rson].cnt); } inline void pushdown(int o) { if (tree[o].lazy) { tree[lson].lazy += tree[o].lazy; tree[rson].lazy += tree[o].lazy; tree[lson].hb = (tree[lson].hb * m2[tree[o].lazy]) % mod; tree[rson].hb = (tree[rson].hb * m2[tree[o].lazy]) % mod;//hb??lb tree[o].lazy = 0; } } inline void build(int o, int l, int r) { tree[o] = { 0,0,0,0 };//??????? if (l == r) { tree[o].hb = hbit(arr[l]); tree[o].lb = arr[l] - tree[o].hb; tree[o].cnt = __builtin_popcount(arr[l]); return; } int mid = (l + r) \u0026gt;\u0026gt; 1; build(lson, l, mid); build(rson, mid + 1, r); pushup(o); } inline void modify(int o, int l, int r, int ml, int mr) { if (!tree[o].cnt) { return; } if (l == r) { if (tree[o].cnt \u0026gt; 1) { tree[o].lb -= lowbit(tree[o].lb); tree[o].cnt--;; } else { tree[o].hb = tree[o].cnt = 0; } return; } pushdown(o); int mid = (l + r) \u0026gt;\u0026gt; 1; if (ml \u0026lt;= mid) { modify(lson, l, mid, ml, mr); } if (mid + 1 \u0026lt;= mr) { modify(rson, mid + 1, r, ml, mr); } pushup(o); } inline void update(int o, int l, int r, int ul, int ur) { if (!tree[o].cnt) { return; } if (ul \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= ur) { tree[o].hb = tree[o].hb * 2 % mod; tree[o].lazy += 1; return; } pushdown(o); int mid = (l + r) \u0026gt;\u0026gt; 1; if (ul \u0026lt;= mid) { update(lson, l, mid, ul, ur); } if (mid + 1 \u0026lt;= ur) { update(rson, mid + 1, r, ul, ur); } pushup(o); } inline ll query(int o, int l, int r, int ql, int qr) { if (!tree[o].cnt) { return 0ll; } if (ql \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= qr) { return tree[o].hb + tree[o].lb; } pushdown(o); int mid = (l + r) \u0026gt;\u0026gt; 1; ll res = 0; if (ql \u0026lt;= mid) { res = (res + query(lson, l, mid, ql, qr)) % mod; } if (mid + 1 \u0026lt;= qr) {//qr??r res = (res + query(rson, mid + 1, r, ql, qr)) % mod; } return res; } int main() { int T, n, m; init(); scanf(\u0026#34;%d\u0026#34;, \u0026amp;T); while (T--) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 1; i \u0026lt;= n; i++) { scanf(\u0026#34;%lld\u0026#34;, arr + i); } scanf(\u0026#34;%d\u0026#34;, \u0026amp;m); build(1, 1, n); int opt, opl, opr; while (m--) { scanf(\u0026#34;%d%d%d\u0026#34;, \u0026amp;opt, \u0026amp;opl, \u0026amp;opr); if (opt == 1) { printf(\u0026#34;%lld\\n\u0026#34;, query(1, 1, n, opl, opr)); } else if (opt == 2) { modify(1, 1, n, opl, opr); } else { update(1, 1, n, opl, opr); } } } return 0; } 2021 ICPC 网络赛 第二场 L Euler Function #\r题目大意 #\r给定序列，定义两种操作\n0 l r w ：区间l到r的值都乘以w\n1 l r ：查询区间l到r的欧拉函数值的和 mod 998244353\n注意序列初始值和w值皆不大于100\n解题思路 #\r注意欧拉函数的性质： 若i mod p=0,其中p为质数,则 $φ(i * p) = p * φ( i )$ 否则 $φ( i * p ) = ( p - 1) * φ( i )$ 。\n因为x和w的值都不大于100，所以其实其都能分解为100以内的质数，根据上面描述的性质，我们将一次区间乘w的更新分解为对其质因数的分别更新，这样，如果某质数为该区间内共同的因数，则对当前区间欧拉函数的修改就变为简单的区间乘问题（直接乘p就行）。否则，继续向下递归修改。100以内的素数只有不到30个，可以使用bitset进行状态压缩处理，这样通过二进制位上的运算就可以很好的反应结点势能情况。\nAC代码 #\r#include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;bitset\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #pragma warning(disable:4996) #define inr register int #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define lson (o\u0026lt;\u0026lt;1) #define rson (o\u0026lt;\u0026lt;1|1) using namespace std; typedef long long ll; const double pi = acos(-1.0); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 100007;//1e5+7 const int maxm = 107; const ll mod = 998244353;//1e9+7 //势能线段树的核心思想：虽然有些操作看起来不能区间修改，但是其实修改的操作是有上界的 //即某些操作在执行了一定次数后便会退化，比如开根、取最小值等操作。 //那么思路就是在线段树中记录这种势能的变化，当操作退化时将单点操作转为区间操作，即可大大节省时间 int phi[maxm], prime[maxm];//前一百个数的欧拉函数，前一百中的素数 bool del[maxm];//判断素数 int cnt[maxm][30]; bitset\u0026lt;30\u0026gt;st[maxm]; ll ksm(ll x, ll y) { ll ans = 1; x = x % mod; while (y \u0026gt; 0) { if (y \u0026amp; 1) ans = (ans * x) % mod; y \u0026gt;\u0026gt;= 1; x = (x * x) % mod; } return ans; } void euler()//欧拉筛，同时求欧拉函数 { for (int i = 2; i \u0026lt; maxm; i++) { if (!del[i]) { prime[++prime[0]] = i; phi[i] = i - 1; } for (int j = 1; j \u0026lt;= prime[0] \u0026amp;\u0026amp; ll(prime[j]) * i \u0026lt; maxm; j++) { int tp = prime[j] * i; del[tp] = 1; if (i % prime[j] == 0) { phi[tp] = phi[i] * prime[j]; break; } phi[tp] = phi[i] * (prime[j] - 1); } } phi[1] = 1; } ll w[maxn]; struct node { ll sum, lazy;//求和值，区间乘lazy bitset\u0026lt;30\u0026gt;tag;//可以看作一种势能的记录？ }tree[maxn \u0026lt;\u0026lt; 2]; void pushup(int o) { tree[o].sum = (tree[lson].sum + tree[rson].sum) % mod;//朴素的计算 tree[o].tag = tree[lson].tag \u0026amp; tree[rson].tag;//代表区间全部覆盖的质因数 } void pushdown(int o)//区间乘正常操作 { tree[lson].lazy = tree[lson].lazy * tree[o].lazy % mod; tree[rson].lazy = tree[rson].lazy * tree[o].lazy % mod; tree[lson].sum = tree[lson].sum * tree[o].lazy % mod; tree[rson].sum = tree[rson].sum * tree[o].lazy % mod; tree[o].lazy = 1; } void build(int o, int l, int r) { if (l == r) { tree[o] = { phi[w[l]],1,st[w[l]] };//存入对应的欧拉函数、质因数状态，lazy初始化 return; } tree[o] = { 0,1 };//非叶节点的tag等待pushup int mid = (l + r) \u0026gt;\u0026gt; 1; build(lson, l, mid); build(rson, mid + 1, r); pushup(o); } void modify(int o, int l, int r, int ml, int mr, pair\u0026lt;int, int\u0026gt;d)//对数的修改一定是以素数的幂次进行的 { if (ml \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= mr \u0026amp;\u0026amp; tree[o].tag[d.first]) {//如果当前节点在修改区间内，该素数是该区间共有的因数 tree[o].lazy = tree[o].lazy * ksm(prime[d.first], d.second) % mod;//那么就是区间乘，直接乘就行 tree[o].sum = tree[o].sum * ksm(prime[d.first], d.second) % mod; } else if (l == r) { //如果来到叶节点 if (tree[o].tag[d.first]) {//若叶节点包含该素数 tree[o].sum = tree[o].sum * ksm(prime[d.first], d.second) % mod;//直接乘上去 } else {//若不包含 tree[o].tag[d.first] = 1;//更新tag tree[o].sum = tree[o].sum * (prime[d.first] - 1) % mod;//更新欧拉函数的值（根据上面的性质） tree[o].sum = tree[o].sum * ksm(prime[d.first], d.second - 1) % mod; } } else {//若区间不全包含该素数，向下遍历更新，可知该操作的势能一定是不断减小的 if (tree[o].lazy != 1) { pushdown(o); } int mid = (l + r) \u0026gt;\u0026gt; 1; if (ml \u0026lt;= mid) { modify(lson, l, mid, ml, mr, d); } if (mid + 1 \u0026lt;= mr) { modify(rson, mid + 1, r, ml, mr, d); } pushup(o); } } ll query(int o, int l, int r, int ql, int qr) { if (ql \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= qr) { return tree[o].sum; } pushdown(o); int mid = (l + r) \u0026gt;\u0026gt; 1; ll res = 0; if (ql \u0026lt;= mid) { res = (res + query(lson, l, mid, ql, qr)) % mod; } if (mid + 1 \u0026lt;= qr) { res = (res + query(rson, mid + 1, r, ql, qr)) % mod; } return res; } ll ans[maxn]; int main() { ios; euler(); for (int i = 1, x; i \u0026lt; maxm; i++) {//分解质因数 for (int j = 1; j \u0026lt;= prime[0]; j++) { x = i; while (x % prime[j] == 0) { cnt[i][j]++; x /= prime[j]; } st[i][j] = cnt[i][j];//状态压缩，01表示是否含有质因数 } } int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; w[i]; } build(1, 1, n); int op, opl, opr, x; while (m--) { cin \u0026gt;\u0026gt; op \u0026gt;\u0026gt; opl \u0026gt;\u0026gt; opr; if (op) { ans[++ans[0]] = query(1, 1, n, opl, opr); } else { cin \u0026gt;\u0026gt; x; for (int i = 1; i \u0026lt;= prime[0]; i++) {//每次修改的是该数对应的所有质因数及其次数 if (cnt[x][i]) { modify(1, 1, n, opl, opr, { i,cnt[x][i] }); } } } } for (int i = 1; i \u0026lt; ans[0]; i++) { cout \u0026lt;\u0026lt; ans[i] \u0026lt;\u0026lt; endl; } if (ans[0]) { cout \u0026lt;\u0026lt; ans[ans[0]]; } return 0; } 简单练习 #\r本来只是准备了上面三道题的，在整理的过程中发现自己以前做过的题中还有一道也运用了势能线段树的思想，而且比上面三道题都要简单，有兴趣的同学可以去看一下\n题目来源：2021东北四省赛 D Lowbit（\rHDU7116）\n参考文献 #\rhttps://blog.csdn.net/Daniel__d/article/details/105104831\nhttps://blog.csdn.net/qq_33969563/article/details/120497762\n","date":"2021年10月3日","externalUrl":null,"permalink":"/posts/%E8%A1%A5%E6%A1%A3%E5%8A%BF%E8%83%BD%E7%BA%BF%E6%AE%B5%E6%A0%91%E5%90%89%E5%8F%B8%E6%9C%BA%E7%BA%BF%E6%AE%B5%E6%A0%91%E4%B8%93%E9%A2%98/","section":"文章","summary":"","title":"（补档）势能线段树（吉司机线段树）专题","type":"posts"},{"content":"","date":"2021年10月3日","externalUrl":null,"permalink":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","section":"Tags","summary":"","title":"数据结构","type":"tags"},{"content":"\r背包-模板题 #\r参考：\r背包九讲——全篇详细理解与代码实现\n1. 01背包 #\r例题\r采药\n#include \u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); using namespace std; typedef long long ll; const int maxn = 10000007; const ll inf = 0x3f3f3f3f; ll v[maxn], w[maxn], dp[maxn]; ll n, m; //P1048 01背包 int main() { ios; cin \u0026gt;\u0026gt; m \u0026gt;\u0026gt; n; for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; w[i] \u0026gt;\u0026gt; v[i]; } for (int i = 1; i \u0026lt;= n; i++) { for (int j = m; j \u0026gt;= w[i]; j--) { dp[j] = max(dp[j], dp[j - w[i]] + v[i]); } } cout \u0026lt;\u0026lt; dp[m] \u0026lt;\u0026lt; endl; return 0; } 2.完全背包 #\r例题：\rP1853 投资的最大效益\n#include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 10000007;//1e5+7 const ll mod = 1000000007;//1e9+7 ll dp[maxn]; ll w[maxn]; ll v[maxn]; int main() { ll s; int n,d; cin\u0026gt;\u0026gt;s\u0026gt;\u0026gt;n\u0026gt;\u0026gt;d; for(int i = 1;i\u0026lt;=d;i++){ cin\u0026gt;\u0026gt;w[i]\u0026gt;\u0026gt;v[i]; } ll nw = s,sx; for(int ye = 1;ye\u0026lt;=n;ye++){ sx = nw; for(int i = 1;i\u0026lt;=d;i++){ for(int j = w[i];j\u0026lt;=sx;j++){ dp[j] = max(dp[j],dp[j - w[i]] + v[i]); } } nw += dp[sx]; } cout\u0026lt;\u0026lt;nw\u0026lt;\u0026lt;endl; return 0; } 例题2：\rP1616 疯狂的采药 先物品后容积\n#include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 10000007;//1e5+7 const ll mod = 1000000007;//1e9+7 ll dp[maxn]; int main() { ll n,m; cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;m; for(ll i = 1,w,v;i\u0026lt;=m;i++){ cin\u0026gt;\u0026gt;w\u0026gt;\u0026gt;v; for(ll j = w;j\u0026lt;=n;j++){ dp[j] = max(dp[j],dp[j - w] + v); } } cout\u0026lt;\u0026lt;dp[n]\u0026lt;\u0026lt;endl; return 0; } 3.多重背包 #\r例题：\rP1776 宝物筛选\n转化为01背包问题求解 #\r既然01背包问题是最基本的背包问题，那么我们可以考虑把完全背包问题转化为01背包问题来解。思路：将多种物品用二进制的思想合并。\n#include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 10000007;//1e5+7 const ll mod = 1000000007;//1e9+7 ll dp[maxn]; ll w[maxn]; ll v[maxn]; int main() { ll s; int n,d; cin\u0026gt;\u0026gt;s\u0026gt;\u0026gt;n\u0026gt;\u0026gt;d; for(int i = 1;i\u0026lt;=d;i++){ cin\u0026gt;\u0026gt;w[i]\u0026gt;\u0026gt;v[i]; } ll nw = s,sx; for(int ye = 1;ye\u0026lt;=n;ye++){ sx = nw; for(int i = 1;i\u0026lt;=d;i++){ for(int j = w[i];j\u0026lt;=sx;j++){ dp[j] = max(dp[j],dp[j - w[i]] + v[i]); } } nw += dp[sx]; } cout\u0026lt;\u0026lt;nw\u0026lt;\u0026lt;endl; return 0; } \u0026#34; ```cpp #include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 100007;//1e5+7 const ll mod = 1000000007;//1e9+7 ll w[maxn]; ll v[maxn]; ll tw[maxn]; ll tv[maxn]; ll s[maxn]; ll ksm(ll x,ll y) { ll res = 1; while(y \u0026gt; 0){ if(y\u0026amp;1){ res = res * x; } y \u0026gt;\u0026gt;= 1; x = x * x; } return res; } ll dp[maxn]; int main() { int n; ll W; cin\u0026gt;\u0026gt;n\u0026gt;\u0026gt;W; for(int i = 1;i\u0026lt;=n;i++){ cin\u0026gt;\u0026gt;tv[i]\u0026gt;\u0026gt;tw[i]\u0026gt;\u0026gt;s[i]; } int m = 0; for(int i = 1;i\u0026lt;=n;i++){ ll mxs = W / s[i]; ll zs = 0; while(s[i] \u0026gt; 0){ ll nw = ksm(2,zs); if(s[i] \u0026gt;= nw){ s[i] -= nw; w[++m] = tw[i] * nw; v[m] = tv[i] * nw; } else{ w[++m] = tw[i] * s[i]; v[m] = tv[i] * s[i]; s[i] = 0; } zs++; } } for(int i = 1;i\u0026lt;=m;i++){ for(int j = W;j\u0026gt;=w[i];j--){ dp[j] = max(dp[j],dp[j - w[i]] + v[i]); } } cout\u0026lt;\u0026lt;dp[W]\u0026lt;\u0026lt;endl; return 0; } 4.混合背包 #\r例题：\rP1833 樱花 分类讨论 + 降维打击：完全型物品完全解法，01型物品、多重型物品，降维成01背包解决\n#include\u0026lt;bits/stdc++.h\u0026gt; #define ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; const int maxn = 1000007; int dp[maxn]; bool inf[maxn]; int w[maxn]; int v[maxn]; int main() { int sh,sm,eh,em,n,m = 0; scanf(\u0026#34;%d:%d %d:%d %d\u0026#34;,\u0026amp;sh,\u0026amp;sm,\u0026amp;eh,\u0026amp;em,\u0026amp;n); int T = (eh - sh) * 60 + em - sm ; //cout\u0026lt;\u0026lt;T\u0026lt;\u0026lt;endl; memset(dp,0,sizeof(dp)); memset(inf,0,sizeof(inf)); for(int i = 1,tw,tv,s;i\u0026lt;=n;i++){ cin\u0026gt;\u0026gt;tw\u0026gt;\u0026gt;tv\u0026gt;\u0026gt;s; if(s == 0){ w[++m] = tw; v[m] = tv; inf[m] = 1; } else{ for(int j = 1;j\u0026lt;=s;j\u0026lt;\u0026lt;=1){ w[++m] = tw * j; v[m] = tv * j; s -= j; } if(s){ w[++m] = tw * s; v[m] = tv * s; } } } for(int i = 1;i\u0026lt;=m;i++){ if(inf[i]){ for(int j = w[i];j\u0026lt;=T;j++){ dp[j] = max(dp[j],dp[j - w[i]] + v[i]); } } else{ for(int j = T;j\u0026gt;=w[i];j--){ dp[j] = max(dp[j],dp[j - w[i]] + v[i]); } } } cout\u0026lt;\u0026lt;dp[T]\u0026lt;\u0026lt;endl; return 0; } 5.二维费用背包 #\r例题：\rP1507 NASA的食物计划 在普通背包的基础上多加了一维费用而已\n#include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 1007;//1e5+7 const ll mod = 1000000007;//1e9+7 int dp[maxn][maxn]; int main() { int T,W,n; cin\u0026gt;\u0026gt;T\u0026gt;\u0026gt;W; cin\u0026gt;\u0026gt;n; for(int i = 1,t,w,v;i\u0026lt;=n;i++){ cin\u0026gt;\u0026gt;t\u0026gt;\u0026gt;w\u0026gt;\u0026gt;v; for(int j = W;j\u0026gt;=w;j--){ for(int k = T;k\u0026gt;=t;k--){ dp[j][k] = max(dp[j][k],dp[j - w][k - t] + v); } } } cout\u0026lt;\u0026lt;dp[W][T]\u0026lt;\u0026lt;endl; return 0; } 6.分组背包 #\r例题：\rP1757 通天之分组背包\n以前的01物品变成了一个个的物品组，做法别无二致，在循环的内部多一重对组内物品的枚举即可\nfor (所有的组k)\rfor (int j = V; j \u0026gt;= 0; j--)\rfor (所有属于组k的i)\rf[j] = max{f[j], f[j - w[i]] + v[i]} #include\u0026lt;bits/stdc++.h\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl #define inr register int using namespace std; typedef long long ll; const double pi=acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 100007;//1e5+7 const ll mod = 1000000007;//1e9+7 int dp[maxn]; struct node{ int w,v; }; vector\u0026lt;node\u0026gt;G[maxn]; int main() { int n,m,mx = 0; cin\u0026gt;\u0026gt;m\u0026gt;\u0026gt;n; for(int i = 1,a,b,c;i\u0026lt;=n;i++){ cin\u0026gt;\u0026gt;a\u0026gt;\u0026gt;b\u0026gt;\u0026gt;c; node nd; nd.w = a; nd.v = b; mx = max(c,mx); G[c].push_back(nd); } for(int i = 0;i\u0026lt;=mx;i++){ if(G[i].size()){ for(int j = m;j\u0026gt;=0;j--){ for(int k = 0;k\u0026lt;=G[i].size();k++){ if(G[i][k].w \u0026lt;= j){ dp[j] = max(dp[j],dp[j - G[i][k].w] + G[i][k].v); } } }\t} } cout\u0026lt;\u0026lt;dp[m]\u0026lt;\u0026lt;endl; return 0; } 7.有依赖的背包 #\r例题：\rP1064 金明的预算方案\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #define ios ios::sync_with_stdio(false);cin.tie(0);cout.tie(0); #define debug(a) cout \u0026lt;\u0026lt; #a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl using namespace std; typedef long long ll; const double pi = acos(-1); const double eps = 1e-8; const int inf = 0x3f3f3f3f; const int maxn = 67; const int maxm = 50007;//5e4+7 const ll mod = 1000000007;//1e9+7 int read() { int res = 0, ch, flag = 0; if ((ch = getchar()) == \u0026#39;-\u0026#39;) flag = 1; else if (ch \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; ch \u0026lt;= \u0026#39;9\u0026#39;) res = ch - \u0026#39;0\u0026#39;; while ((ch = getchar()) \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; ch \u0026lt;= \u0026#39;9\u0026#39;) res = res * 10 + ch - \u0026#39;0\u0026#39;; return flag ? -res : res; } int n, m; struct node { int w, v; }; vector\u0026lt;node\u0026gt;G[maxn]; node zj[maxn]; int pdp[maxn][maxm]; int dp[maxm]; bool pd[maxn]; int main() { n = read(); m = read(); n /= 10; for (int i = 1,q, w, v; i \u0026lt;= m; i++) { w = read();v = read();q = read(); w /= 10; if (q) { G[q].push_back(node{ w,v * w }); } else { pd[i] = 1; zj[i] = node{ w,v * w }; } } for (int i = 1; i \u0026lt;= m; i++) { if (pd[i]) { for (int j = zj[i].w; j \u0026lt;= n; j++) { pdp[i][j] = zj[i].v; } for (int j = 0; j \u0026lt; G[i].size(); j++) { for (int k = n; k \u0026gt;= G[i][j].w + zj[i].w; k--) { pdp[i][k] = max(pdp[i][k], pdp[i][k - G[i][j].w] + G[i][j].v); } } } } for (int i = 1; i \u0026lt;= m; i++) { if (pd[i]) { for (int j = n; j \u0026gt;= 0;j--) { for (int k = zj[i].w; k \u0026lt;= j; k++) { dp[j] = max(dp[j], dp[j - k] + pdp[i][k]); } } } } printf(\u0026#34;%d\\n\u0026#34;,dp[n] * 10); return 0; } 总结 #\r总结：不论背包问题多么复杂，皆可向01背包的思路靠近\n","date":"2020年11月28日","externalUrl":null,"permalink":"/posts/%E8%A1%A5%E6%A1%A3%E7%AE%80%E5%8D%95%E8%83%8C%E5%8C%85-%E6%A8%A1%E6%9D%BF%E9%A2%98/","section":"文章","summary":"","title":"（补档）简单背包 模板题","type":"posts"},{"content":"","date":"2020年11月28日","externalUrl":null,"permalink":"/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"Tags","summary":"","title":"动态规划","type":"tags"},{"content":"","date":"0001年1月1日","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"0001年1月1日","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\r没有更多了 #\r","date":"0001年1月1日","externalUrl":null,"permalink":"/more/","section":"更多","summary":"","title":"更多","type":"more"}]